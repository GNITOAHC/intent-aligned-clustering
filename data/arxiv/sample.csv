id,text
0,"Class imbalance is a common problem in supervised learning and impedes the
predictive performance of classification models. Popular countermeasures
include oversampling the minority class. Standard methods like SMOTE rely on
finding nearest neighbours and linear interpolations which are problematic in
case of high-dimensional, complex data distributions. Generative Adversarial
Networks (GANs) have been proposed as an alternative method for generating
artificial minority examples as they can model complex distributions. However,
prior research on GAN-based oversampling does not incorporate recent
advancements from the literature on generating realistic tabular data with
GANs. Previous studies also focus on numerical variables whereas categorical
features are common in many business applications of classification methods
such as credit scoring. The paper propoes an oversampling method based on a
conditional Wasserstein GAN that can effectively model tabular datasets with
numerical and categorical variables and pays special attention to the
down-stream classification task through an auxiliary classifier loss. We
benchmark our method against standard oversampling methods and the imbalanced
baseline on seven real-world datasets. Empirical results evidence the
competitiveness of GAN-based oversampling."
1,"We revisit the standard formulation of tabular actor-critic algorithm as a
two time-scale stochastic approximation with value function computed on a
faster time-scale and policy computed on a slower time-scale. This emulates
policy iteration. We observe that reversal of the time scales will in fact
emulate value iteration and is a legitimate algorithm. We provide a proof of
convergence and compare the two empirically with and without function
approximation (with both linear and nonlinear function approximators) and
observe that our proposed critic-actor algorithm performs on par with
actor-critic in terms of both accuracy and computational effort."
2,"We show that using nearest neighbours in the latent space of autoencoders
(AE) significantly improves performance of semi-supervised novelty detection in
both single and multi-class contexts. Autoencoding methods detect novelty by
learning to differentiate between the non-novel training class(es) and all
other unseen classes. Our method harnesses a combination of the reconstructions
of the nearest neighbours and the latent-neighbour distances of a given input's
latent representation. We demonstrate that our nearest-latent-neighbours (NLN)
algorithm is memory and time efficient, does not require significant data
augmentation, nor is reliant on pre-trained networks. Furthermore, we show that
the NLN-algorithm is easily applicable to multiple datasets without
modification. Additionally, the proposed algorithm is agnostic to autoencoder
architecture and reconstruction error method. We validate our method across
several standard datasets for a variety of different autoencoding architectures
such as vanilla, adversarial and variational autoencoders using either
reconstruction, residual or feature consistent losses. The results show that
the NLN algorithm grants up to a 17% increase in Area Under the Receiver
Operating Characteristics (AUROC) curve performance for the multi-class case
and 8% for single-class novelty detection."
3,"We introduce Universum learning for multiclass problems and propose a novel
formulation for multiclass universum SVM (MU-SVM). We also propose a span bound
for MU-SVM that can be used for model selection thereby avoiding resampling.
Empirical results demonstrate the effectiveness of MU-SVM and the proposed
bound."
4,"Reward-free reinforcement learning (RL) is a framework which is suitable for
both the batch RL setting and the setting where there are many reward functions
of interest. During the exploration phase, an agent collects samples without
using a pre-specified reward function. After the exploration phase, a reward
function is given, and the agent uses samples collected during the exploration
phase to compute a near-optimal policy. Jin et al. [2020] showed that in the
tabular setting, the agent only needs to collect polynomial number of samples
(in terms of the number states, the number of actions, and the planning
horizon) for reward-free RL. However, in practice, the number of states and
actions can be large, and thus function approximation schemes are required for
generalization. In this work, we give both positive and negative results for
reward-free RL with linear function approximation. We give an algorithm for
reward-free RL in the linear Markov decision process setting where both the
transition and the reward admit linear representations. The sample complexity
of our algorithm is polynomial in the feature dimension and the planning
horizon, and is completely independent of the number of states and actions. We
further give an exponential lower bound for reward-free RL in the setting where
only the optimal $Q$-function admits a linear representation. Our results imply
several interesting exponential separations on the sample complexity of
reward-free RL."
5,"Canonical Correlation Analysis (CCA) is widely used for multimodal data
analysis and, more recently, for discriminative tasks such as multi-view
learning; however, it makes no use of class labels. Recent CCA methods have
started to address this weakness but are limited in that they do not
simultaneously optimize the CCA projection for discrimination and the CCA
projection itself, or they are linear only. We address these deficiencies by
simultaneously optimizing a CCA-based and a task objective in an end-to-end
manner. Together, these two objectives learn a non-linear CCA projection to a
shared latent space that is highly correlated and discriminative. Our method
shows a significant improvement over previous state-of-the-art (including deep
supervised approaches) for cross-view classification, regularization with a
second view, and semi-supervised learning on real data."
6,"We propose a new method of program learning in a Domain Specific Language
(DSL) which is based on gradient descent with no direct search. The first
component of our method is a probabilistic representation of the DSL variables.
At each timestep in the program sequence, different DSL functions are applied
on the DSL variables with a certain probability, leading to different possible
outcomes. Rather than handling all these outputs separately, whose number grows
exponentially with each timestep, we collect them into a superposition of
variables which captures the information in a single, but fuzzy, state. This
state is to be contrasted at the final timestep with the ground-truth output,
through a loss function. The second component of our method is an
attention-based recurrent neural network, which provides an appropriate
initialization point for the gradient descent that optimizes the probabilistic
representation. The method we have developed surpasses the state-of-the-art for
synthesising long programs and is able to learn programs under noise."
7,"Several papers argue that wide minima generalize better than narrow minima.
In this paper, through detailed experiments that not only corroborate the
generalization properties of wide minima, we also provide empirical evidence
for a new hypothesis that the density of wide minima is likely lower than the
density of narrow minima. Further, motivated by this hypothesis, we design a
novel explore-exploit learning rate schedule. On a variety of image and natural
language datasets, compared to their original hand-tuned learning rate
baselines, we show that our explore-exploit schedule can result in either up to
0.84% higher absolute accuracy using the original training budget or up to 57%
reduced training time while achieving the original reported accuracy. For
example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN)
dataset by just modifying the learning rate schedule of a high performing
model."
8,"Biclustering is a powerful approach to search for patterns in data, as it can
be driven by a function that measures the quality of diverse types of patterns
of interest. However, due to its computational complexity, the exploration of
the search space is usually guided by an algorithmic strategy, sometimes
introducing random factors that simplify the computational cost (e.g. greedy
search or evolutionary computation).
  Shifting patterns are specially interesting as they account constant
fluctuations in data, i.e. they capture situations in which all the values in
the pattern move up or down for one dimension maintaining the range amplitude
for all the dimensions. This behaviour is very common in nature, e.g. in the
analysis of gene expression data, where a subset of genes might go up or down
for a subset of patients or experimental conditions, identifying functionally
coherent categories.
  Boolean reasoning was recently revealed as an appropriate methodology to
address the search for constant biclusters. In this work, this direction is
extended to search for more general biclusters that include shifting patterns.
The mathematical foundations are described in order to associate Boolean
concepts with shifting patterns, and the methodology is presented to show that
the induction of shifting patterns by means of Boolean reasoning is due to the
ability of finding all inclusion--maximal {\delta}-shifting patterns.
  Experiments with a real dataset show the potential of our approach at finding
biclusters with {\delta}-shifting patterns, which have been evaluated with the
mean squared residue (MSR), providing an excellent performance at finding
results very close to zero."
9,"Explaining machine learning models is an important and increasingly popular
area of research interest. The Shapley value from game theory has been proposed
as a prime approach to compute feature importance towards model predictions on
images, text, tabular data, and recently graph neural networks (GNNs) on
graphs. In this work, we revisit the appropriateness of the Shapley value for
GNN explanation, where the task is to identify the most important subgraph and
constituent nodes for GNN predictions. We claim that the Shapley value is a
non-ideal choice for graph data because it is by definition not
structure-aware. We propose a Graph Structure-aware eXplanation (GStarX) method
to leverage the critical graph structure information to improve the
explanation. Specifically, we define a scoring function based on a new
structure-aware value from the cooperative game theory proposed by Hamiache and
Navarro (HN). When used to score node importance, the HN value utilizes graph
structures to attribute cooperation surplus between neighbor nodes, resembling
message passing in GNNs, so that node importance scores reflect not only the
node feature importance, but also the node structural roles. We demonstrate
that GStarX produces qualitatively more intuitive explanations, and
quantitatively improves explanation fidelity over strong baselines on chemical
graph property prediction and text graph sentiment classification."
10,"Low-cost cross-modal representation learning is crucial for deriving semantic
representations across diverse modalities such as text, audio, images, and
video. Traditional approaches typically depend on large specialized models
trained from scratch, requiring extensive datasets and resulting in high
resource and time costs. To overcome these challenges, we introduce a novel
approach named Lightweight Cross-Modal Representation Learning (LightCRL). This
method uses a single neural network titled Deep Fusion Encoder (DFE), which
projects data from multiple modalities into a shared latent representation
space. This reduces the overall parameter count while still delivering robust
performance comparable to more complex systems."
11,"To date, various neural methods have been proposed for causal effect
estimation based on observational data, where a default assumption is the same
distribution and availability of variables at both training and inference
(i.e., runtime) stages. However, distribution shift (i.e., domain shift) could
happen during runtime, and bigger challenges arise from the impaired
accessibility of variables. This is commonly caused by increasing privacy and
ethical concerns, which can make arbitrary variables unavailable in the entire
runtime data and imputation impractical. We term the co-occurrence of domain
shift and inaccessible variables runtime domain corruption, which seriously
impairs the generalizability of a trained counterfactual predictor. To counter
runtime domain corruption, we subsume counterfactual prediction under the
notion of domain adaptation. Specifically, we upper-bound the error w.r.t. the
target domain (i.e., runtime covariates) by the sum of source domain error and
inter-domain distribution distance. In addition, we build an adversarially
unified variational causal effect model, named VEGAN, with a novel two-stage
adversarial domain adaptation scheme to reduce the latent distribution
disparity between treated and control groups first, and between training and
runtime variables afterwards. We demonstrate that VEGAN outperforms other
state-of-the-art baselines on individual-level treatment effect estimation in
the presence of runtime domain corruption on benchmark datasets."
12,"Given a resource-rich source graph and a resource-scarce target graph, how
can we effectively transfer knowledge across graphs and ensure a good
generalization performance? In many high-impact domains (e.g., brain networks
and molecular graphs), collecting and annotating data is prohibitively
expensive and time-consuming, which makes domain adaptation an attractive
option to alleviate the label scarcity issue. In light of this, the
state-of-the-art methods focus on deriving domain-invariant graph
representation that minimizes the domain discrepancy. However, it has recently
been shown that a small domain discrepancy loss may not always guarantee a good
generalization performance, especially in the presence of disparate graph
structures and label distribution shifts. In this paper, we present TRANSNET, a
generic learning framework for augmenting knowledge transfer across graphs. In
particular, we introduce a novel notion named trinity signal that can naturally
formulate various graph signals at different granularity (e.g., node
attributes, edges, and subgraphs). With that, we further propose a domain
unification module together with a trinity-signal mixup scheme to jointly
minimize the domain discrepancy and augment the knowledge transfer across
graphs. Finally, comprehensive empirical results show that TRANSNET outperforms
all existing approaches on seven benchmark datasets by a significant margin."
13,"Reducing traffic accidents is a crucial global public safety concern.
Accident prediction is key to improving traffic safety, enabling proactive
measures to be taken before a crash occurs, and informing safety policies,
regulations, and targeted interventions. Despite numerous studies on accident
prediction over the past decades, many have limitations in terms of
generalizability, reproducibility, or feasibility for practical use due to
input data or problem formulation. To address existing shortcomings, we propose
CrashFormer, a multi-modal architecture that utilizes comprehensive (but
relatively easy to obtain) inputs such as the history of accidents, weather
information, map images, and demographic information. The model predicts the
future risk of accidents on a reasonably acceptable cadence (i.e., every six
hours) for a geographical location of 5.161 square kilometers. CrashFormer is
composed of five components: a sequential encoder to utilize historical
accidents and weather data, an image encoder to use map imagery data, a raw
data encoder to utilize demographic information, a feature fusion module for
aggregating the encoded features, and a classifier that accepts the aggregated
data and makes predictions accordingly. Results from extensive real-world
experiments in 10 major US cities show that CrashFormer outperforms
state-of-the-art sequential and non-sequential models by 1.8% in F1-score on
average when using ``sparse'' input data."
14,"There is often variation in the shape and size of input data used for deep
learning. In many cases, such data can be represented using tensors with
non-uniform shapes, or ragged tensors. Due to limited and non-portable support
for efficient execution on ragged tensors, current deep learning frameworks
generally use techniques such as padding and masking to make the data shapes
uniform and then offload the computations to optimized kernels for dense tensor
algebra. Such techniques can, however, lead to a lot of wasted computation and
therefore, a loss in performance. This paper presents CoRa, a tensor compiler
that allows users to easily generate efficient code for ragged tensor operators
targeting a wide range of CPUs and GPUs. Evaluating CoRa on a variety of
operators on ragged tensors as well as on an encoder layer of the transformer
model, we find that CoRa (i)performs competitively with hand-optimized
implementations of the operators and the transformer encoder and (ii) achieves,
over PyTorch, a 1.6X geomean speedup for the encoder on an Nvidia GPU and a
1.86X geomean speedup for the multi-head attention module used in transformers
on an ARM CPU."
15,"In this paper, we study the robustness of graph convolutional networks
(GCNs). Despite the good performance of GCNs on graph semi-supervised learning
tasks, previous works have shown that the original GCNs are very unstable to
adversarial perturbations. In particular, we can observe a severe performance
degradation by slightly changing the graph adjacency matrix or the features of
a few nodes, making it unsuitable for security-critical applications. Inspired
by the previous works on adversarial defense for deep neural networks, and
especially adversarial training algorithm, we propose a method called
GraphDefense to defend against the adversarial perturbations. In addition, for
our defense method, we could still maintain semi-supervised learning settings,
without a large label rate. We also show that adversarial training in features
is equivalent to adversarial training for edges with a small perturbation. Our
experiments show that the proposed defense methods successfully increase the
robustness of Graph Convolutional Networks. Furthermore, we show that with
careful design, our proposed algorithm can scale to large graphs, such as
Reddit dataset."
16,"Explanations of time series models are useful for high stakes applications
like healthcare but have received little attention in machine learning
literature. We propose FIT, a framework that evaluates the importance of
observations for a multivariate time-series black-box model by quantifying the
shift in the predictive distribution over time. FIT defines the importance of
an observation based on its contribution to the distributional shift under a
KL-divergence that contrasts the predictive distribution against a
counterfactual where the rest of the features are unobserved. We also
demonstrate the need to control for time-dependent distribution shifts. We
compare with state-of-the-art baselines on simulated and real-world clinical
data and demonstrate that our approach is superior in identifying important
time points and observations throughout the time series."
17,"Machine unlearning (MU) aims to eliminate information that has been learned
from specific training data, namely forgetting data, from a pre-trained model.
Currently, the mainstream of existing MU methods involves modifying the
forgetting data with incorrect labels and subsequently fine-tuning the model.
While learning such incorrect information can indeed remove knowledge, the
process is quite unnatural as the unlearning process undesirably reinforces the
incorrect information and leads to over-forgetting. Towards more
\textit{natural} machine unlearning, we inject correct information from the
remaining data to the forgetting samples when changing their labels. Through
pairing these adjusted samples with their labels, the model will tend to use
the injected correct information and naturally suppress the information meant
to be forgotten. Albeit straightforward, such a first step towards natural
machine unlearning can significantly outperform current state-of-the-art
approaches. In particular, our method substantially reduces the over-forgetting
and leads to strong robustness to hyperparameters, making it a promising
candidate for practical machine unlearning."
18,"Density ratio estimation (DRE) is a fundamental machine learning technique
for identifying relationships between two probability distributions.
$f$-divergence loss functions, derived from variational representations of
$f$-divergence, are commonly employed in DRE to achieve state-of-the-art
results. This study presents a novel perspective on DRE using $f$-divergence
loss functions by deriving the upper and lower bounds on $L_p$ errors. These
bounds apply to any estimator within a class of Lipschitz continuous
estimators, irrespective of the specific $f$-divergence loss functions
utilized. The bounds are formulated as a product of terms that include the data
dimension and the expected value of the density ratio raised to the power of
$p$. Notably, the lower bound incorporates an exponential term dependent on the
Kullback--Leibler divergence, indicating that the $L_p$ error significantly
increases with the Kullback--Leibler divergence for $p > 1$, and this increase
becomes more pronounced as $p$ increases. Furthermore, these theoretical
findings are substantiated through numerical experiments."
19,"We propose a self-supervised framework that learns to group visual entities
based on their rate of co-occurrence in space and time. To model statistical
dependencies between the entities, we set up a simple binary classification
problem in which the goal is to predict if two visual primitives occur in the
same spatial or temporal context. We apply this framework to three domains:
learning patch affinities from spatial adjacency in images, learning frame
affinities from temporal adjacency in videos, and learning photo affinities
from geospatial proximity in image collections. We demonstrate that in each
case the learned affinities uncover meaningful semantic groupings. From patch
affinities we generate object proposals that are competitive with
state-of-the-art supervised methods. From frame affinities we generate movie
scene segmentations that correlate well with DVD chapter structure. Finally,
from geospatial affinities we learn groups that relate well to semantic place
categories."
20,"In this paper, we introduce a novel approach for diagnosis of Parkinson's
Disease (PD) based on deep Echo State Networks (ESNs). The identification of PD
is performed by analyzing the whole time-series collected from a tablet device
during the sketching of spiral tests, without the need for feature extraction
and data preprocessing. We evaluated the proposed approach on a public dataset
of spiral tests. The results of experimental analysis show that DeepESNs
perform significantly better than shallow ESN model. Overall, the proposed
approach obtains state-of-the-art results in the identification of PD on this
kind of temporal data."
21,"Feature attribution methods, which explain an individual prediction made by a
model as a sum of attributions for each input feature, are an essential tool
for understanding the behavior of complex deep learning models. However,
ensuring that models produce meaningful explanations, rather than ones that
rely on noise, is not straightforward. Exacerbating this problem is the fact
that attribution methods do not provide insight as to why features are assigned
their attribution values, leading to explanations that are difficult to
interpret. In real-world problems we often have sets of additional information
for each feature that are predictive of that feature's importance to the task
at hand. Here, we propose the deep attribution prior (DAPr) framework to
exploit such information to overcome the limitations of attribution methods.
Our framework jointly learns a relationship between prior information and
feature importance, as well as biases models to have explanations that rely on
features predicted to be important. We find that our framework both results in
networks that generalize better to out of sample data and admits new methods
for interpreting model behavior."
22,"In many naturally occurring optimization problems one needs to ensure that
the definition of the optimization problem lends itself to solutions that are
tractable to compute. In cases where exact solutions cannot be computed
tractably, it is beneficial to have strong guarantees on the tractable
approximate solutions. In order operate under these criterion most optimization
problems are cast under the umbrella of convexity or submodularity. In this
report we will study design and optimization over a common class of functions
called submodular functions. Set functions, and specifically submodular set
functions, characterize a wide variety of naturally occurring optimization
problems, and the property of submodularity of set functions has deep
theoretical consequences with wide ranging applications. Informally, the
property of submodularity of set functions concerns the intuitive ""principle of
diminishing returns. This property states that adding an element to a smaller
set has more value than adding it to a larger set. Common examples of
submodular monotone functions are entropies, concave functions of cardinality,
and matroid rank functions; non-monotone examples include graph cuts, network
flows, and mutual information.
  In this paper we will review the formal definition of submodularity; the
optimization of submodular functions, both maximization and minimization; and
finally discuss some applications in relation to learning and reasoning using
submodular functions."
23,"Neural networks with binary weights are computation-efficient and
hardware-friendly, but their training is challenging because it involves a
discrete optimization problem. Surprisingly, ignoring the discrete nature of
the problem and using gradient-based methods, such as the Straight-Through
Estimator, still works well in practice. This raises the question: are there
principled approaches which justify such methods? In this paper, we propose
such an approach using the Bayesian learning rule. The rule, when applied to
estimate a Bernoulli distribution over the binary weights, results in an
algorithm which justifies some of the algorithmic choices made by the previous
approaches. The algorithm not only obtains state-of-the-art performance, but
also enables uncertainty estimation for continual learning to avoid
catastrophic forgetting. Our work provides a principled approach for training
binary neural networks which justifies and extends existing approaches."
24,"We consider the bandit problem of selecting $K$ out of $N$ arms at each time
step. The reward can be a non-linear function of the rewards of the selected
individual arms. The direct use of a multi-armed bandit algorithm requires
choosing among $\binom{N}{K}$ options, making the action space large. To
simplify the problem, existing works on combinatorial bandits {typically}
assume feedback as a linear function of individual rewards. In this paper, we
prove the lower bound for top-$K$ subset selection with bandit feedback with
possibly correlated rewards. We present a novel algorithm for the combinatorial
setting without using individual arm feedback or requiring linearity of the
reward function. Additionally, our algorithm works on correlated rewards of
individual arms. Our algorithm, aDaptive Accept RejecT (DART), sequentially
finds good arms and eliminates bad arms based on confidence bounds. DART is
computationally efficient and uses storage linear in $N$. Further, DART
achieves a regret bound of $\tilde{\mathcal{O}}(K\sqrt{KNT})$ for a time
horizon $T$, which matches the lower bound in bandit feedback up to a factor of
$\sqrt{\log{2NT}}$. When applied to the problem of cross-selling optimization
and maximizing the mean of individual rewards, the performance of the proposed
algorithm surpasses that of state-of-the-art algorithms. We also show that DART
significantly outperforms existing methods for both linear and non-linear joint
reward environments."
25,"Gaussian processes (GPs) are non-parametric, flexible, models that work well
in many tasks. Combining GPs with deep learning methods via deep kernel
learning (DKL) is especially compelling due to the strong representational
power induced by the network. However, inference in GPs, whether with or
without DKL, can be computationally challenging on large datasets. Here, we
propose GP-Tree, a novel method for multi-class classification with Gaussian
processes and DKL. We develop a tree-based hierarchical model in which each
internal node of the tree fits a GP to the data using the P\'olya Gamma
augmentation scheme. As a result, our method scales well with both the number
of classes and data size. We demonstrate the effectiveness of our method
against other Gaussian process training baselines, and we show how our general
GP approach achieves improved accuracy on standard incremental few-shot
learning benchmarks."
26,"In a business-to-business (B2B) customer relationship management (CRM) use
case, each client is a potential business organization/company with a solid
business strategy and focused and rational decisions. This paper introduces a
graph-based analytics approach to improve CRM within a B2B environment. In our
approach, in the first instance, we have designed a graph database using the
Neo4j platform. Secondly, the graph database has been investigated by using
data mining and exploratory analysis coupled with cypher graph query language.
Specifically, we have applied the graph convolution network (GCN) to enable CRM
analytics to forecast sales. This is the first step towards a GCN-based binary
classification based on graph databases in the domain of B2B CRM. We evaluate
the performance of the proposed GCN model on graph databases and compare it
with Random Forest (RF), Convolutional Neural Network (CNN), and Artificial
Neural Network (ANN). The proposed GCN approach is further augmented with the
shortest path and eigenvector centrality attribute to significantly improve the
accuracy of sales prediction. Experimental results reveal that the proposed
graph-based deep learning approach outperforms the Random Forests (RsF) and two
deep learning models, i.e., CNN and ANN under different combinations of graph
features."
27,"Multi-armed bandits (MAB) are extensively studied in various settings where
the objective is to \textit{maximize} the actions' outcomes (i.e., rewards)
over time. Since safety is crucial in many real-world problems, safe versions
of MAB algorithms have also garnered considerable interest. In this work, we
tackle a different critical task through the lens of \textit{linear stochastic
bandits}, where the aim is to keep the actions' outcomes close to a target
level while respecting a \textit{two-sided} safety constraint, which we call
\textit{leveling}. Such a task is prevalent in numerous domains. Many
healthcare problems, for instance, require keeping a physiological variable in
a range and preferably close to a target level. The radical change in our
objective necessitates a new acquisition strategy, which is at the heart of a
MAB algorithm. We propose SALE-LTS: Safe Leveling via Linear Thompson Sampling
algorithm, with a novel acquisition strategy to accommodate our task and show
that it achieves sublinear regret with the same time and dimension dependence
as previous works on the classical reward maximization problem absent any
safety constraint. We demonstrate and discuss our algorithm's empirical
performance in detail via thorough experiments."
28,"Deep neural networks (DNNs) have been emerged as the state-of-the-art
algorithms in broad range of applications. To reduce the memory foot-print of
DNNs, in particular for embedded applications, sparsification techniques have
been proposed. Unfortunately, these techniques come with a large hardware
overhead. In this paper, we present a hardware-aware pruning method where the
locations of non-zero weights are derived in real-time from a Linear Feedback
Shift Registers (LFSRs). Using the proposed method, we demonstrate a total
saving of energy and area up to 63.96% and 64.23% for VGG-16 network on
down-sampled ImageNet, respectively for iso-compression-rate and iso-accuracy."
29,"This paper studies a new variant of the stochastic multi-armed bandits
problem where auxiliary information about the arm rewards is available in the
form of control variates. In many applications like queuing and wireless
networks, the arm rewards are functions of some exogenous variables. The mean
values of these variables are known a priori from historical data and can be
used as control variates. Leveraging the theory of control variates, we obtain
mean estimates with smaller variance and tighter confidence bounds. We develop
an upper confidence bound based algorithm named UCB-CV and characterize the
regret bounds in terms of the correlation between rewards and control variates
when they follow a multivariate normal distribution. We also extend UCB-CV to
other distributions using resampling methods like Jackknifing and Splitting.
Experiments on synthetic problem instances validate performance guarantees of
the proposed algorithms."
30,"Deep neural networks often suffer from overconfidence which can be partly
remedied by improved out-of-distribution detection. For this purpose, we
propose a novel approach that allows for the generation of out-of-distribution
datasets based on a given in-distribution dataset. This new dataset can then be
used to improve out-of-distribution detection for the given dataset and machine
learning task at hand. The samples in this dataset are with respect to the
feature space close to the in-distribution dataset and therefore realistic and
plausible. Hence, this dataset can also be used to safeguard neural networks,
i.e., to validate the generalization performance. Our approach first generates
suitable representations of an in-distribution dataset using an autoencoder and
then transforms them using our novel proposed Soft Brownian Offset method.
After transformation, the decoder part of the autoencoder allows for the
generation of these implicit out-of-distribution samples. This newly generated
dataset then allows for mixing with other datasets and thus improved training
of an out-of-distribution classifier, increasing its performance.
Experimentally, we show that our approach is promising for time series using
synthetic data. Using our new method, we also show in a quantitative case study
that we can improve the out-of-distribution detection for the MNIST dataset.
Finally, we provide another case study on the synthetic generation of
out-of-distribution trajectories, which can be used to validate trajectory
prediction algorithms for automated driving."
31,"Deep neural networks trained on a wide range of datasets demonstrate
impressive transferability. Deep features appear general in that they are
applicable to many datasets and tasks. Such property is in prevalent use in
real-world applications. A neural network pretrained on large datasets, such as
ImageNet, can significantly boost generalization and accelerate training if
fine-tuned to a smaller target dataset. Despite its pervasiveness, few effort
has been devoted to uncovering the reason of transferability in deep feature
representations. This paper tries to understand transferability from the
perspectives of improved generalization, optimization and the feasibility of
transferability. We demonstrate that 1) Transferred models tend to find flatter
minima, since their weight matrices stay close to the original flat region of
pretrained parameters when transferred to a similar target dataset; 2)
Transferred representations make the loss landscape more favorable with
improved Lipschitzness, which accelerates and stabilizes training
substantially. The improvement largely attributes to the fact that the
principal component of gradient is suppressed in the pretrained parameters,
thus stabilizing the magnitude of gradient in back-propagation. 3) The
feasibility of transferability is related to the similarity of both input and
label. And a surprising discovery is that the feasibility is also impacted by
the training stages in that the transferability first increases during
training, and then declines. We further provide a theoretical analysis to
verify our observations."
32,"While the conditional sequence modeling with the transformer architecture has
demonstrated its effectiveness in dealing with offline reinforcement learning
(RL) tasks, it is struggle to handle out-of-distribution states and actions.
Existing work attempts to address this issue by data augmentation with the
learned policy or adding extra constraints with the value-based RL algorithm.
However, these studies still fail to overcome the following challenges: (1)
insufficiently utilizing the historical temporal information among inter-steps,
(2) overlooking the local intrastep relationships among return-to-gos (RTGs),
states, and actions, (3) overfitting suboptimal trajectories with noisy labels.
To address these challenges, we propose Decision Mamba (DM), a novel
multi-grained state space model (SSM) with a self-evolving policy learning
strategy. DM explicitly models the historical hidden state to extract the
temporal information by using the mamba architecture. To capture the
relationship among RTG-state-action triplets, a fine-grained SSM module is
designed and integrated into the original coarse-grained SSM in mamba,
resulting in a novel mamba architecture tailored for offline RL. Finally, to
mitigate the overfitting issue on noisy trajectories, a self-evolving policy is
proposed by using progressive regularization. The policy evolves by using its
own past knowledge to refine the suboptimal actions, thus enhancing its
robustness on noisy demonstrations. Extensive experiments on various tasks show
that DM outperforms other baselines substantially."
33,"This paper explores using a Long short-term memory (LSTM) based sequence
autoencoder to learn interesting features for detecting surveillance aircraft
using ADS-B flight data. An aircraft periodically broadcasts ADS-B (Automatic
Dependent Surveillance - Broadcast) data to ground receivers. The ability of
LSTM networks to model varying length time series data and remember
dependencies that span across events makes it an ideal candidate for
implementing a sequence autoencoder for ADS-B data because of its possible
variable length time series, irregular sampling and dependencies that span
across events."
34,"Successful analytics solutions that provide valuable insights often hinge on
the connection of various data sources. While it is often feasible to generate
larger data pools within organizations, the application of analytics within
(inter-organizational) business networks is still severely constrained. As data
is distributed across several legal units, potentially even across countries,
the fear of disclosing sensitive information as well as the sheer volume of the
data that would need to be exchanged are key inhibitors for the creation of
effective system-wide solutions -- all while still reaching superior prediction
performance. In this work, we propose a meta machine learning method that deals
with these obstacles to enable comprehensive analyses within a business
network. We follow a design science research approach and evaluate our method
with respect to feasibility and performance in an industrial use case. First,
we show that it is feasible to perform network-wide analyses that preserve data
confidentiality as well as limit data transfer volume. Second, we demonstrate
that our method outperforms a conventional isolated analysis and even gets
close to a (hypothetical) scenario where all data could be shared within the
network. Thus, we provide a fundamental contribution for making business
networks more effective, as we remove a key obstacle to tap the huge potential
of learning from data that is scattered throughout the network."
35,"Using neural networks in practical settings would benefit from the ability of
the networks to learn new tasks throughout their lifetimes without forgetting
the previous tasks. This ability is limited in the current deep neural networks
by a problem called catastrophic forgetting, where training on new tasks tends
to severely degrade performance on previous tasks. One way to lessen the impact
of the forgetting problem is to constrain parameters that are important to
previous tasks to stay close to the optimal parameters. Recently, multiple
competitive approaches for computing the importance of the parameters with
respect to the previous tasks have been presented. In this paper, we propose a
learning to optimize algorithm for mitigating catastrophic forgetting. Instead
of trying to formulate a new constraint function ourselves, we propose to train
another neural network to predict parameter update steps that respect the
importance of parameters to the previous tasks. In the proposed meta-training
scheme, the update predictor is trained to minimize loss on a combination of
current and past tasks. We show experimentally that the proposed approach works
in the continual learning setting."
36,"Classic algorithms and machine learning systems like neural networks are both
abundant in everyday life. While classic computer science algorithms are
suitable for precise execution of exactly defined tasks such as finding the
shortest path in a large graph, neural networks allow learning from data to
predict the most likely answer in more complex tasks such as image
classification, which cannot be reduced to an exact algorithm. To get the best
of both worlds, this thesis explores combining both concepts leading to more
robust, better performing, more interpretable, more computationally efficient,
and more data efficient architectures. The thesis formalizes the idea of
algorithmic supervision, which allows a neural network to learn from or in
conjunction with an algorithm. When integrating an algorithm into a neural
architecture, it is important that the algorithm is differentiable such that
the architecture can be trained end-to-end and gradients can be propagated back
through the algorithm in a meaningful way. To make algorithms differentiable,
this thesis proposes a general method for continuously relaxing algorithms by
perturbing variables and approximating the expectation value in closed form,
i.e., without sampling. In addition, this thesis proposes differentiable
algorithms, such as differentiable sorting networks, differentiable renderers,
and differentiable logic gate networks. Finally, this thesis presents
alternative training strategies for learning with algorithms."
37,"We consider the problem of representing collective behavior of large
populations and predicting the evolution of a population distribution over a
discrete state space. A discrete time mean field game (MFG) is motivated as an
interpretable model founded on game theory for understanding the aggregate
effect of individual actions and predicting the temporal evolution of
population distributions. We achieve a synthesis of MFG and Markov decision
processes (MDP) by showing that a special MFG is reducible to an MDP. This
enables us to broaden the scope of mean field game theory and infer MFG models
of large real-world systems via deep inverse reinforcement learning. Our method
learns both the reward function and forward dynamics of an MFG from real data,
and we report the first empirical test of a mean field game model of a
real-world social media population."
38,"Semi-supervised learning (SSL) plays an increasingly important role in the
big data era because a large number of unlabeled samples can be used
effectively to improve the performance of the classifier. Semi-supervised
support vector machine (S$^3$VM) is one of the most appealing methods for SSL,
but scaling up S$^3$VM for kernel learning is still an open problem. Recently,
a doubly stochastic gradient (DSG) algorithm has been proposed to achieve
efficient and scalable training for kernel methods. However, the algorithm and
theoretical analysis of DSG are developed based on the convexity assumption
which makes them incompetent for non-convex problems such as S$^3$VM. To
address this problem, in this paper, we propose a triply stochastic gradient
algorithm for S$^3$VM, called TSGS$^3$VM. Specifically, to handle two types of
data instances involved in S$^3$VM, TSGS$^3$VM samples a labeled instance and
an unlabeled instance as well with the random features in each iteration to
compute a triply stochastic gradient. We use the approximated gradient to
update the solution. More importantly, we establish new theoretic analysis for
TSGS$^3$VM which guarantees that TSGS$^3$VM can converge to a stationary point.
Extensive experimental results on a variety of datasets demonstrate that
TSGS$^3$VM is much more efficient and scalable than existing S$^3$VM
algorithms."
39,"The mobile gaming industry, particularly the free-to-play sector, has been
around for more than a decade, yet it still experiences rapid growth. The
concept of games-as-service requires game developers to pay much more attention
to recommendations of content in their games. With recommender systems (RS),
the inevitable problem of bias in the data comes hand in hand. A lot of
research has been done on the case of bias in RS for online retail or services,
but much less is available for the specific case of the game industry. Also, in
previous works, various debiasing techniques were tested on explicit feedback
datasets, while it is much more common in mobile gaming data to only have
implicit feedback. This case study aims to identify and categorize potential
bias within datasets specific to model-based recommendations in mobile games,
review debiasing techniques in the existing literature, and assess their
effectiveness on real-world data gathered through implicit feedback. The
effectiveness of these methods is then evaluated based on their debiasing
quality, data requirements, and computational demands."
40,"Clustering and classification critically rely on distance metrics that
provide meaningful comparisons between data points. We present mixed-integer
optimization approaches to find optimal distance metrics that generalize the
Mahalanobis metric extensively studied in the literature. Additionally, we
generalize and improve upon leading methods by removing reliance on
pre-designated ""target neighbors,"" ""triplets,"" and ""similarity pairs."" Another
salient feature of our method is its ability to enable active learning by
recommending precise regions to sample after an optimal metric is computed to
improve classification performance. This targeted acquisition can significantly
reduce computational burden by ensuring training data completeness,
representativeness, and economy. We demonstrate classification and
computational performance of the algorithms through several simple and
intuitive examples, followed by results on real image and medical datasets."
41,"Classifiers that can be implemented on chip with minimal computational and
memory resources are essential for edge computing in emerging applications such
as medical and IoT devices. This paper introduces a machine learning model
based on oblique decision trees to enable resource-efficient classification on
a neural implant. By integrating model compression with probabilistic routing
and implementing cost-aware learning, our proposed model could significantly
reduce the memory and hardware cost compared to state-of-the-art models, while
maintaining the classification accuracy. We trained the resource-efficient
oblique tree with power-efficient regularization (ResOT-PE) on three neural
classification tasks to evaluate the performance, memory, and hardware
requirements. On seizure detection task, we were able to reduce the model size
by 3.4X and the feature extraction cost by 14.6X compared to the ensemble of
boosted trees, using the intracranial EEG from 10 epilepsy patients. In a
second experiment, we tested the ResOT-PE model on tremor detection for
Parkinson's disease, using the local field potentials from 12 patients
implanted with a deep-brain stimulation (DBS) device. We achieved a comparable
classification performance as the state-of-the-art boosted tree ensemble, while
reducing the model size and feature extraction cost by 10.6X and 6.8X,
respectively. We also tested on a 6-class finger movement detection task using
ECoG recordings from 9 subjects, reducing the model size by 17.6X and feature
computation cost by 5.1X. The proposed model can enable a low-power and
memory-efficient implementation of classifiers for real-time neurological
disease detection and motor decoding."
42,"The probabilistic classification vector machine (PCVM) synthesizes the
advantages of both the support vector machine and the relevant vector machine,
delivering a sparse Bayesian solution to classification problems. However, the
PCVM is currently only applicable to binary cases. Extending the PCVM to
multi-class cases via heuristic voting strategies such as one-vs-rest or
one-vs-one often results in a dilemma where classifiers make contradictory
predictions, and those strategies might lose the benefits of probabilistic
outputs. To overcome this problem, we extend the PCVM and propose a multi-class
probabilistic classification vector machine (mPCVM). Two learning algorithms,
i.e., one top-down algorithm and one bottom-up algorithm, have been implemented
in the mPCVM. The top-down algorithm obtains the maximum a posteriori (MAP)
point estimates of the parameters based on an expectation-maximization
algorithm, and the bottom-up algorithm is an incremental paradigm by maximizing
the marginal likelihood. The superior performance of the mPCVMs, especially
when the investigated problem has a large number of classes, is extensively
evaluated on synthetic and benchmark data sets."
43,"We propose a novel output layer activation function, which we name ASTra
(Asymmetric Sigmoid Transfer function), which makes the classification of
minority examples, in scenarios of high imbalance, more tractable. We combine
this with a loss function that helps to effectively target minority
misclassification. These two methods can be used together or separately, with
their combination recommended for the most severely imbalanced cases. The
proposed approach is tested on datasets with IRs from 588.24 to 4000 and very
few minority examples (in some datasets, as few as five). Results using neural
networks with from two to 12 hidden units are demonstrated to be comparable to,
or better than, equivalent results obtained in a recent study that deployed a
wide range of complex, hybrid data-level ensemble classifiers."
44,"Data-driven black-box model-based optimization (MBO) problems arise in a
great number of practical application scenarios, where the goal is to find a
design over the whole space maximizing a black-box target function based on a
static offline dataset. In this work, we consider a more general but
challenging MBO setting, named constrained MBO (CoMBO), where only part of the
design space can be optimized while the rest is constrained by the environment.
A new challenge arising from CoMBO is that most observed designs that satisfy
the constraints are mediocre in evaluation. Therefore, we focus on optimizing
these mediocre designs in the offline dataset while maintaining the given
constraints rather than further boosting the best observed design in the
traditional MBO setting. We propose retrieval-enhanced offline model-based
optimization (ROMO), a new derivable forward approach that retrieves the
offline dataset and aggregates relevant samples to provide a trusted
prediction, and use it for gradient-based optimization. ROMO is simple to
implement and outperforms state-of-the-art approaches in the CoMBO setting.
Empirically, we conduct experiments on a synthetic Hartmann (3D) function
dataset, an industrial CIO dataset, and a suite of modified tasks in the
Design-Bench benchmark. Results show that ROMO performs well in a wide range of
constrained optimization tasks."
45,"Large language models (LLMs) have demonstrated their prowess in generating
synthetic text and images; however, their potential for generating tabular data
-- arguably the most common data type in business and scientific applications
-- is largely underexplored. This paper demonstrates that LLMs, used as-is, or
after traditional fine-tuning, are severely inadequate as synthetic table
generators. Due to the autoregressive nature of LLMs, fine-tuning with random
order permutation runs counter to the importance of modeling functional
dependencies, and renders LLMs unable to model conditional mixtures of
distributions (key to capturing real world constraints). We showcase how LLMs
can be made to overcome some of these deficiencies by making them
permutation-aware."
46,"Solving the Goal-Conditioned Reward Sparse (GCRS) task is a challenging
reinforcement learning problem due to the sparsity of reward signals. In this
work, we propose a new formulation of GCRS tasks from the perspective of the
drifted random walk on the state space, and design a novel method called
Evolutionary Stochastic Policy Distillation (ESPD) to solve them based on the
insight of reducing the First Hitting Time of the stochastic process. As a
self-imitate approach, ESPD enables a target policy to learn from a series of
its stochastic variants through the technique of policy distillation (PD). The
learning mechanism of ESPD can be considered as an Evolution Strategy (ES) that
applies perturbations upon policy directly on the action space, with a SELECT
function to check the superiority of stochastic variants and then use PD to
update the policy. The experiments based on the MuJoCo robotics control suite
show the high learning efficiency of the proposed method."
47,"The focus in machine learning has branched beyond training classifiers on a
single task to investigating how previously acquired knowledge in a source
domain can be leveraged to facilitate learning in a related target domain,
known as inductive transfer learning. Three active lines of research have
independently explored transfer learning using neural networks. In weight
transfer, a model trained on the source domain is used as an initialization
point for a network to be trained on the target domain. In deep metric
learning, the source domain is used to construct an embedding that captures
class structure in both the source and target domains. In few-shot learning,
the focus is on generalizing well in the target domain based on a limited
number of labeled examples. We compare state-of-the-art methods from these
three paradigms and also explore hybrid adapted-embedding methods that use
limited target-domain data to fine tune embeddings constructed from
source-domain data. We conduct a systematic comparison of methods in a variety
of domains, varying the number of labeled instances available in the target
domain ($k$), as well as the number of target-domain classes. We reach three
principal conclusions: (1) Deep embeddings are far superior, compared to weight
transfer, as a starting point for inter-domain transfer or model re-use (2) Our
hybrid methods robustly outperform every few-shot learning and every deep
metric learning method previously proposed, with a mean error reduction of 34%
over state-of-the-art. (3) Among loss functions for discovering embeddings, the
histogram loss (Ustinova & Lempitsky, 2016) is most robust. We hope our results
will motivate a unification of research in weight transfer, deep metric
learning, and few-shot learning."
48,"Mining data streams is a challenge per se. It must be ready to deal with an
enormous amount of data and with problems not present in batch machine
learning, such as concept drift. Therefore, applying a batch-designed
technique, such as dynamic selection of classifiers (DCS) also presents a
challenge. The dynamic characteristic of ensembles that deal with streams
presents barriers to the application of traditional DCS techniques in such
classifiers. scikit-dyn2sel is an open-source python library tailored for
dynamic selection techniques in streaming data. scikit-dyn2sel's development
follows code quality and testing standards, including PEP8 compliance and
automated high test coverage using codecov.io and circleci.com. Source code,
documentation, and examples are made available on GitHub at
https://github.com/luccaportes/Scikit-DYN2SEL."
49,"We describe novel subgradient methods for a broad class of matrix
optimization problems involving nuclear norm regularization. Unlike existing
approaches, our method executes very cheap iterations by combining low-rank
stochastic subgradients with efficient incremental SVD updates, made possible
by highly optimized and parallelizable dense linear algebra operations on small
matrices. Our practical algorithms always maintain a low-rank factorization of
iterates that can be conveniently held in memory and efficiently multiplied to
generate predictions in matrix completion settings. Empirical comparisons
confirm that our approach is highly competitive with several recently proposed
state-of-the-art solvers for such problems."
50,"Semi-supervised multi-label feature selection has recently been developed to
solve the curse of dimensionality problem in high-dimensional multi-label data
with certain samples missing labels. Although many efforts have been made, most
existing methods use a predefined graph approach to capture the sample
similarity or the label correlation. In this manner, the presence of noise and
outliers within the original feature space can undermine the reliability of the
resulting sample similarity graph. It also fails to precisely depict the label
correlation due to the existence of unknown labels. Besides, these methods only
consider the discriminative power of selected features, while neglecting their
redundancy. In this paper, we propose an Adaptive Collaborative Correlation
lEarning-based Semi-Supervised Multi-label Feature Selection (Access-MFS)
method to address these issues. Specifically, a generalized regression model
equipped with an extended uncorrelated constraint is introduced to select
discriminative yet irrelevant features and maintain consistency between
predicted and ground-truth labels in labeled data, simultaneously. Then, the
instance correlation and label correlation are integrated into the proposed
regression model to adaptively learn both the sample similarity graph and the
label similarity graph, which mutually enhance feature selection performance.
Extensive experimental results demonstrate the superiority of the proposed
Access-MFS over other state-of-the-art methods."
51,"Federated learning is a technique that enables a centralized server to learn
from distributed clients via communications without accessing the client local
data. However, existing federated learning works mainly focus on a single task
scenario with static data. In this paper, we introduce the problem of continual
federated learning, where clients incrementally learn new tasks and history
data cannot be stored due to certain reasons, such as limited storage and data
retention policy. Generative replay based methods are effective for continual
learning without storing history data, but adapting them for this setting is
challenging. By analyzing the behaviors of clients during training, we find
that the unstable training process caused by distributed training on non-IID
data leads to a notable performance degradation. To address this problem, we
propose our FedCIL model with two simple but effective solutions: model
consolidation and consistency enforcement. Our experimental results on multiple
benchmark datasets demonstrate that our method significantly outperforms
baselines."
52,"Time Series Motif Discovery (TSMD) refers to the task of identifying patterns
that occur multiple times (possibly with minor variations) in a time series.
All existing methods for TSMD have one or more of the following limitations:
they only look for the two most similar occurrences of a pattern; they only
look for patterns of a pre-specified, fixed length; they cannot handle
variability along the time axis; and they only handle univariate time series.
In this paper, we present a new method, LoCoMotif, that has none of these
limitations. The method is motivated by a concrete use case from physiotherapy.
We demonstrate the value of the proposed method on this use case. We also
introduce a new quantitative evaluation metric for motif discovery, and
benchmark data for comparing TSMD methods. LoCoMotif substantially outperforms
the existing methods, on top of being more broadly applicable."
53,"The dominant paradigm for learning on graph-structured data is message
passing. Despite being a strong inductive bias, the local message passing
mechanism suffers from pathological issues such as over-smoothing,
over-squashing, and limited node-level expressivity. To address these
limitations we propose Bundle Neural Networks (BuNN), a new type of GNN that
operates via message diffusion over flat vector bundles - structures analogous
to connections on Riemannian manifolds that augment the graph by assigning to
each node a vector space and an orthogonal map. A BuNN layer evolves the
features according to a diffusion-type partial differential equation. When
discretized, BuNNs are a special case of Sheaf Neural Networks (SNNs), a
recently proposed MPNN capable of mitigating over-smoothing. The continuous
nature of message diffusion enables BuNNs to operate on larger scales of the
graph and, therefore, to mitigate over-squashing. Finally, we prove that BuNN
can approximate any feature transformation over nodes on any (potentially
infinite) family of graphs given injective positional encodings, resulting in
universal node-level expressivity. We support our theory via synthetic
experiments and showcase the strong empirical performance of BuNNs over a range
of real-world tasks, achieving state-of-the-art results on several standard
benchmarks in transductive and inductive settings."
54,"The message-passing scheme is the core of graph representation learning.
While most existing message-passing graph neural networks (MPNNs) are
permutation-invariant in graph-level representation learning and
permutation-equivariant in node- and edge-level representation learning, their
expressive power is commonly limited by the 1-Weisfeiler-Lehman (1-WL) graph
isomorphism test. Recently proposed expressive graph neural networks (GNNs)
with specially designed complex message-passing mechanisms are not practical.
To bridge the gap, we propose a plug-in Equivariant Distance ENcoding (EDEN)
for MPNNs. EDEN is derived from a series of interpretable transformations on
the graph's distance matrix. We theoretically prove that EDEN is
permutation-equivariant for all level graph representation learning, and we
empirically illustrate that EDEN's expressive power can reach up to the 3-WL
test. Extensive experiments on real-world datasets show that combining EDEN
with conventional GNNs surpasses recent advanced GNNs."
55,"We consider the Domain Adaptation problem, also known as the covariate shift
problem, where the distributions that generate the training and test data
differ while retaining the same labeling function. This problem occurs across a
large range of practical applications, and is related to the more general
challenge of transfer learning. Most recent work on the topic focuses on
optimization techniques that are specific to an algorithm or practical use case
rather than a more general approach. The sparse literature attempting to
provide general bounds seems to suggest that efficient learning even under
strong assumptions is not possible for covariate shift. Our main contribution
is to recontextualize these results by showing that any Probably Approximately
Correct (PAC) learnable concept class is still PAC learnable under covariate
shift conditions with only a polynomial increase in the number of training
samples. This approach essentially demonstrates that the Domain Adaptation
learning problem is as hard as the underlying PAC learning problem, provided
some conditions over the training and test distributions. We also present
bounds for the rejection sampling algorithm, justifying it as a solution to the
Domain Adaptation problem in certain scenarios."
56,"Computational imaging methods that can exploit multiple modalities have the
potential to enhance the capabilities of traditional sensing systems. In this
paper, we propose a new method that reconstructs multimodal images from their
linear measurements by exploiting redundancies across different modalities. Our
method combines a convolutional group-sparse representation of images with
total variation (TV) regularization for high-quality multimodal imaging. We
develop an online algorithm that enables the unsupervised learning of
convolutional dictionaries on large-scale datasets that are typical in such
applications. We illustrate the benefit of our approach in the context of joint
intensity-depth imaging."
57,"Camouflaged object detection (COD) is a challenging task due to the low
boundary contrast between the object and its surroundings. In addition, the
appearance of camouflaged objects varies significantly, e.g., object size and
shape, aggravating the difficulties of accurate COD. In this paper, we propose
a novel Context-aware Cross-level Fusion Network (C2F-Net) to address the
challenging COD task. Specifically, we propose an Attention-induced Cross-level
Fusion Module (ACFM) to integrate the multi-level features with informative
attention coefficients. The fused features are then fed to the proposed
Dual-branch Global Context Module (DGCM), which yields multi-scale feature
representations for exploiting rich global context information. In C2F-Net, the
two modules are conducted on high-level features using a cascaded manner.
Extensive experiments on three widely used benchmark datasets demonstrate that
our C2F-Net is an effective COD model and outperforms state-of-the-art models
remarkably. Our code is publicly available at:
https://github.com/thograce/C2FNet."
58,"Nowadays, driven by the increasing concern on diet and health, food computing
has attracted enormous attention from both industry and research community. One
of the most popular research topics in this domain is Food Retrieval, due to
its profound influence on health-oriented applications. In this paper, we focus
on the task of cross-modal retrieval between food images and cooking recipes.
We present Modality-Consistent Embedding Network (MCEN) that learns
modality-invariant representations by projecting images and texts to the same
embedding space. To capture the latent alignments between modalities, we
incorporate stochastic latent variables to explicitly exploit the interactions
between textual and visual features. Importantly, our method learns the
cross-modal alignments during training but computes embeddings of different
modalities independently at inference time for the sake of efficiency.
Extensive experimental results clearly demonstrate that the proposed MCEN
outperforms all existing approaches on the benchmark Recipe1M dataset and
requires less computational cost."
59,"The ubiquitous multi-camera setup on modern autonomous vehicles provides an
opportunity to construct surround-view depth. Existing methods, however, either
perform independent monocular depth estimations on each camera or rely on
computationally heavy self attention mechanisms. In this paper, we propose a
novel guided attention architecture, EGA-Depth, which can improve both the
efficiency and accuracy of self-supervised multi-camera depth estimation. More
specifically, for each camera, we use its perspective view as the query to
cross-reference its neighboring views to derive informative features for this
camera view. This allows the model to perform attention only across views with
considerable overlaps and avoid the costly computations of standard
self-attention. Given its efficiency, EGA-Depth enables us to exploit
higher-resolution visual features, leading to improved accuracy. Furthermore,
EGA-Depth can incorporate more frames from previous time steps as it scales
linearly w.r.t. the number of views and frames. Extensive experiments on two
challenging autonomous driving benchmarks nuScenes and DDAD demonstrate the
efficacy of our proposed EGA-Depth and show that it achieves the new
state-of-the-art in self-supervised multi-camera depth estimation."
60,"The paper presents a Traffic Sign Recognition (TSR) system, which can fast
and accurately recognize traffic signs of different sizes in images. The system
consists of two well-designed Convolutional Neural Networks (CNNs), one for
region proposals of traffic signs and one for classification of each region. In
the proposal CNN, a Fully Convolutional Network (FCN) with a dual multi-scale
architecture is proposed to achieve scale invariant detection. In training the
proposal network, a modified ""Online Hard Example Mining"" (OHEM) scheme is
adopted to suppress false positives. The classification network fuses
multi-scale features as representation and adopts an ""Inception"" module for
efficiency. We evaluate the proposed TSR system and its components with
extensive experiments. Our method obtains $99.88\%$ precision and $96.61\%$
recall on the Swedish Traffic Signs Dataset (STSD), higher than
state-of-the-art methods. Besides, our system is faster and more lightweight
than state-of-the-art deep learning networks for traffic sign recognition."
61,"Human pose estimation aims to accurately estimate a wide variety of human
poses. However, existing datasets often follow a long-tailed distribution that
unusual poses only occupy a small portion, which further leads to the lack of
diversity of rare poses. These issues result in the inferior generalization
ability of current pose estimators. In this paper, we present a simple yet
effective data augmentation method, termed Pose Transformation (PoseTrans), to
alleviate the aforementioned problems. Specifically, we propose Pose
Transformation Module (PTM) to create new training samples that have diverse
poses and adopt a pose discriminator to ensure the plausibility of the
augmented poses. Besides, we propose Pose Clustering Module (PCM) to measure
the pose rarity and select the ""rarest"" poses to help balance the long-tailed
distribution. Extensive experiments on three benchmark datasets demonstrate the
effectiveness of our method, especially on rare poses. Also, our method is
efficient and simple to implement, which can be easily integrated into the
training pipeline of existing pose estimation models."
62,"We present a convolutional approach to reflection symmetry detection in 2D.
Our model, built on the products of complex-valued wavelet convolutions,
simplifies previous edge-based pairwise methods. Being parameter-centered, as
opposed to feature-centered, it has certain computational advantages when the
object sizes are known a priori, as demonstrated in an ellipse detection
application. The method outperforms the best-performing algorithm on the CVPR
2013 Symmetry Detection Competition Database in the single-symmetry case. Code
and a new database for 2D symmetry detection is available."
63,"Generic visual tracking is difficult due to many challenge factors (e.g.,
occlusion, blur, etc.). Each of these factors may cause serious problems for a
tracking algorithm, and when they work together can make things even more
complicated. Despite a great amount of efforts devoted to understanding the
behavior of tracking algorithms, reliable and quantifiable ways for studying
the per factor tracking behavior remain barely available. Addressing this
issue, in this paper we contribute to the community a tracking diagnosis
toolkit, TracKlinic, for diagnosis of challenge factors of tracking algorithms.
  TracKlinic consists of two novel components focusing on the data and analysis
aspects, respectively. For the data component, we carefully prepare a set of
2,390 annotated videos, each involving one and only one major challenge factor.
When analyzing an algorithm for a specific challenge factor, such
one-factor-per-sequence rule greatly inhibits the disturbance from other
factors and consequently leads to more faithful analysis. For the analysis
component, given the tracking results on all sequences, it investigates the
behavior of the tracker under each individual factor and generates the report
automatically. With TracKlinic, a thorough study is conducted on ten
state-of-the-art trackers on nine challenge factors (including two compound
ones). The results suggest that, heavy shape variation and occlusion are the
two most challenging factors faced by most trackers. Besides, out-of-view,
though does not happen frequently, is often fatal. By sharing TracKlinic, we
expect to make it much easier for diagnosing tracking algorithms, and to thus
facilitate developing better ones."
64,"Recently, directly detecting 3D objects from 3D point clouds has received
increasing attention. To extract object representation from an irregular point
cloud, existing methods usually take a point grouping step to assign the points
to an object candidate so that a PointNet-like network could be used to derive
object features from the grouped points. However, the inaccurate point
assignments caused by the hand-crafted grouping scheme decrease the performance
of 3D object detection.
  In this paper, we present a simple yet effective method for directly
detecting 3D objects from the 3D point cloud. Instead of grouping local points
to each object candidate, our method computes the feature of an object from all
the points in the point cloud with the help of an attention mechanism in the
Transformers \cite{vaswani2017attention}, where the contribution of each point
is automatically learned in the network training. With an improved attention
stacking scheme, our method fuses object features in different stages and
generates more accurate object detection results. With few bells and whistles,
the proposed method achieves state-of-the-art 3D object detection performance
on two widely used benchmarks, ScanNet V2 and SUN RGB-D. The code and models
are publicly available at \url{https://github.com/zeliu98/Group-Free-3D}"
65,"Hyperspectral cameras can provide unique spectral signatures for consistently
distinguishing materials that can be used to solve surveillance tasks. In this
paper, we propose a novel real-time hyperspectral likelihood maps-aided
tracking method (HLT) inspired by an adaptive hyperspectral sensor. A moving
object tracking system generally consists of registration, object detection,
and tracking modules. We focus on the target detection part and remove the
necessity to build any offline classifiers and tune a large amount of
hyperparameters, instead learning a generative target model in an online manner
for hyperspectral channels ranging from visible to infrared wavelengths. The
key idea is that, our adaptive fusion method can combine likelihood maps from
multiple bands of hyperspectral imagery into one single more distinctive
representation increasing the margin between mean value of foreground and
background pixels in the fused map. Experimental results show that the HLT not
only outperforms all established fusion methods but is on par with the current
state-of-the-art hyperspectral target tracking frameworks."
66,"Scarcity of labeled data has motivated the development of semi-supervised
learning methods, which learn from large portions of unlabeled data alongside a
few labeled samples. Consistency Regularization between model's predictions
under different input perturbations, particularly has shown to provide
state-of-the art results in a semi-supervised framework. However, most of these
method have been limited to classification and segmentation applications. We
propose Transformation Consistency Regularization, which delves into a more
challenging setting of image-to-image translation, which remains unexplored by
semi-supervised algorithms. The method introduces a diverse set of geometric
transformations and enforces the model's predictions for unlabeled data to be
invariant to those transformations. We evaluate the efficacy of our algorithm
on three different applications: image colorization, denoising and
super-resolution. Our method is significantly data efficient, requiring only
around 10 - 20% of labeled samples to achieve similar image reconstructions to
its fully-supervised counterpart. Furthermore, we show the effectiveness of our
method in video processing applications, where knowledge from a few frames can
be leveraged to enhance the quality of the rest of the movie."
67,"Structured illumination microscopy (SIM) is a very important super-resolution
microscopy technique, which provides high speed super-resolution with about
two-fold spatial resolution enhancement. Several attempts aimed at improving
the performance of SIM reconstruction algorithm have been reported. However,
most of these highlight only one specific aspect of the SIM reconstruction --
such as the determination of the illumination pattern phase shift accurately --
whereas other key elements -- such as determination of modulation factor,
estimation of object power spectrum, Wiener filtering frequency components with
inclusion of object power spectrum information, translocating and the merging
of the overlapping frequency components -- are usually glossed over
superficially. In addition, most of the work reported lie scattered throughout
the literature and a comprehensive review of the theoretical background is
found lacking. The purpose of the present work is two-fold: 1) to collect the
essential theoretical details of SIM algorithm at one place, thereby making
them readily accessible to readers for the first time; and 2) to provide an
open source SIM reconstruction code (named OpenSIM), which enables users to
interactively vary the code parameters and study it's effect on reconstructed
SIM image."
68,"Current talking face generation methods mainly focus on speech-lip
synchronization. However, insufficient investigation on the facial talking
style leads to a lifeless and monotonous avatar. Most previous works fail to
imitate expressive styles from arbitrary video prompts and ensure the
authenticity of the generated video. This paper proposes an unsupervised
variational style transfer model (VAST) to vivify the neutral photo-realistic
avatars. Our model consists of three key components: a style encoder that
extracts facial style representations from the given video prompts; a hybrid
facial expression decoder to model accurate speech-related movements; a
variational style enhancer that enhances the style space to be highly
expressive and meaningful. With our essential designs on facial style learning,
our model is able to flexibly capture the expressive facial style from
arbitrary video prompts and transfer it onto a personalized image renderer in a
zero-shot manner. Experimental results demonstrate the proposed approach
contributes to a more vivid talking avatar with higher authenticity and richer
expressiveness."
69,"In this work, we propose to detect the iris and periocular regions
simultaneously using coarse annotations and two well-known object detectors:
YOLOv2 and Faster R-CNN. We believe coarse annotations can be used in
recognition systems based on the iris and periocular regions, given the much
smaller engineering effort required to manually annotate the training images.
We manually made coarse annotations of the iris and periocular regions (122K
images from the visible (VIS) spectrum and 38K images from the near-infrared
(NIR) spectrum). The iris annotations in the NIR databases were generated
semi-automatically by first applying an iris segmentation CNN and then
performing a manual inspection. These annotations were made for 11 well-known
public databases (3 NIR and 8 VIS) designed for the iris-based recognition
problem and are publicly available to the research community. Experimenting our
proposal on these databases, we highlight two results. First, the Faster R-CNN
+ Feature Pyramid Network (FPN) model reported an Intersection over Union (IoU)
higher than YOLOv2 (91.86% vs 85.30%). Second, the detection of the iris and
periocular regions being performed simultaneously is as accurate as performed
separately, but with a lower computational cost, i.e., two tasks were carried
out at the cost of one."
70,"We tackle the problem of getting a full 6-DOF pose estimation of a query
image inside a given point cloud. This technical report re-evaluates the
algorithms proposed by Y. Li et al. ""Worldwide Pose Estimation using 3D Point
Cloud"". Our code computes poses from 3 or 4 points, with both known and unknown
focal length. The results can easily be displayed and analyzed with Meshlab. We
found both advantages and shortcomings of the methods proposed. Furthermore,
additional priors and parameters for point selection, RANSAC and pose quality
estimate (inlier test) are proposed and applied."
71,"Recently, there has been a growing interest in constructing deep learning
schemes for Low-Light Vision (LLV). Existing techniques primarily focus on
designing task-specific and data-dependent vision models on the standard RGB
domain, which inherently contain latent data associations. In this study, we
propose a generic low-light vision solution by introducing a generative block
to convert data from the RAW to the RGB domain. This novel approach connects
diverse vision problems by explicitly depicting data generation, which is the
first in the field. To precisely characterize the latent correspondence between
the generative procedure and the vision task, we establish a bilevel model with
the parameters of the generative block defined as the upper level and the
parameters of the vision task defined as the lower level. We further develop
two types of learning strategies targeting different goals, namely low cost and
high accuracy, to acquire a new bilevel generative learning paradigm. The
generative blocks embrace a strong generalization ability in other low-light
vision tasks through the bilevel optimization on enhancement tasks. Extensive
experimental evaluations on three representative low-light vision tasks, namely
enhancement, detection, and segmentation, fully demonstrate the superiority of
our proposed approach. The code will be available at
https://github.com/Yingchi1998/BGL."
72,"Memorability is considered to be an important characteristic of visual
content, whereas for advertisement and educational purposes it is often
crucial. Despite numerous studies on understanding and predicting image
memorability, there are almost no achievements in memorability modification. In
this work, we study two approaches to image editing - GAN and classical image
processing - and show their impact on memorability. The visual features which
influence memorability directly stay unknown till now, hence it is impossible
to control it manually. As a solution, we let GAN learn it deeply using labeled
data, and then use it for conditional generation of new images. By analogy with
algorithms which edit facial attributes, we consider memorability as yet
another attribute and operate with it in the same way. Obtained data is also
interesting for analysis, simply because there are no real-world examples of
successful change of image memorability while preserving its other attributes.
We believe this may give many new answers to the question ""what makes an image
memorable?"" Apart from that we also study the influence of conventional
photo-editing tools (Photoshop, Instagram, etc.) used daily by a wide audience
on memorability. In this case, we start from real practical methods and study
it using statistics and recent advances in memorability prediction.
Photographers, designers, and advertisers will benefit from the results of this
study directly."
73,"In this paper, we propose a pipeline for multi-target visual tracking under
multi-camera system. For multi-camera system tracking problem, efficient data
association across cameras, and at the same time, across frames becomes more
important than single-camera system tracking. However, most of the multi-camera
tracking algorithms emphasis on single camera across frame data association.
Thus in our work, we model our tracking problem as a global graph, and adopt
Generalized Maximum Multi Clique optimization problem as our core algorithm to
take both across frame and across camera data correlation into account all
together. Furthermore, in order to compute good similarity scores as the input
of our graph model, we extract both appearance and dynamic motion similarities.
For appearance feature, Local Maximal Occurrence Representation(LOMO) feature
extraction algorithm for ReID is conducted. When it comes to capturing the
dynamic information, we build Hankel matrix for each tracklet of target and
apply rank estimation with Iterative Hankel Total Least Squares(IHTLS)
algorithm to it. We evaluate our tracker on the challenging Terrace Sequences
from EPFL CVLAB as well as recently published Duke MTMC dataset."
74,"In this paper, we aim to segment an image degraded by blur and Poisson noise.
We adopt a smoothing-and-thresholding (SaT) segmentation framework that finds a
piecewise-smooth solution, followed by $k$-means clustering to segment the
image. Specifically for the image smoothing step, we replace the least-squares
fidelity for Gaussian noise in the Mumford-Shah model with a maximum posterior
(MAP) term to deal with Poisson noise and we incorporate the weighted
difference of anisotropic and isotropic total variation (AITV) as a
regularization to promote the sparsity of image gradients. For such a nonconvex
model, we develop a specific splitting scheme and utilize a proximal operator
to apply the alternating direction method of multipliers (ADMM). Convergence
analysis is provided to validate the efficacy of the ADMM scheme. Numerical
experiments on various segmentation scenarios (grayscale/color and multiphase)
showcase that our proposed method outperforms a number of segmentation methods,
including the original SaT."
75,"The success and generalisation of deep learning algorithms heavily depend on
learning good feature representations. In medical imaging this entails
representing anatomical information, as well as properties related to the
specific imaging setting. Anatomical information is required to perform further
analysis, whereas imaging information is key to disentangle scanner variability
and potential artefacts. The ability to factorise these would allow for
training algorithms only on the relevant information according to the task. To
date, such factorisation has not been attempted. In this paper, we propose a
methodology of latent space factorisation relying on the cycle-consistency
principle. As an example application, we consider cardiac MR segmentation,
where we separate information related to the myocardium from other features
related to imaging and surrounding substructures. We demonstrate the proposed
method's utility in a semi-supervised setting: we use very few labelled images
together with many unlabelled images to train a myocardium segmentation neural
network. Specifically, we achieve comparable performance to fully supervised
networks using a fraction of labelled images in experiments on ACDC and a
dataset from Edinburgh Imaging Facility QMRI. Code will be made available at
https://github.com/agis85/spatial_factorisation."
76,"Deep learning has shown state-of-art classification performance on datasets
such as ImageNet, which contain a single object in each image. However,
multi-object classification is far more challenging. We present a unified
framework which leverages the strengths of multiple machine learning methods,
viz deep learning, probabilistic models and kernel methods to obtain
state-of-art performance on Microsoft COCO, consisting of non-iconic images. We
incorporate contextual information in natural images through a conditional
latent tree probabilistic model (CLTM), where the object co-occurrences are
conditioned on the extracted fc7 features from pre-trained Imagenet CNN as
input. We learn the CLTM tree structure using conditional pairwise
probabilities for object co-occurrences, estimated through kernel methods, and
we learn its node and edge potentials by training a new 3-layer neural network,
which takes fc7 features as input. Object classification is carried out via
inference on the learnt conditional tree model, and we obtain significant gain
in precision-recall and F-measures on MS-COCO, especially for difficult object
categories. Moreover, the latent variables in the CLTM capture scene
information: the images with top activations for a latent node have common
themes such as being a grasslands or a food scene, and on on. In addition, we
show that a simple k-means clustering of the inferred latent nodes alone
significantly improves scene classification performance on the MIT-Indoor
dataset, without the need for any retraining, and without using scene labels
during training. Thus, we present a unified framework for multi-object
classification and unsupervised scene understanding."
77,"Pedestrian tracking has long been considered an important problem, especially
in security applications. Previously,many approaches have been proposed with
various types of sensors. One popular method is Pedestrian Dead Reckoning(PDR)
[1] which is based on the inertial measurement unit(IMU) sensor. However PDR is
an integration and threshold based method, which suffers from accumulation
errors and low accuracy. In this paper, we propose a novel method in which the
sensor data is fed into a deep learning model to predict the displacements and
orientations of the pedestrian. We also devise a new apparatus to collect and
construct databases containing synchronized IMU sensor data and precise
locations measured by a LIDAR. The preliminary results are promising, and we
plan to push this forward by collecting more data and adapting the deep
learning model for all general pedestrian motions."
78,"Image to image translation is the problem of transferring an image from a
source domain to a different (but related) target domain. We present a new
unsupervised image to image translation technique that leverages the underlying
semantic information for object transfiguration and domain transfer tasks.
Specifically, we present a generative adversarial learning approach that
jointly translates images and labels from a source domain to a target domain.
Our main technical contribution is an encoder-decoder based network
architecture that jointly encodes the image and its underlying semantics and
translates both individually to the target domain. Additionally, we propose
object transfiguration and cross-domain semantic consistency losses that
preserve semantic labels. Through extensive experimental evaluation, we
demonstrate the effectiveness of our approach as compared to the
state-of-the-art methods on unsupervised image-to-image translation, domain
adaptation, and object transfiguration."
79,"We address the problem of Visual Question Answering (VQA), which requires
joint image and language understanding to answer a question about a given
photograph. Recent approaches have applied deep image captioning methods based
on convolutional-recurrent networks to this problem, but have failed to model
spatial inference. To remedy this, we propose a model we call the Spatial
Memory Network and apply it to the VQA task. Memory networks are recurrent
neural networks with an explicit attention mechanism that selects certain parts
of the information stored in memory. Our Spatial Memory Network stores neuron
activations from different spatial regions of the image in its memory, and uses
the question to choose relevant regions for computing the answer, a process of
which constitutes a single ""hop"" in the network. We propose a novel spatial
attention architecture that aligns words with image patches in the first hop,
and obtain improved results by adding a second attention hop which considers
the whole question to choose visual evidence based on the results of the first
hop. To better understand the inference process learned by the network, we
design synthetic questions that specifically require spatial inference and
visualize the attention weights. We evaluate our model on two published visual
question answering datasets, DAQUAR [1] and VQA [2], and obtain improved
results compared to a strong deep baseline model (iBOWIMG) which concatenates
image and question features to predict the answer [3]."
80,"In computed tomography (CT), metal implants increase the inconsistencies
between the measured data and the linear attenuation assumption made by
analytic CT reconstruction algorithms. The inconsistencies give rise to dark
and bright bands and streaks in the reconstructed image, collectively called
metal artifacts. These artifacts make it difficult for radiologists to render
correct diagnostic decisions. We describe a data-driven metal artifact
reduction (MAR) algorithm for image-guided spine surgery that applies to
scenarios in which a prior CT scan of the patient is available. We tested the
proposed method with two clinical datasets that were both obtained during spine
surgery. Using the proposed method, we were not only able to remove the dark
and bright streaks caused by the implanted screws but we also recovered the
anatomical structures hidden by these artifacts. This results in an improved
capability of surgeons to confirm the correctness of the implanted pedicle
screw placements."
81,"We propose a filtering feature selection framework that considers subsets of
features as paths in a graph, where a node is a feature and an edge indicates
pairwise (customizable) relations among features, dealing with relevance and
redundancy principles. By two different interpretations (exploiting properties
of power series of matrices and relying on Markov chains fundamentals) we can
evaluate the values of paths (i.e., feature subsets) of arbitrary lengths,
eventually go to infinite, from which we dub our framework Infinite Feature
Selection (Inf-FS). Going to infinite allows to constrain the computational
complexity of the selection process, and to rank the features in an elegant
way, that is, considering the value of any path (subset) containing a
particular feature. We also propose a simple unsupervised strategy to cut the
ranking, so providing the subset of features to keep. In the experiments, we
analyze diverse settings with heterogeneous features, for a total of 11
benchmarks, comparing against 18 widely-known comparative approaches. The
results show that Inf-FS behaves better in almost any situation, that is, when
the number of features to keep are fixed a priori, or when the decision of the
subset cardinality is part of the process."
82,"Global optimization algorithms have shown impressive performance in
data-association based multi-object tracking, but handling online data remains
a difficult hurdle to overcome. In this paper, we present a hybrid data
association framework with a min-cost multi-commodity network flow for robust
online multi-object tracking. We build local target-specific models interleaved
with global optimization of the optimal data association over multiple video
frames. More specifically, in the min-cost multi-commodity network flow, the
target-specific similarities are online learned to enforce the local
consistency for reducing the complexity of the global data association.
Meanwhile, the global data association taking multiple video frames into
account alleviates irrecoverable errors caused by the local data association
between adjacent frames. To ensure the efficiency of online tracking, we give
an efficient near-optimal solution to the proposed min-cost multi-commodity
flow problem, and provide the empirical proof of its sub-optimality. The
comprehensive experiments on real data demonstrate the superior tracking
performance of our approach in various challenging situations."
83,"Pose variation is one of the key factors which prevents the network from
learning a robust person re-identification (Re-ID) model. To address this
issue, we propose a novel person pose-guided image generation method, which is
called the semantic attention network. The network consists of several semantic
attention blocks, where each block attends to preserve and update the pose code
and the clothing textures. The introduction of the binary segmentation mask and
the semantic parsing is important for seamlessly stitching foreground and
background in the pose-guided image generation. Compared with other methods,
our network can characterize better body shape and keep clothing attributes,
simultaneously. Our synthesized image can obtain better appearance and shape
consistency related to the original image. Experimental results show that our
approach is competitive with respect to both quantitative and qualitative
results on Market-1501 and DeepFashion. Furthermore, we conduct extensive
evaluations by using person re-identification (Re-ID) systems trained with the
pose-transferred person based augmented data. The experiment shows that our
approach can significantly enhance the person Re-ID accuracy."
84,"Video generation is an inherently challenging task, as it requires modeling
realistic temporal dynamics as well as spatial content. Existing methods
entangle the two intrinsically different tasks of motion and content creation
in a single generator network, but this approach struggles to simultaneously
generate plausible motion and content. To im-prove motion modeling in video
generation tasks, we propose a two-stream model that disentangles motion
generation from content generation, called a Two-Stream Variational Adversarial
Network (TwoStreamVAN). Given an action label and a noise vector, our model is
able to create clear and consistent motion, and thus yields photorealistic
videos. The key idea is to progressively generate and fuse multi-scale motion
with its corresponding spatial content. Our model significantly outperforms
existing methods on the standard Weizmann Human Action, MUG Facial Expression,
and VoxCeleb datasets, as well as our new dataset of diverse human actions with
challenging and complex motion. Our code is available at
https://github.com/sunxm2357/TwoStreamVAN/."
85,"Pavement damage segmentation has benefited enormously from deep learning. %
and large-scale datasets. However, few current public datasets limit the
potential exploration of deep learning in the application of pavement damage
segmentation. To address this problem, this study has proposed Pavementscapes,
a large-scale dataset to develop and evaluate methods for pavement damage
segmentation. Pavementscapes is comprised of 4,000 images with a resolution of
$1024 \times 2048$, which have been recorded in the real-world pavement
inspection projects with 15 different pavements. A total of 8,680 damage
instances are manually labeled with six damage classes at the pixel level. The
statistical study gives a thorough investigation and analysis of the proposed
dataset. The numeral experiments propose the top-performing deep neural
networks capable of segmenting pavement damages, which provides the baselines
of the open challenge for pavement inspection. The experiment results also
indicate the existing problems for damage segmentation using deep learning, and
this study provides potential solutions."
86,"The fusion techniques that utilize multiple feature sets to form new features
that are often more robust and contain useful information for future processing
are referred to as feature fusion. The term data fusion is applied to the class
of techniques used for combining decisions obtained from multiple feature sets
to form global decisions. Feature and data fusion interchangeably represent two
important classes of techniques that have proved to be of practical importance
in a wide range of medical imaging problems"
87,"This paper introduces a novel methodology that combines the multi-resolution
feature of the Gabor wavelet transformation (GWT) with the local interactions
of the facial structures expressed through the Pseudo Hidden Markov model
(PHMM). Unlike the traditional zigzag scanning method for feature extraction a
continuous scanning method from top-left corner to right then top-down and
right to left and so on until right-bottom of the image i.e. a spiral scanning
technique has been proposed for better feature selection. Unlike traditional
HMMs, the proposed PHMM does not perform the state conditional independence of
the visible observation sequence assumption. This is achieved via the concept
of local structures introduced by the PHMM used to extract facial bands and
automatically select the most informative features of a face image. Thus, the
long-range dependency problem inherent to traditional HMMs has been drastically
reduced. Again with the use of most informative pixels rather than the whole
image makes the proposed method reasonably faster for face recognition. This
method has been successfully tested on frontal face images from the ORL, FRAV2D
and FERET face databases where the images vary in pose, illumination,
expression, and scale. The FERET data set contains 2200 frontal face images of
200 subjects, while the FRAV2D data set consists of 1100 images of 100 subjects
and the full ORL database is considered. The results reported in this
application are far better than the recent and most referred systems."
88,"Learning transformation invariant representations of visual data is an
important problem in computer vision. Deep convolutional networks have
demonstrated remarkable results for image and video classification tasks.
However, they have achieved only limited success in the classification of
images that undergo geometric transformations. In this work we present a novel
Transformation Invariant Graph-based Network (TIGraNet), which learns
graph-based features that are inherently invariant to isometric transformations
such as rotation and translation of input images. In particular, images are
represented as signals on graphs, which permits to replace classical
convolution and pooling layers in deep networks with graph spectral convolution
and dynamic graph pooling layers that together contribute to invariance to
isometric transformation. Our experiments show high performance on rotated and
translated images from the test set compared to classical architectures that
are very sensitive to transformations in the data. The inherent invariance
properties of our framework provide key advantages, such as increased
resiliency to data variability and sustained performance with limited training
sets. Our code is available online."
89,"Conventional object detection methods essentially suppose that the training
and testing data are collected from a restricted target domain with expensive
labeling cost. For alleviating the problem of domain dependency and cumbersome
labeling, this paper proposes to detect objects in an unrestricted environment
by leveraging domain knowledge trained from an auxiliary source domain with
sufficient labels. Specifically, we propose a multi-adversarial Faster-RCNN
(MAF) framework for unrestricted object detection, which inherently addresses
domain disparity minimization for domain adaptation in feature representation.
The paper merits are in three-fold: 1) With the idea that object detectors
often becomes domain incompatible when image distribution resulted domain
disparity appears, we propose a hierarchical domain feature alignment module,
in which multiple adversarial domain classifier submodules for layer-wise
domain feature confusion are designed; 2) An information invariant scale
reduction module (SRM) for hierarchical feature map resizing is proposed for
promoting the training efficiency of adversarial domain adaptation; 3) In order
to improve the domain adaptability, the aggregated proposal features with
detection results are feed into a proposed weighted gradient reversal layer
(WGRL) for characterizing hard confused domain samples. We evaluate our MAF on
unrestricted tasks, including Cityscapes, KITTI, Sim10k, etc. and the
experiments show the state-of-the-art performance over the existing detectors."
90,"Remote sensing image scene classification plays an important role in a wide
range of applications and hence has been receiving remarkable attention. During
the past years, significant efforts have been made to develop various datasets
or present a variety of approaches for scene classification from remote sensing
images. However, a systematic review of the literature concerning datasets and
methods for scene classification is still lacking. In addition, almost all
existing datasets have a number of limitations, including the small scale of
scene classes and the image numbers, the lack of image variations and
diversity, and the saturation of accuracy. These limitations severely limit the
development of new approaches especially deep learning-based methods. This
paper first provides a comprehensive review of the recent progress. Then, we
propose a large-scale dataset, termed ""NWPU-RESISC45"", which is a publicly
available benchmark for REmote Sensing Image Scene Classification (RESISC),
created by Northwestern Polytechnical University (NWPU). This dataset contains
31,500 images, covering 45 scene classes with 700 images in each class. The
proposed NWPU-RESISC45 (i) is large-scale on the scene classes and the total
image number, (ii) holds big variations in translation, spatial resolution,
viewpoint, object pose, illumination, background, and occlusion, and (iii) has
high within-class diversity and between-class similarity. The creation of this
dataset will enable the community to develop and evaluate various data-driven
algorithms. Finally, several representative methods are evaluated using the
proposed dataset and the results are reported as a useful baseline for future
research."
91,"We propose a training and evaluation approach for autoencoder Generative
Adversarial Networks (GANs), specifically the Boundary Equilibrium Generative
Adversarial Network (BEGAN), based on methods from the image quality assessment
literature. Our approach explores a multidimensional evaluation criterion that
utilizes three distance functions: an $l_1$ score, the Gradient Magnitude
Similarity Mean (GMSM) score, and a chrominance score. We show that each of the
different distance functions captures a slightly different set of properties in
image space and, consequently, requires its own evaluation criterion to
properly assess whether the relevant property has been adequately learned. We
show that models using the new distance functions are able to produce better
images than the original BEGAN model in predicted ways."
92,"Sign Language Recognition has emerged as one of the important area of
research in Computer Vision. The difficulty faced by the researchers is that
the instances of signs vary with both motion and appearance. Thus, in this
paper a novel approach for recognizing various alphabets of Indian Sign
Language is proposed where continuous video sequences of the signs have been
considered. The proposed system comprises of three stages: Preprocessing stage,
Feature Extraction and Classification. Preprocessing stage includes skin
filtering, histogram matching. Eigen values and Eigen Vectors were considered
for feature extraction stage and finally Eigen value weighted Euclidean
distance is used to recognize the sign. It deals with bare hands, thus allowing
the user to interact with the system in natural way. We have considered 24
different alphabets in the video sequences and attained a success rate of
96.25%."
93,"A common strategy for improving model robustness is through data
augmentations. Data augmentations encourage models to learn desired
invariances, such as invariance to horizontal flipping or small changes in
color. Recent work has shown that arbitrary style transfer can be used as a
form of data augmentation to encourage invariance to textures by creating
painting-like images from photographs. However, a stylized photograph is not
quite the same as an artist-created painting. Artists depict perceptually
meaningful cues in paintings so that humans can recognize salient components in
scenes, an emphasis which is not enforced in style transfer. Therefore, we
study how style transfer and paintings differ in their impact on model
robustness. First, we investigate the role of paintings as style images for
stylization-based data augmentation. We find that style transfer functions well
even without paintings as style images. Second, we show that learning from
paintings as a form of perceptual data augmentation can improve model
robustness. Finally, we investigate the invariances learned from stylization
and from paintings, and show that models learn different invariances from these
differing forms of data. Our results provide insights into how stylization
improves model robustness, and provide evidence that artist-created paintings
can be a valuable source of data for model robustness."
94,"The non-stationary nature of image characteristics calls for adaptive
processing, based on the local image content. We propose a simple and flexible
method to learn local tuning of parameters in adaptive image processing: we
extract simple local features from an image and learn the relation between
these features and the optimal filtering parameters. Learning is performed by
optimizing a user defined cost function (any image quality metric) on a
training set. We apply our method to three classical problems (denoising,
demosaicing and deblurring) and we show the effectiveness of the learned
parameter modulation strategies. We also show that these strategies are
consistent with theoretical results from the literature."
95,"Data of different modalities generally convey complimentary but heterogeneous
information, and a more discriminative representation is often preferred by
combining multiple data modalities like the RGB and infrared features. However
in reality, obtaining both data channels is challenging due to many
limitations. For example, the RGB surveillance cameras are often restricted
from private spaces, which is in conflict with the need of abnormal activity
detection for personal security. As a result, using partial data channels to
build a full representation of multi-modalities is clearly desired. In this
paper, we propose a novel Partial-modal Generative Adversarial Networks
(PM-GANs) that learns a full-modal representation using data from only partial
modalities. The full representation is achieved by a generated representation
in place of the missing data channel. Extensive experiments are conducted to
verify the performance of our proposed method on action recognition, compared
with four state-of-the-art methods. Meanwhile, a new Infrared-Visible Dataset
for action recognition is introduced, and will be the first publicly available
action dataset that contains paired infrared and visible spectrum."
96,"Medical image segmentation has made significant progress in recent years.
Deep learning-based methods are recognized as data-hungry techniques, requiring
large amounts of data with manual annotations. However, manual annotation is
expensive in the field of medical image analysis, which requires
domain-specific expertise. To address this challenge, few-shot learning has the
potential to learn new classes from only a few examples. In this work, we
propose a novel framework for few-shot medical image segmentation, termed
CAT-Net, based on cross masked attention Transformer. Our proposed network
mines the correlations between the support image and query image, limiting them
to focus only on useful foreground information and boosting the representation
capacity of both the support prototype and query features. We further design an
iterative refinement framework that refines the query image segmentation
iteratively and promotes the support feature in turn. We validated the proposed
method on three public datasets: Abd-CT, Abd-MRI, and Card-MRI. Experimental
results demonstrate the superior performance of our method compared to
state-of-the-art methods and the effectiveness of each component. Code:
https://github.com/hust-linyi/CAT-Net."
97,"Recently, contrastive self-supervised learning has become a key component for
learning visual representations across many computer vision tasks and
benchmarks. However, contrastive learning in the context of domain adaptation
remains largely underexplored. In this paper, we propose to extend contrastive
learning to a new domain adaptation setting, a particular situation occurring
where the similarity is learned and deployed on samples following different
probability distributions without access to labels. Contrastive learning learns
by comparing and contrasting positive and negative pairs of samples in an
unsupervised setting without access to source and target labels. We have
developed a variation of a recently proposed contrastive learning framework
that helps tackle the domain adaptation problem, further identifying and
removing possible negatives similar to the anchor to mitigate the effects of
false negatives. Extensive experiments demonstrate that the proposed method
adapts well, and improves the performance on the downstream domain adaptation
task."
98,"We present a simple and effective deep convolutional neural network (CNN)
model for video deblurring. The proposed algorithm mainly consists of optical
flow estimation from intermediate latent frames and latent frame restoration
steps. It first develops a deep CNN model to estimate optical flow from
intermediate latent frames and then restores the latent frames based on the
estimated optical flow. To better explore the temporal information from videos,
we develop a temporal sharpness prior to constrain the deep CNN model to help
the latent frame restoration. We develop an effective cascaded training
approach and jointly train the proposed CNN model in an end-to-end manner. We
show that exploring the domain knowledge of video deblurring is able to make
the deep CNN model more compact and efficient. Extensive experimental results
show that the proposed algorithm performs favorably against state-of-the-art
methods on the benchmark datasets as well as real-world videos."
99,"In this paper, we study a novel problem in egocentric action recognition,
which we term as ""Multimodal Generalization"" (MMG). MMG aims to study how
systems can generalize when data from certain modalities is limited or even
completely missing. We thoroughly investigate MMG in the context of standard
supervised action recognition and the more challenging few-shot setting for
learning new action categories. MMG consists of two novel scenarios, designed
to support security, and efficiency considerations in real-world applications:
(1) missing modality generalization where some modalities that were present
during the train time are missing during the inference time, and (2)
cross-modal zero-shot generalization, where the modalities present during the
inference time and the training time are disjoint. To enable this
investigation, we construct a new dataset MMG-Ego4D containing data points with
video, audio, and inertial motion sensor (IMU) modalities. Our dataset is
derived from Ego4D dataset, but processed and thoroughly re-annotated by human
experts to facilitate research in the MMG problem. We evaluate a diverse array
of models on MMG-Ego4D and propose new methods with improved generalization
ability. In particular, we introduce a new fusion module with modality dropout
training, contrastive-based alignment training, and a novel cross-modal
prototypical loss for better few-shot performance. We hope this study will
serve as a benchmark and guide future research in multimodal generalization
problems. The benchmark and code will be available at
https://github.com/facebookresearch/MMG_Ego4D."
100,"Unsupervised learning of visual similarities is of paramount importance to
computer vision, particularly due to lacking training data for fine-grained
similarities. Deep learning of similarities is often based on relationships
between pairs or triplets of samples. Many of these relations are unreliable
and mutually contradicting, implying inconsistencies when trained without
supervision information that relates different tuples or triplets to each
other. To overcome this problem, we use local estimates of reliable
(dis-)similarities to initially group samples into compact surrogate classes
and use local partial orders of samples to classes to link classes to each
other. Similarity learning is then formulated as a partial ordering task with
soft correspondences of all samples to classes. Adopting a strategy of
self-supervision, a CNN is trained to optimally represent samples in a mutually
consistent manner while updating the classes. The similarity learning and
grouping procedure are integrated in a single model and optimized jointly. The
proposed unsupervised approach shows competitive performance on detailed pose
estimation and object classification."
101,"In this paper, we propose a lossless data hiding scheme in JPEG images. After
quantified DCT transform, coefficients have characteristics that distribution
in high frequencies is relatively sparse and absolute values are small. To
improve encoding efficiency, we put forward an encoding algorithm that searches
for a high frequency as terminate point and recode the coefficients above, so
spare space is reserved to embed secret data and appended data with no file
expansion. Receiver can obtain terminate point through data analysis, extract
additional data and recover original JPEG images lossless. Experimental results
show that the proposed method has a larger capacity than state-of-the-art
works."
102,"3D reconstruction of depth and motion from monocular video in dynamic
environments is a highly ill-posed problem due to scale ambiguities when
projecting to the 2D image domain. In this work, we investigate the performance
of the current State-of-the-Art (SotA) deep multi-view systems in such
environments. We find that current supervised methods work surprisingly well
despite not modelling individual object motions, but make systematic errors due
to a lack of dense ground truth data. To detect such errors during usage, we
extend the cost volume based Deep Video to Depth (DeepV2D) framework
\cite{teed2018deepv2d} with a learned uncertainty. Our Deep Video to certain
Depth (DeepV2cD) model allows i) to perform en par or better with current SotA
and ii) achieve a better uncertainty measure than the naive Shannon entropy.
Our experiments show that a simple filter strategy based on the uncertainty can
significantly reduce systematic errors. This results in cleaner reconstructions
both on static and dynamic parts of the scene."
103,"Multiview super-resolution image reconstruction (SRIR) is often cast as a
resampling problem by merging non-redundant data from multiple low-resolution
(LR) images on a finer high-resolution (HR) grid, while inverting the effect of
the camera point spread function (PSF). One main problem with multiview methods
is that resampling from nonuniform samples (provided by LR images) and the
inversion of the PSF are highly nonlinear and ill-posed problems. Non-linearity
and ill-posedness are typically overcome by linearization and regularization,
often through an iterative optimization process, which essentially trade off
the very same information (i.e. high frequency) that we want to recover. We
propose a novel point of view for multiview SRIR: Unlike existing multiview
methods that reconstruct the entire spectrum of the HR image from the multiple
given LR images, we derive explicit expressions that show how the
high-frequency spectra of the unknown HR image are related to the spectra of
the LR images. Therefore, by taking any of the LR images as the reference to
represent the low-frequency spectra of the HR image, one can reconstruct the
super-resolution image by focusing only on the reconstruction of the
high-frequency spectra. This is very much like single-image methods, which
extrapolate the spectrum of one image, except that we rely on information
provided by all other views, rather than by prior constraints as in
single-image methods (which may not be an accurate source of information). This
is made possible by deriving and applying explicit closed-form expressions that
define how the local high frequency information that we aim to recover for the
reference high resolution image is related to the local low frequency
information in the sequence of views. Results and comparisons with recently
published state-of-the-art methods show the superiority of the proposed
solution."
104,"In 3D shape recognition, multi-view based methods leverage human's
perspective to analyze 3D shapes and have achieved significant outcomes. Most
existing research works in deep learning adopt handcrafted networks as
backbones due to their high capacity of feature extraction, and also benefit
from ImageNet pretraining. However, whether these network architectures are
suitable for 3D analysis or not remains unclear. In this paper, we propose a
neural architecture search method named Auto-MVCNN which is particularly
designed for optimizing architecture in multi-view 3D shape recognition.
Auto-MVCNN extends gradient-based frameworks to process multi-view images, by
automatically searching the fusion cell to explore intrinsic correlation among
view features. Moreover, we develop an end-to-end scheme to enhance retrieval
performance through the trade-off parameter search. Extensive experimental
results show that the searched architectures significantly outperform manually
designed counterparts in various aspects, and our method achieves
state-of-the-art performance at the same time."
105,"In this paper, we present a new approach to estimate the layout of a room
from its single image. While recent approaches for this task use robust
features learnt from data, they resort to optimization for detecting the final
layout. In addition to using learnt robust features, our approach learns an
additional ranking function to estimate the final layout instead of using
optimization. To learn this ranking function, we propose a framework to train a
CNN using max-margin structure cost. Also, while most approaches aim at
detecting cuboidal layouts, our approach detects non-cuboidal layouts for which
we explicitly estimates layout complexity parameters. We use these parameters
to propose layout candidates in a novel way. Our approach shows
state-of-the-art results on standard datasets with mostly cuboidal layouts and
also performs well on a dataset containing rooms with non-cuboidal layouts."
106,"Learning discriminative representation using large-scale face datasets in the
wild is crucial for real-world applications, yet it remains challenging. The
difficulties lie in many aspects and this work focus on computing resource
constraint and long-tailed class distribution. Recently, classification-based
representation learning with deep neural networks and well-designed losses have
demonstrated good recognition performance. However, the computing and memory
cost linearly scales up to the number of identities (classes) in the training
set, and the learning process suffers from unbalanced classes. In this work, we
propose a dynamic class queue (DCQ) to tackle these two problems. Specifically,
for each iteration during training, a subset of classes for recognition are
dynamically selected and their class weights are dynamically generated
on-the-fly which are stored in a queue. Since only a subset of classes is
selected for each iteration, the computing requirement is reduced. By using a
single server without model parallel, we empirically verify in large-scale
datasets that 10% of classes are sufficient to achieve similar performance as
using all classes. Moreover, the class weights are dynamically generated in a
few-shot manner and therefore suitable for tail classes with only a few
instances. We show clear improvement over a strong baseline in the largest
public dataset Megaface Challenge2 (MF2) which has 672K identities and over 88%
of them have less than 10 instances. Code is available at
https://github.com/bilylee/DCQ"
107,"This paper tackles the problem of novel view synthesis from a single image.
In particular, we target real-world scenes with rich geometric structure, a
challenging task due to the large appearance variations of such scenes and the
lack of simple 3D models to represent them. Modern, learning-based approaches
mostly focus on appearance to synthesize novel views and thus tend to generate
predictions that are inconsistent with the underlying scene structure. By
contrast, in this paper, we propose to exploit the 3D geometry of the scene to
synthesize a novel view. Specifically, we approximate a real-world scene by a
fixed number of planes, and learn to predict a set of homographies and their
corresponding region masks to transform the input image into a novel view. To
this end, we develop a new region-aware geometric transform network that
performs these multiple tasks in a common framework. Our results on the outdoor
KITTI and the indoor ScanNet datasets demonstrate the effectiveness of our
network in generating high quality synthetic views that respect the scene
geometry, thus outperforming the state-of-the-art methods."
108,"The success of deep neural networks (DNNs) is attributable to three factors:
increased compute capacity, more complex models, and more data. These factors,
however, are not always present, especially for edge applications such as
autonomous driving, augmented reality, and internet-of-things. Training DNNs
requires a large amount of data, which is difficult to obtain. Edge devices
such as mobile phones have limited compute capacity, and therefore, require
specialized and efficient DNNs. However, due to the enormous design space and
prohibitive training costs, designing efficient DNNs for different target
devices is challenging. So the question is, with limited data, compute
capacity, and model complexity, can we still successfully apply deep neural
networks?
  This dissertation focuses on the above problems and improving the efficiency
of deep neural networks at four levels. Model efficiency: we designed neural
networks for various computer vision tasks and achieved more than 10x faster
speed and lower energy. Data efficiency: we developed an advanced tool that
enables 6.2x faster annotation of a LiDAR point cloud. We also leveraged domain
adaptation to utilize simulated data, bypassing the need for real data.
Hardware efficiency: we co-designed neural networks and hardware accelerators
and achieved 11.6x faster inference. Design efficiency: the process of finding
the optimal neural networks is time-consuming. Our automated neural
architecture search algorithms discovered, using 421x lower computational cost
than previous search methods, models with state-of-the-art accuracy and
efficiency."
109,"In this paper, we propose a way of synthesizing realistic images directly
with natural language description, which has many useful applications, e.g.
intelligent image manipulation. We attempt to accomplish such synthesis: given
a source image and a target text description, our model synthesizes images to
meet two requirements: 1) being realistic while matching the target text
description; 2) maintaining other image features that are irrelevant to the
text description. The model should be able to disentangle the semantic
information from the two modalities (image and text), and generate new images
from the combined semantics. To achieve this, we proposed an end-to-end neural
architecture that leverages adversarial learning to automatically learn
implicit loss functions, which are optimized to fulfill the aforementioned two
requirements. We have evaluated our model by conducting experiments on
Caltech-200 bird dataset and Oxford-102 flower dataset, and have demonstrated
that our model is capable of synthesizing realistic images that match the given
descriptions, while still maintain other features of original images."
110,"Fully autonomous drones have been demonstrated to find lost or injured
persons under strongly occluding forest canopy. Airborne Optical Sectioning
(AOS), a novel synthetic aperture imaging technique, together with
deep-learning-based classification enables high detection rates under realistic
search-and-rescue conditions. We demonstrate that false detections can be
significantly suppressed and true detections boosted by combining
classifications from multiple AOS rather than single integral images. This
improves classification rates especially in the presence of occlusion. To make
this possible, we modified the AOS imaging process to support large overlaps
between subsequent integrals, enabling real-time and on-board scanning and
processing of groundspeeds up to 10 m/s."
111,"In object detection, reducing computational cost is as important as improving
accuracy for most practical usages. This paper proposes a novel network
structure, which is an order of magnitude lighter than other state-of-the-art
networks while maintaining the accuracy. Based on the basic principle of more
layers with less channels, this new deep neural network minimizes its
redundancy by adopting recent innovations including C.ReLU and Inception
structure. We also show that this network can be trained efficiently to achieve
solid results on well-known object detection benchmarks: 84.9% and 84.2% mAP on
VOC2007 and VOC2012 while the required compute is less than 10% of the recent
ResNet-101."
112,"Deep Reinforcement Learning has been successfully applied to learn robotic
control. However, the corresponding algorithms struggle when applied to
problems where the agent is only rewarded after achieving a complex task. In
this context, using demonstrations can significantly speed up the learning
process, but demonstrations can be costly to acquire. In this paper, we propose
to leverage a sequential bias to learn control policies for complex robotic
tasks using a single demonstration. To do so, our method learns a
goal-conditioned policy to control a system between successive low-dimensional
goals. This sequential goal-reaching approach raises a problem of compatibility
between successive goals: we need to ensure that the state resulting from
reaching a goal is compatible with the achievement of the following goals. To
tackle this problem, we present a new algorithm called DCIL-II. We show that
DCIL-II can solve with unprecedented sample efficiency some challenging
simulated tasks such as humanoid locomotion and stand-up as well as fast
running with a simulated Cassie robot. Our method leveraging sequentiality is a
step towards the resolution of complex robotic tasks under minimal
specification effort, a key feature for the next generation of autonomous
robots."
113,"The four-wheeled Mecanum robot is widely used in various industries due to
its maneuverability and strong load capacity, which is suitable for performing
precise transportation tasks in a narrow environment, but while the Mecanum
wheel robot has mobility, it also consumes more energy than ordinary robots.
The power consumed by the Mecanum wheel mobile robot varies enormously
depending on their operating regimes and environments. Therefore, only knowing
the working environment of the robot and the accurate power consumption model
can we accurately predict the power consumption of the robot. In order to
increase the appli-cable scenarios of energy consumption modeling for Mecanum
wheel robots and improve the accuracy of energy consumption modeling, this
paper focuses on various factors that affect the energy consumption of the
Mecanum wheel robot, such as motor temperature, terrain, the center of gravity
position, etc. The model is derived from the kinematic and kinetic model
combined with electrical engineering and energy flow principles. The model has
been simulated in MATLAB and experimentally validated with the four-wheeled
Mecanum robot platform in our lab. Experimental results show that the model is
90% accurate. The results of energy consumption modeling can help robots to
save energy by helping them to perform rational path planning and task
planning."
114,"A key challenge in Imitation Learning (IL) is that optimal state actions
demonstrations are difficult for the teacher to provide. For example in
robotics, providing kinesthetic demonstrations on a robotic manipulator
requires the teacher to control multiple degrees of freedom at once. The
difficulty of requiring optimal state action demonstrations limits the space of
problems where the teacher can provide quality feedback. As an alternative to
state action demonstrations, the teacher can provide corrective feedback such
as their preferences or rewards. Prior work has created algorithms designed to
learn from specific types of noisy feedback, but across teachers and tasks
different forms of feedback may be required. Instead we propose that in order
to learn from a diversity of scenarios we need to learn from a variety of
feedback. To learn from a variety of feedback we make the following insight:
the teacher's cost function is latent and we can model a stream of feedback as
a stream of loss functions. We then use any online learning algorithm to
minimize the sum of these losses. With this insight we can learn from a
diversity of feedback that is weakly correlated with the teacher's true cost
function. We unify prior work into a general corrective feedback meta-algorithm
and show that regardless of feedback we can obtain the same regret bounds. We
demonstrate our approach by learning to perform a household navigation task on
a robotic racecar platform. Our results show that our approach can learn
quickly from a variety of noisy feedback."
115,"In this paper we consider infinite horizon discounted dynamic programming
problems with finite state and control spaces, and partial state observations.
We discuss an algorithm that uses multistep lookahead, truncated rollout with a
known base policy, and a terminal cost function approximation. This algorithm
is also used for policy improvement in an approximate policy iteration scheme,
where successive policies are approximated by using a neural network
classifier. A novel feature of our approach is that it is well suited for
distributed computation through an extended belief space formulation and the
use of a partitioned architecture, which is trained with multiple neural
networks. We apply our methods in simulation to a class of sequential repair
problems where a robot inspects and repairs a pipeline with potentially several
rupture sites under partial information about the state of the pipeline."
116,"We present an integrated Task-Motion Planning framework for robot navigation
in belief space. Autonomous robots operating in real world complex scenarios
require planning in the discrete (task) space and the continuous (motion)
space. To this end, we propose a framework for integrating belief space
reasoning within a hybrid task planner. The expressive power of PDDL+ combined
with heuristic-driven semantic attachments performs the propagated and
posterior belief estimates while planning. The underlying methodology for the
development of the combined hybrid planner is discussed, providing suggestions
for improvements and future work. Furthermore we validate key aspects of our
approach using a realistic scenario in simulation."
117,"The real-world application of small drones is mostly hampered by energy
limitations. Neuromorphic computing promises extremely energy-efficient AI for
autonomous flight, but is still challenging to train and deploy on real robots.
In order to reap the maximal benefits from neuromorphic computing, it is
desired to perform all autonomy functions end-to-end on a single neuromorphic
chip, from low-level attitude control to high-level navigation. This research
presents the first neuromorphic control system using a spiking neural network
(SNN) to effectively map a drone's raw sensory input directly to motor
commands. We apply this method to low-level attitude estimation and control for
a quadrotor, deploying the SNN on a tiny Crazyflie. We propose a modular SNN,
separately training and then merging estimation and control sub-networks. The
SNN is trained with imitation learning, using a flight dataset of sensory-motor
pairs. Post-training, the network is deployed on the Crazyflie, issuing control
commands from sensor inputs at $500$Hz. Furthermore, for the training procedure
we augmented training data by flying a controller with additional excitation
and time-shifting the target data to enhance the predictive capabilities of the
SNN. On the real drone the perception-to-control SNN tracks attitude commands
with an average error of $3$ degrees, compared to $2.5$ degrees for the regular
flight stack. We also show the benefits of the proposed learning modifications
for reducing the average tracking error and reducing oscillations. Our work
shows the feasibility of performing neuromorphic end-to-end control, laying the
basis for highly energy-efficient and low-latency neuromorphic autopilots."
118,"Robotic vision introduces requirements for real-time processing of
fast-varying, noisy information in a continuously changing environment. In a
real-world environment, convenient assumptions, such as static camera systems
and deep learning algorithms devouring high volumes of ideally slightly-varying
data are hard to survive. Leveraging on recent studies on the neural connectome
associated with eye movements, we designed a neuromorphic oculomotor controller
and placed it at the heart of our in-house biomimetic robotic head prototype.
The controller is unique in the sense that (1) all data are encoded and
processed by a spiking neural network (SNN), and (2) by mimicking the
associated brain areas' topology, the SNN is biologically interpretable and
requires no training to operate. Here, we report the robot's target tracking
ability, demonstrate that its eye kinematics are similar to those reported in
human eye studies and show that a biologically-constrained learning, although
not required for the SNN's function, can be used to further refine its
performance. This work aligns with our ongoing effort to develop
energy-efficient neuromorphic SNNs and harness their emerging intelligence to
control biomimetic robots with versatility and robustness."
119,"Finding controllers that perform well across multiple morphologies is an
important milestone for large-scale robotics, in line with recent advances via
foundation models in other areas of machine learning. However, the challenges
of learning a single controller to control multiple morphologies make the `one
robot one task' paradigm dominant in the field. To alleviate these challenges,
we present a pipeline that: (1) leverages Quality Diversity algorithms like
MAP-Elites to create a dataset of many single-task/single-morphology teacher
controllers, then (2) distills those diverse controllers into a single
multi-morphology controller that performs well across many different body plans
by mimicking the sensory-action patterns of the teacher controllers via
supervised learning. The distilled controller scales well with the number of
teachers/morphologies and shows emergent properties. It generalizes to unseen
morphologies in a zero-shot manner, providing robustness to morphological
perturbations and instant damage recovery. Lastly, the distilled controller is
also independent of the teacher controllers -- we can distill the teacher's
knowledge into any controller model, making our approach synergistic with
architectural improvements and existing training algorithms for teacher
controllers."
120,"In this work, we propose the Informed Batch Belief Trees (IBBT) algorithm for
motion planning under motion and sensing uncertainties. The original stochastic
motion planning problem is divided into a deterministic motion planning problem
and a graph search problem. We solve the deterministic planning problem using
sampling-based methods such as PRM or RRG to construct a graph of nominal
trajectories. Then, an informed cost-to-go heuristic for the original problem
is computed based on the nominal trajectory graph. Finally, we grow a belief
tree by searching over the graph using the proposed heuristic. IBBT interleaves
between batch state sampling, nominal trajectory graph construction, heuristic
computing, and search over the graph to find belief space motion plans. IBBT is
an anytime, incremental algorithm. With an increasing number of batches of
samples added to the graph, the algorithm finds motion plans that converge to
the optimal one. IBBT is efficient by reusing results between sequential
iterations. The belief tree searching is an ordered search guided by an
informed heuristic. We test IBBT in different planning environments. Our
numerical investigation confirms that IBBT finds non-trivial motion plans and
is faster compared with previous similar methods."
121,"Hierarchical Reinforcement Learning (HRL) algorithms have been demonstrated
to perform well on high-dimensional decision making and robotic control tasks.
However, because they solely optimize for rewards, the agent tends to search
the same space redundantly. This problem reduces the speed of learning and
achieved reward. In this work, we present an Off-Policy HRL algorithm that
maximizes entropy for efficient exploration. The algorithm learns a temporally
abstracted low-level policy and is able to explore broadly through the addition
of entropy to the high-level. The novelty of this work is the theoretical
motivation of adding entropy to the RL objective in the HRL setting. We
empirically show that the entropy can be added to both levels if the
Kullback-Leibler (KL) divergence between consecutive updates of the low-level
policy is sufficiently small. We performed an ablative study to analyze the
effects of entropy on hierarchy, in which adding entropy to high-level emerged
as the most desirable configuration. Furthermore, a higher temperature in the
low-level leads to Q-value overestimation and increases the stochasticity of
the environment that the high-level operates on, making learning more
challenging. Our method, SHIRO, surpasses state-of-the-art performance on a
range of simulated robotic control benchmark tasks and requires minimal tuning."
122,"Despite outstanding success in vision amongst other domains, many of the
recent deep learning approaches have evident drawbacks for robots. This
manuscript surveys recent work in the literature that pertain to applying deep
learning systems to the robotics domain, either as means of estimation or as a
tool to resolve motor commands directly from raw percepts. These recent
advances are only a piece to the puzzle. We suggest that deep learning as a
tool alone is insufficient in building a unified framework to acquire general
intelligence. For this reason, we complement our survey with insights from
cognitive development and refer to ideas from classical control theory,
producing an integrated direction for a lifelong learning architecture."
123,"The use of mobile robots is being popular over the world mainly for
autonomous explorations in hazardous/ toxic or unknown environments. This
exploration will be more effective and efficient if the explorations in unknown
environment can be aided with the learning from past experiences. Currently
reinforcement learning is getting more acceptances for implementing learning in
robots from the system-environment interactions. This learning can be
implemented using the concept of both single-agent and multiagent. This paper
describes such a multiagent approach for implementing a type of reinforcement
learning using a priority based behaviour-based architecture. This proposed
methodology has been successfully tested in both indoor and outdoor
environments."
124,"This paper presents a learning from demonstration approach to programming
safe, autonomous behaviors for uncommon driving scenarios. Simulation is used
to re-create a targeted driving situation, one containing a road-side hazard
creating a significant occlusion in an urban neighborhood, and collect optimal
driving behaviors from 24 users. Paper employs a key-frame based approach
combined with an algorithm to linearly combine models in order to extend the
behavior to novel variations of the target situation. This approach is
theoretically agnostic to the kind of LfD framework used for modeling data and
our results suggest it generalizes well to variations containing an additional
number of hazards occurring in sequence. The linear combination algorithm is
informed by analysis of driving data, which also suggests that decision-making
algorithms need to consider a trade-off between road-rules and immediate
rewards to tackle some complex cases."
125,"Motivated by the stringent requirements of unstructured real-world where a
plethora of unknown objects reside in arbitrary locations of the surface, we
propose a voxel-based deep 3D Convolutional Neural Network (3D CNN) that
generates feasible 6-DoF grasp poses in unrestricted workspace with
reachability awareness. Unlike the majority of works that predict if a proposed
grasp pose within the restricted workspace will be successful solely based on
grasp pose stability, our approach further learns a reachability predictor that
evaluates if the grasp pose is reachable or not from robot's own experience. To
avoid the laborious real training data collection, we exploit the power of
simulation to train our networks on a large-scale synthetic dataset. This work
is an early attempt that simultaneously evaluates grasping reachability from
learned knowledge while proposing feasible grasp poses with 3D CNN.
Experimental results in both simulation and real-world demonstrate that our
approach outperforms several other methods and achieves 82.5% grasping success
rate on unknown objects."
126,"We formalize decision-making problems in robotics and automated control using
continuous MDPs and actions that take place over continuous time intervals. We
then approximate the continuous MDP using finer and finer discretizations.
Doing this results in a family of systems, each of which has an extremely large
action space, although only a few actions are ""interesting"". We can view the
decision maker as being unaware of which actions are ""interesting"". We can
model this using MDPUs, MDPs with unawareness, where the action space is much
smaller. As we show, MDPUs can be used as a general framework for learning
tasks in robotic problems. We prove results on the difficulty of learning a
near-optimal policy in an an MDPU for a continuous task. We apply these ideas
to the problem of having a humanoid robot learn on its own how to walk."
127,"In this article, we propose a backpropagation-free approach to robotic
control through the neuro-cognitive computational framework of neural
generative coding (NGC), designing an agent built completely from powerful
predictive coding/processing circuits that facilitate dynamic, online learning
from sparse rewards, embodying the principles of planning-as-inference.
Concretely, we craft an adaptive agent system, which we call active predictive
coding (ActPC), that balances an internally-generated epistemic signal (meant
to encourage intelligent exploration) with an internally-generated instrumental
signal (meant to encourage goal-seeking behavior) to ultimately learn how to
control various simulated robotic systems as well as a complex robotic arm
using a realistic robotics simulator, i.e., the Surreal Robotics Suite, for the
block lifting task and can pick-and-place problems. Notably, our experimental
results demonstrate that our proposed ActPC agent performs well in the face of
sparse (extrinsic) reward signals and is competitive with or outperforms
several powerful backprop-based RL approaches."
128,"To achieve a successful grasp, gripper attributes such as its geometry and
kinematics play a role as important as the object geometry. The majority of
previous work has focused on developing grasp methods that generalize over
novel object geometry but are specific to a certain robot hand. We propose
UniGrasp, an efficient data-driven grasp synthesis method that considers both
the object geometry and gripper attributes as inputs. UniGrasp is based on a
novel deep neural network architecture that selects sets of contact points from
the input point cloud of the object. The proposed model is trained on a large
dataset to produce contact points that are in force closure and reachable by
the robot hand. By using contact points as output, we can transfer between a
diverse set of multifingered robotic hands. Our model produces over 90% valid
contact points in Top10 predictions in simulation and more than 90% successful
grasps in real world experiments for various known two-fingered and
three-fingered grippers. Our model also achieves 93%, 83% and 90% successful
grasps in real world experiments for an unseen two-fingered gripper and two
unseen multi-fingered anthropomorphic robotic hands."
129,"We present a novel human-aware navigation approach, where the robot learns to
mimic humans to navigate safely in crowds. The presented model, referred to as
DeepMoTIon, is trained with pedestrian surveillance data to predict human
velocity in the environment. The robot processes LiDAR scans via the trained
network to navigate to the target location. We conduct extensive experiments to
assess the components of our network and prove their necessity to imitate
humans. Our experiments show that DeepMoTIion outperforms all the benchmarks in
terms of human imitation, achieving a 24% reduction in time series-based path
deviation over the next best approach. In addition, while many other approaches
often failed to reach the target, our method reached the target in 100% of the
test cases while complying with social norms and ensuring human safety."
130,"Learning algorithms, like Quality-Diversity (QD), can be used to acquire
repertoires of diverse robotics skills. This learning is commonly done via
computer simulation due to the large number of evaluations required. However,
training in a virtual environment generates a gap between simulation and
reality. Here, we build upon the Reset-Free QD (RF-QD) algorithm to learn
controllers directly on a physical robot. This method uses a dynamics model,
learned from interactions between the robot and the environment, to predict the
robot's behaviour and improve sample efficiency. A behaviour selection policy
filters out uninteresting or unsafe policies predicted by the model. RF-QD also
includes a recovery policy that returns the robot to a safe zone when it has
walked outside of it, allowing continuous learning. We demonstrate that our
method enables a physical quadruped robot to learn a repertoire of behaviours
in two hours without human supervision. We successfully test the solution
repertoire using a maze navigation task. Finally, we compare our approach to
the MAP-Elites algorithm. We show that dynamics awareness and a recovery policy
are required for training on a physical robot for optimal archive generation.
Video available at https://youtu.be/BgGNvIsRh7Q"
131,"The study of dexterous manipulation has provided important insights in humans
sensorimotor control as well as inspiration for manipulation strategies in
robotic hands. Previous work focused on experimental environment with
restrictions. Here we describe a method using the deformation and color
distribution of the fingernail and its surrounding skin, to estimate the
fingertip forces, torques and contact surface curvatures for various objects,
including the shape and material of the contact surfaces and the weight of the
objects. The proposed method circumvents limitations associated with sensorized
objects, gloves or fixed contact surface type. In addition, compared with
previous single finger estimation in an experimental environment, we extend the
approach to multiple finger force estimation, which can be used for
applications such as human grasping analysis. Four algorithms are used, c.q.,
Gaussian process (GP), Convolutional Neural Networks (CNN), Neural Networks
with Fast Dropout (NN-FD) and Recurrent Neural Networks with Fast Dropout
(RNN-FD), to model a mapping from images to the corresponding labels. The
results further show that the proposed method has high accuracy to predict
force, torque and contact surface."
132,"It has been suggested that, when faced with large amounts of uncertainty in
situations of automated control, type-2 fuzzy logic based controllers will
out-perform the simpler type-1 varieties due to the latter lacking the
flexibility to adapt accordingly. This paper aims to investigate this problem
in detail in order to analyse when a type-2 controller will improve upon type-1
performance. A robotic sailing boat is subjected to several experiments in
which the uncertainty and difficulty of the sailing problem is increased in
order to observe the effects on measured performance. Improved performance is
observed but not in every case. The size of the FOU is shown to be have a large
effect on performance with potentially severe performance penalties for
incorrectly sized footprints."
133,"This paper presents a data structure that summarizes distances between
configurations across a robot configuration space, using a binary space
partition whose cells contain parameters used for a locally linear
approximation of the distance function. Querying the data structure is
extremely fast, particularly when compared to the graph search required for
querying Probabilistic Roadmaps, and memory requirements are promising. The
paper explores the use of the data structure constructed for a single robot to
provide a heuristic for challenging multi-robot motion planning problems.
Potential applications also include the use of remote computation to analyze
the space of robot motions, which then might be transmitted on-demand to robots
with fewer computational resources."
134,"The use of multi-camera views simultaneously has been shown to improve the
generalization capabilities and performance of visual policies. However, the
hardware cost and design constraints in real-world scenarios can potentially
make it challenging to use multiple cameras. In this study, we present a novel
approach to enhance the generalization performance of vision-based
Reinforcement Learning (RL) algorithms for robotic manipulation tasks. Our
proposed method involves utilizing a technique known as knowledge distillation,
in which a pre-trained ``teacher'' policy trained with multiple camera
viewpoints guides a ``student'' policy in learning from a single camera
viewpoint. To enhance the student policy's robustness against camera location
perturbations, it is trained using data augmentation and extreme viewpoint
changes. As a result, the student policy learns robust visual features that
allow it to locate the object of interest accurately and consistently,
regardless of the camera viewpoint. The efficacy and efficiency of the proposed
method were evaluated both in simulation and real-world environments. The
results demonstrate that the single-view visual student policy can successfully
learn to grasp and lift a challenging object, which was not possible with a
single-view policy alone. Furthermore, the student policy demonstrates
zero-shot transfer capability, where it can successfully grasp and lift objects
in real-world scenarios for unseen visual configurations."
135,"Robots operating in the real world will experience a range of different
environments and tasks. It is essential for the robot to have the ability to
adapt to its surroundings to work efficiently in changing conditions.
Evolutionary robotics aims to solve this by optimizing both the control and
body (morphology) of a robot, allowing adaptation to internal, as well as
external factors. Most work in this field has been done in physics simulators,
which are relatively simple and not able to replicate the richness of
interactions found in the real world. Solutions that rely on the complex
interplay between control, body, and environment are therefore rarely found. In
this paper, we rely solely on real-world evaluations and apply evolutionary
search to yield combinations of morphology and control for our mechanically
self-reconfiguring quadruped robot. We evolve solutions on two distinct
physical surfaces and analyze the results in terms of both control and
morphology. We then transition to two previously unseen surfaces to demonstrate
the generality of our method. We find that the evolutionary search finds
high-performing and diverse morphology-controller configurations by adapting
both control and body to the different properties of the physical environments.
We additionally find that morphology and control vary with statistical
significance between the environments. Moreover, we observe that our method
allows for morphology and control parameters to transfer to previously-unseen
terrains, demonstrating the generality of our approach."
136,"We provide a systematic analysis of levels of integration between discrete
high-level reasoning and continuous low-level reasoning to address hybrid
planning problems in robotics. We identify four distinct strategies for such an
integration: (i) low-level checks are done for all possible cases in advance
and then this information is used during plan generation, (ii) low-level checks
are done exactly when they are needed during the search for a plan, (iii) first
all plans are computed and then infeasible ones are filtered, and (iv) by means
of replanning, after finding a plan, low-level checks identify whether it is
infeasible or not; if it is infeasible, a new plan is computed considering the
results of previous low- level checks. We perform experiments on hybrid
planning problems in robotic manipulation and legged locomotion domains
considering these four methods of integration, as well as some of their
combinations. We analyze the usefulness of levels of integration in these
domains, both from the point of view of computational efficiency (in time and
space) and from the point of view of plan quality relative to its feasibility.
We discuss advantages and disadvantages of each strategy in the light of
experimental results and provide some guidelines on choosing proper strategies
for a given domain."
137,"A limitation of model-based reinforcement learning (MBRL) is the exploitation
of errors in the learned models. Black-box models can fit complex dynamics with
high fidelity, but their behavior is undefined outside of the data
distribution.Physics-based models are better at extrapolating, due to the
general validity of their informed structure, but underfit in the real world
due to the presence of unmodeled phenomena. In this work, we demonstrate
experimentally that for the offline model-based reinforcement learning setting,
physics-based models can be beneficial compared to high-capacity function
approximators if the mechanical structure is known. Physics-based models can
learn to perform the ball in a cup (BiC) task on a physical manipulator using
only 4 minutes of sampled data using offline MBRL. We find that black-box
models consistently produce unviable policies for BiC as all predicted
trajectories diverge to physically impossible state, despite having access to
more data than the physics-based model. In addition, we generalize the approach
of physics parameter identification from modeling holonomic multi-body systems
to systems with nonholonomic dynamics using end-to-end automatic
differentiation.
  Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/"
138,"Dubins tours represent a solution of the Dubins Traveling Salesman Problem
(DTSP) that is a variant of the optimization routing problem to determine a
curvature-constrained shortest path to visit a set of locations such that the
path is feasible for Dubins vehicle, which moves only forward and has a limited
turning radius. The DTSP combines the NP-hard combinatorial optimization to
determine the optimal sequence of visits to the locations, as in the regular
TSP, with the continuous optimization of the heading angles at the locations,
where the optimal heading values depend on the sequence of visits and vice
versa. We address the computationally challenging DTSP by fast evaluation of
the sequence of visits by the proposed Windowing Surrogate Model (WiSM) which
estimates the length of the optimal Dubins path connecting a sequence of
locations in a Dubins tour. The estimation is sped up by a regression model
trained using close to optimum solutions of small Dubins tours that are
generalized for large-scale instances of the addressed DTSP utilizing the
sliding window technique and a cache for already computed results. The reported
results support that the proposed WiSM enables a fast convergence of a
relatively simple evolutionary algorithm to high-quality solutions of the DTSP.
We show that with an increasing number of locations, our algorithm scales
significantly better than other state-of-the-art DTSP solvers."
139,"Learning to control robots directly based on images is a primary challenge in
robotics. However, many existing reinforcement learning approaches require
iteratively obtaining millions of robot samples to learn a policy, which can
take significant time. In this paper, we focus on learning a realistic world
model capturing the dynamics of scene changes conditioned on robot actions. Our
dreaming model can emulate samples equivalent to a sequence of images from the
actual environment, technically by learning an action-conditioned future
representation/scene regressor. This allows the agent to learn action policies
(i.e., visuomotor policies) by interacting with the dreaming model rather than
the real-world. We experimentally confirm that our dreaming model enables robot
learning of policies that transfer to the real-world."
140,"Deep Reinforcement Learning (DRL) has achieved impressive performance in
robotics and autonomous systems (RAS). A key challenge to its deployment in
real-life operations is the presence of spuriously unsafe DRL policies.
Unexplored states may lead the agent to make wrong decisions that could result
in hazards, especially in applications where DRL-trained end-to-end controllers
govern the behaviour of RAS. This paper proposes a novel quantitative
reliability assessment framework for DRL-controlled RAS, leveraging
verification evidence generated from formal reliability analysis of neural
networks. A two-level verification framework is introduced to check the safety
property with respect to inaccurate observations that are due to, e.g.,
environmental noise and state changes. Reachability verification tools are
leveraged locally to generate safety evidence of trajectories. In contrast, at
the global level, we quantify the overall reliability as an aggregated metric
of local safety evidence, corresponding to a set of distinct tasks and their
occurrence probabilities. The effectiveness of the proposed verification
framework is demonstrated and validated via experiments on real RAS."
141,"A robot performing multi-object grasping needs to sense the number of objects
in the hand after grasping. The count plays an important role in determining
the robot's next move and the outcome and efficiency of the whole pick-place
process. This paper presents a data-driven contrastive learning-based counting
classifier with a modified loss function as a simple and effective approach for
object counting despite significant occlusion challenges caused by robotic
fingers and objects. The model was validated against other models with three
different common shapes (spheres, cylinders, and cubes) in simulation and in a
real setup. The proposed contrastive learning-based counting approach achieved
above 96\% accuracy for all three objects in the real setup."
142,"We investigated the application of haptic feedback control and deep
reinforcement learning (DRL) to robot-assisted dressing. Our method uses DRL to
simultaneously train human and robot control policies as separate neural
networks using physics simulations. In addition, we modeled variations in human
impairments relevant to dressing, including unilateral muscle weakness,
involuntary arm motion, and limited range of motion. Our approach resulted in
control policies that successfully collaborate in a variety of simulated
dressing tasks involving a hospital gown and a T-shirt. In addition, our
approach resulted in policies trained in simulation that enabled a real PR2
robot to dress the arm of a humanoid robot with a hospital gown. We found that
training policies for specific impairments dramatically improved performance;
that controller execution speed could be scaled after training to reduce the
robot's speed without steep reductions in performance; that curriculum learning
could be used to lower applied forces; and that multi-modal sensing, including
a simulated capacitive sensor, improved performance."
143,"In this paper, we address the problem of adaptive path planning for accurate
semantic segmentation of terrain using unmanned aerial vehicles (UAVs). The
usage of UAVs for terrain monitoring and remote sensing is rapidly gaining
momentum due to their high mobility, low cost, and flexible deployment.
However, a key challenge is planning missions to maximize the value of acquired
data in large environments given flight time limitations. To address this, we
propose an online planning algorithm which adapts the UAV paths to obtain
high-resolution semantic segmentations necessary in areas on the terrain with
fine details as they are detected in incoming images. This enables us to
perform close inspections at low altitudes only where required, without wasting
energy on exhaustive mapping at maximum resolution. A key feature of our
approach is a new accuracy model for deep learning-based architectures that
captures the relationship between UAV altitude and semantic segmentation
accuracy. We evaluate our approach on the application of crop/weed segmentation
in precision agriculture using real-world field data."
144,"Accurate diagnosis of propeller faults is crucial for ensuring the safe and
efficient operation of quadrotors. Training a fault classifier using simulated
data and deploying it on a real quadrotor is a cost-effective and safe
approach. However, the simulation-to-reality gap often leads to poor
performance of the classifier when applied in real flight. In this work, we
propose a deep learning model that addresses this issue by utilizing newly
identified features (NIF) as input and utilizing domain adaptation techniques
to reduce the simulation-to-reality gap. In addition, we introduce an adjusted
simulation model that generates training data that more accurately reflects the
behavior of real quadrotors. The experimental results demonstrate that our
proposed approach achieves an accuracy of 96\% in detecting propeller faults.
To the best of our knowledge, this is the first reliable and efficient method
for simulation-to-reality fault diagnosis of quadrotor propellers."
145,"In this work, we examine the problem of online decision making under
uncertainty, which we formulate as planning in the belief space. Maintaining
beliefs (i.e., distributions) over high-dimensional states (e.g., entire
trajectories) was not only shown to significantly improve accuracy, but also
allows planning with information-theoretic objectives, as required for the
tasks of active SLAM and information gathering. Nonetheless, planning under
this ""smoothing"" paradigm holds a high computational complexity, which makes it
challenging for online solution. Thus, we suggest the following idea: before
planning, perform a standalone state variable reordering procedure on the
initial belief, and ""push forwards"" all the predicted loop closing variables.
Since the initial variable order determines which subset of them would be
affected by incoming updates, such reordering allows us to minimize the total
number of affected variables, and reduce the computational complexity of
candidate evaluation during planning. We call this approach PIVOT: Predictive
Incremental Variable Ordering Tactic. Applying this tactic can also improve the
state inference efficiency; if we maintain the PIVOT order after the planning
session, then we should similarly reduce the cost of loop closures, when they
actually occur. To demonstrate its effectiveness, we applied PIVOT in a
realistic active SLAM simulation, where we managed to significantly reduce the
computation time of both the planning and inference sessions. The approach is
applicable to general distributions, and induces no loss in accuracy."
146,"The aim of this workshop is to foster the exchange of insights on past and
ongoing research towards effective and long-lasting collaborations between
humans and robots. This workshop will provide a forum for representatives from
academia and industry communities to analyse the different aspects of HRI that
impact on its success. We particularly focus on AI techniques required to
implement autonomous and proactive interactions, on the factors that enhance,
undermine, or recover humans' acceptance and trust in robots, and on the
potential ethical and legal concerns related to the deployment of such robots
in human-centred environments.
  Website: https://sites.google.com/view/traits-hri-2022"
147,"Global mobile robot localization is the problem of determining a robot's pose
in an environment, using sensor data, when the starting position is unknown. A
family of probabilistic algorithms known as Monte Carlo Localization (MCL) is
currently among the most popular methods for solving this problem. MCL
algorithms represent a robot's belief by a set of weighted samples, which
approximate the posterior probability of where the robot is located by using a
Bayesian formulation of the localization problem. This article presents an
extension to the MCL algorithm, which addresses its problems when localizing in
highly symmetrical environments; a situation where MCL is often unable to
correctly track equally probable poses for the robot. The problem arises from
the fact that sample sets in MCL often become impoverished, when samples are
generated according to their posterior likelihood. Our approach incorporates
the idea of clusters of samples and modifies the proposal distribution
considering the probability mass of those clusters. Experimental results are
presented that show that this new extension to the MCL algorithm successfully
localizes in symmetric environments where ordinary MCL often fails."
148,"An overview of the process to develop a safety case for an autonomous robot
deployment on a nuclear site in the UK is described and a safety case for a
hypothetical robot incorporating AI is presented. This forms a first step
towards a deployment, showing what is possible now and what may be possible
with development of tools. It forms the basis for further discussion between
nuclear site licensees, the Office for Nuclear Regulation (ONR), industry and
academia."
149,"Billions of people use chopsticks, a simple yet versatile tool, for fine
manipulation of everyday objects. The small, curved, and slippery tips of
chopsticks pose a challenge for picking up small objects, making them a
suitably complex test case. This paper leverages human demonstrations to
develop an autonomous chopsticks-equipped robotic manipulator. Due to the lack
of accurate models for fine manipulation, we explore model-free imitation
learning, which traditionally suffers from the covariate shift phenomenon that
causes poor generalization. We propose two approaches to reduce covariate
shift, neither of which requires access to an interactive expert or a model,
unlike previous approaches. First, we alleviate single-step prediction errors
by applying an invariant operator to increase the data support at critical
steps for grasping. Second, we generate synthetic corrective labels by adding
bounded noise and combining parametric and non-parametric methods to prevent
error accumulation. We demonstrate our methods on a real chopstick-equipped
robot that we built, and observe the agent's success rate increase from 37.3%
to 80%, which is comparable to the human expert performance of 82.6%."
150,"In this paper, we propose a novel lightweight learning from demonstration
(LfD) model based on reservoir computing that can learn and generate multiple
movement trajectories with prediction intervals, which we call as Context-based
Echo State Network with prediction confidence (CESN+). CESN+ can generate
movement trajectories that may go beyond the initial LfD training based on a
desired set of conditions while providing confidence on its generated output.
To assess the abilities of CESN+, we first evaluate its performance against
Conditional Neural Movement Primitives (CNMP), a comparable framework that uses
a conditional neural process to generate movement primitives. Our findings
indicate that CESN+ not only outperforms CNMP but is also faster to train and
demonstrates impressive performance in generating trajectories for
extrapolation cases. In human-robot shared control applications, the confidence
of the machine generated trajectory is a key indicator of how to arbitrate
control sharing. To show the usability of the CESN+ for human-robot adaptive
shared control, we have designed a proof-of-concept human-robot shared control
task and tested its efficacy in adapting the sharing weight between the human
and the robot by comparing it to a fixed-weight control scheme. The simulation
experiments show that with CESN+ based adaptive sharing the total human load in
shared control can be significantly reduced. Overall, the developed CESN+ model
is a strong lightweight LfD system with desirable properties such fast training
and ability to extrapolate to the new task parameters while producing robust
prediction intervals for its output."
151,"The interactive partially observable Markov decision process (I-POMDP) is a
recently developed framework which extends the POMDP to the multi-agent setting
by including agent models in the state space. This paper argues for formulating
the problem of an agent learning interactively from a human teacher as an
I-POMDP, where the agent \emph{programming} to be learned is captured by random
variables in the agent's state space, all \emph{signals} from the human teacher
are treated as observed random variables, and the human teacher, modeled as a
distinct agent, is explicitly represented in the agent's state space. The main
benefits of this approach are: i. a principled action selection mechanism, ii.
a principled belief update mechanism, iii. support for the most common teacher
\emph{signals}, and iv. the anticipated production of complex beneficial
interactions. The proposed formulation, its benefits, and several open
questions are presented."
152,"The increasing presence of robots alongside humans, such as in human-robot
teams in manufacturing, gives rise to research questions about the kind of
behaviors people prefer in their robot counterparts. We term actions that
support interaction by reducing future interference with others as supportive
robot actions and investigate their utility in a co-located manipulation
scenario. We compare two robot modes in a shared table pick-and-place task: (1)
Task-oriented: the robot only takes actions to further its own task objective
and (2) Supportive: the robot sometimes prefers supportive actions to
task-oriented ones when they reduce future goal-conflicts. Our experiments in
simulation, using a simplified human model, reveal that supportive actions
reduce the interference between agents, especially in more difficult tasks, but
also cause the robot to take longer to complete the task. We implemented these
modes on a physical robot in a user study where a human and a robot perform
object placement on a shared table. Our results show that a supportive robot
was perceived as a more favorable coworker by the human and also reduced
interference with the human in the more difficult of two scenarios. However, it
also took longer to complete the task highlighting an interesting trade-off
between task-efficiency and human-preference that needs to be considered before
designing robot behavior for close-proximity manipulation scenarios."
153,"Simulation-to-real transfer is an important strategy for making reinforcement
learning practical with real robots. Successful sim-to-real transfer systems
have difficulty producing policies which generalize across tasks, despite
training for thousands of hours equivalent real robot time. To address this
shortcoming, we present a novel approach to efficiently learning new robotic
skills directly on a real robot, based on model-predictive control (MPC) and an
algorithm for learning task representations. In short, we show how to reuse the
simulation from the pre-training step of sim-to-real methods as a tool for
foresight, allowing the sim-to-real policy adapt to unseen tasks. Rather than
end-to-end learning policies for single tasks and attempting to transfer them,
we first use simulation to simultaneously learn (1) a continuous
parameterization (i.e. a skill embedding or latent) of task-appropriate
primitive skills, and (2) a single policy for these skills which is conditioned
on this representation. We then directly transfer our multi-skill policy to a
real robot, and actuate the robot by choosing sequences of skill latents which
actuate the policy, with each latent corresponding to a pre-learned primitive
skill controller. We complete unseen tasks by choosing new sequences of skill
latents to control the robot using MPC, where our MPC model is composed of the
pre-trained skill policy executed in the simulation environment, run in
parallel with the real robot. We discuss the background and principles of our
method, detail its practical implementation, and evaluate its performance by
using our method to train a real Sawyer Robot to achieve motion tasks such as
drawing and block pushing."
154,"Due to the complex and dynamic character of intersection scenarios, the
autonomous driving strategy at intersections has been a difficult problem and a
hot point in the research of intelligent transportation systems in recent
years. This paper gives a brief summary of state-of-the-art autonomous driving
strategies at intersections. Firstly, we enumerate and analyze common types of
intersection scenarios, corresponding simulation platforms, as well as related
datasets. Secondly, by reviewing previous studies, we have summarized
characteristics of existing autonomous driving strategies and classified them
into several categories. Finally, we point out problems of the existing
autonomous driving strategies and put forward several valuable research
outlooks."
155,"Over the last several years, use cases for robotics based solutions have
diversified from factory floors to domestic applications. In parallel, Deep
Learning approaches are replacing traditional techniques in Computer Vision,
Natural Language Processing, Speech processing, etc. and are delivering robust
results. Our goal is to survey a number of research internship projects in the
broad area of 'Deep Learning as applied to Robotics' and present a concise view
for the benefit of aspiring student interns. In this paper, we survey the
research work done by Robotic Institute Summer Scholars (RISS), CMU. We
particularly focus on papers that use deep learning to solve core robotic
problems and also robotic solutions. We trust this would be useful particularly
for internship aspirants for the Robotics Institute, CMU"
156,"In this paper, we exploit minimal sensing information gathered from
biologically inspired sensor networks to perform exploration and mapping in an
unknown environment. A probabilistic motion model of mobile sensing nodes,
inspired by motion characteristics of cockroaches, is utilized to extract weak
encounter information in order to build a topological representation of the
environment.
  Neighbor to neighbor interactions among the nodes are exploited to build
point clouds representing spatial features of the manifold characterizing the
environment based on the sampled data.
  To extract dominant features from sampled data, topological data analysis is
used to produce persistence intervals for features, to be used for topological
mapping. In order to improve robustness characteristics of the sampled data
with respect to outliers, density based subsampling algorithms are employed.
Moreover, a robust scale-invariant classification algorithm for persistence
diagrams is proposed to provide a quantitative representation of desired
features in the data. Furthermore, various strategies for defining encounter
metrics with different degrees of information regarding agents' motion are
suggested to enhance the precision of the estimation and classification
performance of the topological method."
157,"This work addresses the task of risk evaluation in traffic scenarios with
limited observability due to restricted sensorial coverage. Here, we
concentrate on intersection scenarios that are difficult to access visually. To
identify the area of sight, we employ ray casting on a local dynamic map
providing geometrical information and road infrastructure. Based on the area
with reduced visibility, we first model scene entities that pose a potential
risk without being visually perceivable yet. Then, we predict a worst-case
trajectory in the survival analysis for collision risk estimation. Resulting
risk indicators are utilized to evaluate the driver's current behavior, to warn
the driver in critical situations, to give suggestions on how to act safely or
to plan safe trajectories. We validate our approach by applying the resulting
intersection warning system on real world scenarios. The proposed system's
behavior reveals to mimic the general behavior of a correctly acting human
driver."
158,"Designing optimal soft modular robots is difficult, due to non-trivial
interactions between morphology and controller. Evolutionary algorithms (EAs),
combined with physical simulators, represent a valid tool to overcome this
issue. In this work, we investigate algorithmic solutions to improve the
Quality Diversity of co-evolved designs of Tensegrity Soft Modular Robots
(TSMRs) for two robotic tasks, namely goal reaching and squeezing trough a
narrow passage. To this aim, we use three different EAs, i.e., MAP-Elites and
two custom algorithms: one based on Viability Evolution (ViE) and NEAT
(ViE-NEAT), the other named Double Map MAP-Elites (DM-ME) and devised to seek
diversity while co-evolving robot morphologies and neural network (NN)-based
controllers. In detail, DM-ME extends MAP-Elites in that it uses two distinct
feature maps, referring to morphologies and controllers respectively, and
integrates a mechanism to automatically define the NN-related feature
descriptor. Considering the fitness, in the goal-reaching task ViE-NEAT
outperforms MAP-Elites and results equivalent to DM-ME. Instead, when
considering diversity in terms of ""illumination"" of the feature space, DM-ME
outperforms the other two algorithms on both tasks, providing a richer pool of
possible robotic designs, whereas ViE-NEAT shows comparable performance to
MAP-Elites on goal reaching, although it does not exploit any map."
159,"Search algorithms are applied where data retrieval with specified
specifications is required. The motivation behind developing search algorithms
in Functional Object-Oriented Networks is that most of the time, a certain
recipe needs to be retrieved or ingredients for a certain recipe needs to be
determined. According to the introduction, there is a time when execution of an
entire recipe is not available for a robot thus prompting the need to retrieve
a certain recipe or ingredients. With a quality FOON, robots can decipher a
task goal, find the correct objects at the required states on which to operate
and output a sequence of proper manipulation motions. This paper shows several
proposed weighted FOON and task planning algorithms that allow a robot and a
human to successfully complete complicated tasks together with higher success
rates than a human doing them alone."
160,"Target-driven visual navigation is a challenging problem that requires a
robot to find the goal using only visual inputs. Many researchers have
demonstrated promising results using deep reinforcement learning (deep RL) on
various robotic platforms, but typical end-to-end learning is known for its
poor extrapolation capability to new scenarios. Therefore, learning a
navigation policy for a new robot with a new sensor configuration or a new
target still remains a challenging problem. In this paper, we introduce a
learning algorithm that enables rapid adaptation to new sensor configurations
or target objects with a few shots. We design a policy architecture with latent
features between perception and inference networks and quickly adapt the
perception network via meta-learning while freezing the inference network. Our
experiments show that our algorithm adapts the learned navigation policy with
only three shots for unseen situations with different sensor configurations or
different target colors. We also analyze the proposed algorithm by
investigating various hyperparameters."
161,"We present a data-driven shared control algorithm that can be used to improve
a human operator's control of complex dynamic machines and achieve tasks that
would otherwise be challenging, or impossible, for the user on their own. Our
method assumes no a priori knowledge of the system dynamics. Instead, both the
dynamics and information about the user's interaction are learned from
observation through the use of a Koopman operator. Using the learned model, we
define an optimization problem to compute the autonomous partner's control
policy. Finally, we dynamically allocate control authority to each partner
based on a comparison of the user input and the autonomously generated control.
We refer to this idea as model-based shared control (MbSC). We evaluate the
efficacy of our approach with two human subjects studies consisting of 32 total
participants (16 subjects in each study). The first study imposes a linear
constraint on the modeling and autonomous policy generation algorithms. The
second study explores the more general, nonlinear variant. Overall, we find
that model-based shared control significantly improves task and control metrics
when compared to a natural learning, or user only, control paradigm. Our
experiments suggest that models learned via the Koopman operator generalize
across users, indicating that it is not necessary to collect data from each
individual user before providing assistance with MbSC. We also demonstrate the
data-efficiency of MbSC and consequently, it's usefulness in online learning
paradigms. Finally, we find that the nonlinear variant has a greater impact on
a user's ability to successfully achieve a defined task than the linear
variant."
162,"Hierarchical Task Network (HTN) planning is a popular approach that cuts down
on the classical planning search space by relying on a given hierarchical
library of domain control knowledge. This provides an intuitive methodology for
specifying high-level instructions on how robots and agents should perform
tasks, while also giving the planner enough flexibility to choose the
lower-level steps and their ordering. In this paper we present the HATP
(Hierarchical Agent-based Task Planner) planning framework which extends the
traditional HTN planning domain representation and semantics by making them
more suitable for roboticists, and treating agents as ""first class"" entities in
the language. The former is achieved by allowing ""social rules"" to be defined
which specify what behaviour is acceptable/unacceptable by the agents/robots in
the domain, and interleaving planning with geometric reasoning in order to
validate online -with respect to a detailed geometric 3D world- the human/robot
actions currently being pursued by HATP."
163,"This paper presents a novel approach to AUV localization, based on a
semantic-aided particle filter. Particle filters have been used successfully
for robotics localization since many years. Most of the approaches are however
based on geometric measurements and geometric information and simulations. In
the past years more and more efforts from research goes towards cognitive
robotics and the marine domain is not exception. Moving from signal to symbol
becomes therefore paramount for more complex applications. This paper presents
a contribution in the well-known area of underwater localization, incorporating
semantic information. An extension to the standard particle filter approach is
presented, based on semantic information of the environment. A comparison with
the geometric approach shows the advantages of a semantic layer to successfully
perform self-localization."
164,"This work presents an efficient framework to generate a motion plan of a
robot with high degrees of freedom (e.g., a humanoid robot).
High-dimensionality of the robot configuration space often leads to
difficulties in utilizing the widely-used motion planning algorithms, since the
volume of the decision space increases exponentially with the number of
dimensions. To handle complications arising from the large decision space, and
to solve a corresponding motion planning problem efficiently, two key concepts
are adopted in this work: First, the Gaussian process latent variable model
(GP-LVM) is utilized for low-dimensional representation of the original
configuration space. Second, an approximate inference algorithm is used,
exploiting through the duality between control and estimation, to explore the
decision space and to compute a high-quality motion trajectory of the robot.
Utilizing the GP-LVM and the duality between control and estimation, we
construct a fully probabilistic generative model with which a high-dimensional
motion planning problem is transformed into a tractable inference problem.
Finally, we compute the motion trajectory via an approximate inference
algorithm based on a variant of the particle filter. The resulting motions can
be viewed in the supplemental video. ( https://youtu.be/kngEaOR4Esc )"
165,"With the rapid progress in Multi-Agent Path Finding (MAPF), researchers have
studied how MAPF algorithms can be deployed to coordinate hundreds of robots in
large automated warehouses. While most works try to improve the throughput of
such warehouses by developing better MAPF algorithms, we focus on improving the
throughput by optimizing the warehouse layout. We show that, even with
state-of-the-art MAPF algorithms, commonly used human-designed layouts can lead
to congestion for warehouses with large numbers of robots and thus have limited
scalability. We extend existing automatic scenario generation methods to
optimize warehouse layouts. Results show that our optimized warehouse layouts
(1) reduce traffic congestion and thus improve throughput, (2) improve the
scalability of the automated warehouses by doubling the number of robots in
some cases, and (3) are capable of generating layouts with user-specified
diversity measures. We include the source code at:
https://github.com/lunjohnzhang/warehouse_env_gen_public"
166,"We present a navigation system that combines ideas from hierarchical planning
and machine learning. The system uses a traditional global planner to compute
optimal paths towards a goal, and a deep local trajectory planner and velocity
controller to compute motion commands. The latter components of the system
adjust the behavior of the robot through attention mechanisms such that it
moves towards the goal, avoids obstacles, and respects the space of nearby
pedestrians. Both the structure of the proposed deep models and the use of
attention mechanisms make the system's execution interpretable. Our simulation
experiments suggest that the proposed architecture outperforms baselines that
try to map global plan information and sensor data directly to velocity
commands. In comparison to a hand-designed traditional navigation system, the
proposed approach showed more consistent performance."
167,"This work addresses the problem of long-horizon task planning with the Large
Language Model (LLM) in an open-world household environment. Existing works
fail to explicitly track key objects and attributes, leading to erroneous
decisions in long-horizon tasks, or rely on highly engineered state features
and feedback, which is not generalizable. We propose an open state
representation that provides continuous expansion and updating of object
attributes from the LLM's inherent capabilities for context understanding and
historical action reasoning. Our proposed representation maintains a
comprehensive record of an object's attributes and changes, enabling robust
retrospective summary of the sequence of actions leading to the current state.
This allows continuously updating world model to enhance context understanding
for decision-making in task planning. We validate our model through experiments
across simulated and real-world task planning scenarios, demonstrating
significant improvements over baseline methods in a variety of tasks requiring
long-horizon state tracking and reasoning. (Video\footnote{Video demonstration:
\url{https://youtu.be/QkN-8pxV3Mo}.})"
168,"In this paper, we propose a novel optimization criterion that leverages
features of the skew normal distribution to better model the problem of
personalized recommendation. Specifically, the developed criterion borrows the
concept and the flexibility of the skew normal distribution, based on which
three hyperparameters are attached to the optimization criterion. Furthermore,
from a theoretical point of view, we not only establish the relation between
the maximization of the proposed criterion and the shape parameter in the skew
normal distribution, but also provide the analogies and asymptotic analysis of
the proposed criterion to maximization of the area under the ROC curve.
Experimental results conducted on a range of large-scale real-world datasets
show that our model significantly outperforms the state of the art and yields
consistently best performance on all tested datasets."
169,"Contract element extraction (CEE) is the novel task of automatically
identifying and extracting legally relevant elements such as contract dates,
payments, and legislation references from contracts. Automatic methods for this
task view it as a sequence labeling problem and dramatically reduce human
labor. However, as contract genres and element types may vary widely, a
significant challenge for this sequence labeling task is how to transfer
knowledge from one domain to another, i.e., cross-domain CEE. Cross-domain CEE
differs from cross-domain named entity recognition (NER) in two important ways.
First, contract elements are far more fine-grained than named entities, which
hinders the transfer of extractors. Second, the extraction zones for
cross-domain CEE are much larger than for cross-domain NER. As a result, the
contexts of elements from different domains can be more diverse. We propose a
framework, the Bi-directional Feedback cLause-Element relaTion network
(Bi-FLEET), for the cross-domain CEE task that addresses the above challenges.
Bi-FLEET has three main components: (1) a context encoder, (2) a clause-element
relation encoder, and (3) an inference layer. To incorporate invariant
knowledge about element and clause types, a clause-element graph is constructed
across domains and a hierarchical graph neural network is adopted in the
clause-element relation encoder. To reduce the influence of context variations,
a multi-task framework with a bi-directional feedback scheme is designed in the
inference layer, conducting both clause classification and element extraction.
The experimental results over both cross-domain NER and CEE tasks show that
Bi-FLEET significantly outperforms state-of-the-art baselines."
170,"Recommender systems exploit interaction history to estimate user preference,
having been heavily used in a wide range of industry applications. However,
static recommendation models are difficult to answer two important questions
well due to inherent shortcomings: (a) What exactly does a user like? (b) Why
does a user like an item? The shortcomings are due to the way that static
models learn user preference, i.e., without explicit instructions and active
feedback from users. The recent rise of conversational recommender systems
(CRSs) changes this situation fundamentally. In a CRS, users and the system can
dynamically communicate through natural language interactions, which provide
unprecedented opportunities to explicitly obtain the exact preference of users.
  Considerable efforts, spread across disparate settings and applications, have
been put into developing CRSs. Existing models, technologies, and evaluation
methods for CRSs are far from mature. In this paper, we provide a systematic
review of the techniques used in current CRSs. We summarize the key challenges
of developing CRSs in five directions: (1) Question-based user preference
elicitation. (2) Multi-turn conversational recommendation strategies. (3)
Dialogue understanding and generation. (4) Exploitation-exploration trade-offs.
(5) Evaluation and user simulation. These research directions involve multiple
research fields like information retrieval (IR), natural language processing
(NLP), and human-computer interaction (HCI). Based on these research
directions, we discuss some future challenges and opportunities. We provide a
road map for researchers from multiple communities to get started in this area.
We hope this survey can help to identify and address challenges in CRSs and
inspire future research."
171,"The recent adoption of recurrent neural networks (RNNs) for session modeling
has yielded substantial performance gains compared to previous approaches. In
terms of context-aware session modeling, however, the existing RNN-based models
are limited in that they are not designed to explicitly model rich static
user-side contexts (e.g., age, gender, location). Therefore, in this paper, we
explore the utility of explicit user-side context modeling for RNN session
models. Specifically, we propose an augmented RNN (ARNN) model that extracts
high-order user-contextual preference using the product-based neural network
(PNN) in order to augment any existing RNN session model. Evaluation results
show that our proposed model outperforms the baseline RNN session model by a
large margin when rich user-side contexts are available."
172,"Text categorization (TC) is the task of automatically organizing a set of
documents into a set of pre-defined categories. Over the last few years,
increased attention has been paid to the use of documents in digital form and
this makes text categorization becomes a challenging issue. The most
significant problem of text categorization is its huge number of features. Most
of these features are redundant, noisy and irrelevant that cause over fitting
with most of the classifiers. Hence, feature extraction is an important step to
improve the overall accuracy and the performance of the text classifiers. In
this paper, we will provide an overview of using principle component analysis
(PCA) as a feature extraction with various classifiers. It was observed that
the performance rate of the classifiers after using PCA to reduce the dimension
of data improved. Experiments are conducted on three UCI data sets, Classic03,
CNAE-9 and DBWorld e-mails. We compare the classification performance results
of using PCA with popular and well-known text classifiers. Results show that
using PCA encouragingly enhances classification performance on most of the
classifiers."
173,"Recommender system has been deployed in a large amount of real-world
applications, profoundly influencing people's daily life and
production.Traditional recommender models mostly collect as comprehensive as
possible user behaviors for accurate preference estimation. However,
considering the privacy, preference shaping and other issues, the users may not
want to disclose all their behaviors for training the model. In this paper, we
study a novel recommendation paradigm, where the users are allowed to indicate
their ""willingness"" on disclosing different behaviors, and the models are
optimized by trading-off the recommendation quality as well as the violation of
the user ""willingness"". More specifically, we formulate the recommendation
problem as a multiplayer game, where the action is a selection vector
representing whether the items are involved into the model training. For
efficiently solving this game, we design a tailored algorithm based on
influence function to lower the time cost for recommendation quality
exploration, and also extend it with multiple anchor selection vectors.We
conduct extensive experiments to demonstrate the effectiveness of our model on
balancing the recommendation quality and user disclosing willingness."
174,"CNNs, RNNs, GCNs, and CapsNets have shown significant insights in
representation learning and are widely used in various text mining tasks such
as large-scale multi-label text classification. However, most existing deep
models for multi-label text classification consider either the non-consecutive
and long-distance semantics or the sequential semantics, but how to consider
them both coherently is less studied. In addition, most existing methods treat
output labels as independent methods, but ignore the hierarchical relations
among them, leading to useful semantic information loss. In this paper, we
propose a novel hierarchical taxonomy-aware and attentional graph capsule
recurrent CNNs framework for large-scale multi-label text classification.
Specifically, we first propose to model each document as a word order preserved
graph-of-words and normalize it as a corresponding words-matrix representation
which preserves both the non-consecutive, long-distance and local sequential
semantics. Then the words-matrix is input to the proposed attentional graph
capsule recurrent CNNs for more effectively learning the semantic features. To
leverage the hierarchical relations among the class labels, we propose a
hierarchical taxonomy embedding method to learn their representations, and
define a novel weighted margin loss by incorporating the label representation
similarity. Extensive evaluations on three datasets show that our model
significantly improves the performance of large-scale multi-label text
classification by comparing with state-of-the-art approaches."
175,"Assigning qualified, unbiased and interested reviewers to paper submissions
is vital for maintaining the integrity and quality of the academic publishing
system and providing valuable reviews to authors. However, matching thousands
of submissions with thousands of potential reviewers within a limited time is a
daunting challenge for a conference program committee. Prior efforts based on
topic modeling have suffered from losing the specific context that help define
the topics in a publication or submission abstract. Moreover, in some cases,
topics identified are difficult to interpret. We propose an approach that
learns from each abstract published by a potential reviewer the topics studied
and the explicit context in which the reviewer studied the topics. Furthermore,
we contribute a new dataset for evaluating reviewer matching systems. Our
experiments show a significant, consistent improvement in precision when
compared with the existing methods. We also use examples to demonstrate why our
recommendations are more explainable. The new approach has been deployed
successfully at top-tier conferences in the last two years."
176,"Currently, the text document retrieval systems have many challenges in
exploring the semantics of queries and documents. Each query implies
information which does not appear in the query but the documents related with
the information are also expected by user. The disadvantage of the previous
spreading activation algorithms could be many irrelevant concepts added to the
query. In this paper, a proposed novel algorithm is only activate and add to
the query named entities which are related with original entities in the query
and explicit relations in the query."
177,"Text similarity detection aims at measuring the degree of similarity between
a pair of texts. Corpora available for text similarity detection are designed
to evaluate the algorithms to assess the paraphrase level among documents. In
this paper we present a textual German corpus for similarity detection. The
purpose of this corpus is to automatically assess the similarity between a pair
of texts and to evaluate different similarity measures, both for whole
documents or for individual sentences. Therefore we have calculated several
simple measures on our corpus based on a library of similarity functions."
178,"Personalized review-based rating prediction aims at leveraging existing
reviews to model user interests and item characteristics for rating prediction.
Most of the existing studies mainly encounter two issues. First, the rich
knowledge contained in the fine-grained aspects of each review and the
knowledge graph is rarely considered to complement the pure text for better
modeling user-item interactions. Second, the power of pre-trained language
models is not carefully studied for personalized review-based rating
prediction. To address these issues, we propose an approach named
Knowledge-aware Collaborative Filtering with Pre-trained Language Model
(KCF-PLM). For the first issue, to utilize rich knowledge, KCF-PLM develops a
transformer network to model the interactions of the extracted aspects w.r.t. a
user-item pair. For the second issue, to better represent users and items,
KCF-PLM takes all the historical reviews of a user or an item as input to
pre-trained language models. Moreover, KCF-PLM integrates the transformer
network and the pre-trained language models through representation propagation
on the knowledge graph and user-item guided attention of the aspect
representations. Thus KCF-PLM combines review text, aspect, knowledge graph,
and pre-trained language models together for review-based rating prediction. We
conduct comprehensive experiments on several public datasets, demonstrating the
effectiveness of KCF-PLM."
179,"Click-Through Rate (CTR) prediction has been an indispensable component for
many industrial applications, such as recommendation systems and online
advertising. CTR prediction systems are usually based on multi-field
categorical features, i.e., every feature is categorical and belongs to one and
only one field. Modeling feature conjunctions is crucial for CTR prediction
accuracy. However, it requires a massive number of parameters to explicitly
model all feature conjunctions, which is not scalable for real-world production
systems. In this paper, we describe a novel Field-Leveraged Embedding Network
(FLEN) which has been deployed in the commercial recommender system in Meitu
and serves the main traffic. FLEN devises a field-wise bi-interaction pooling
technique. By suitably exploiting field information, the field-wise
bi-interaction pooling captures both inter-field and intra-field feature
conjunctions with a small number of model parameters and an acceptable time
complexity for industrial applications. We show that a variety of
state-of-the-art CTR models can be expressed under this technique. Furthermore,
we develop Dicefactor: a dropout technique to prevent independent latent
features from co-adapting. Extensive experiments, including offline evaluations
and online A/B testing on real production systems, demonstrate the
effectiveness and efficiency of FLEN against the state-of-the-arts. Notably,
FLEN has obtained 5.19% improvement on CTR with 1/6 of memory usage and
computation time, compared to last version (i.e. NFM)."
180,"Information retrieval systems retrieves relevant documents based on a query
submitted by the user. The documents are initially indexed and the words in the
documents are assigned weights using a weighting technique called TFIDF which
is the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF
represents the number of occurrences of a term in a document. IDF measures
whether the term is common or rare across all documents. It is computed by
dividing the total number of documents in the system by the number of documents
containing the term and then computing the logarithm of the quotient. By
default, we use base 10 to calculate the logarithm. In this paper, we are going
to test this weighting technique by using a range of log bases from 0.1 to
100.0 to calculate the IDF. Testing different log bases for vector model
weighting technique is to highlight the importance of understanding the
performance of the system at different weighting values. We use the documents
of MED, CRAN, NPL, LISA, and CISI test collections that scientists assembled
explicitly for experiments in data information retrieval systems."
181,"News recommendation is different from movie or e-commercial recommendation as
people usually do not grade the news. Therefore, user feedback for news is
always implicit (click behavior, reading time, etc). Inevitably, there are
noises in implicit feedback. On one hand, the user may exit immediately after
clicking the news as he dislikes the news content, leaving the noise in his
positive implicit feedback; on the other hand, the user may be recommended
multiple interesting news at the same time and only click one of them,
producing the noise in his negative implicit feedback. Opposite implicit
feedback could construct more integrated user preferences and help each other
to minimize the noise influence. Previous works on news recommendation only
used positive implicit feedback and suffered from the noise impact. In this
paper, we propose a denoising neural network for news recommendation with
positive and negative implicit feedback, named DRPN. DRPN utilizes both
feedback for recommendation with a module to denoise both positive and negative
implicit feedback to further enhance the performance. Experiments on the
real-world large-scale dataset demonstrate the state-of-the-art performance of
DRPN."
182,"With the emergence of large language models (LLMs) and their ability to
perform a variety of tasks, their application in recommender systems (RecSys)
has shown promise. However, we are facing significant challenges when deploying
LLMs into RecSys, such as limited prompt length, unstructured item information,
and un-constrained generation of recommendations, leading to sub-optimal
performance. To address these issues, we propose a novel method using a
taxonomy dictionary. This method provides a systematic framework for
categorizing and organizing items, improving the clarity and structure of item
information. By incorporating the taxonomy dictionary into LLM prompts, we
achieve efficient token utilization and controlled feature generation, leading
to more accurate and contextually relevant recommendations. Our Taxonomy-guided
Recommendation (TaxRec) approach features a two-step process: one-time taxonomy
categorization and LLM-based recommendation, enabling zero-shot recommendations
without the need for domain-specific fine-tuning. Experimental results
demonstrate TaxRec significantly enhances recommendation quality compared to
traditional zero-shot approaches, showcasing its efficacy as personal
recommender with LLMs. Code is available at
https://github.com/yueqingliang1/TaxRec."
183,"A framework named Copula Component Analysis (CCA) for blind source separation
is proposed as a generalization of Independent Component Analysis (ICA). It
differs from ICA which assumes independence of sources that the underlying
components may be dependent with certain structure which is represented by
Copula. By incorporating dependency structure, much accurate estimation can be
made in principle in the case that the assumption of independence is
invalidated. A two phrase inference method is introduced for CCA which is based
on the notion of multidimensional ICA."
184,"This paper learns multi-modal embeddings from text, audio, and video
views/modes of data in order to improve upon down-stream sentiment
classification. The experimental framework also allows investigation of the
relative contributions of the individual views in the final multi-modal
embedding. Individual features derived from the three views are combined into a
multi-modal embedding using Deep Canonical Correlation Analysis (DCCA) in two
ways i) One-Step DCCA and ii) Two-Step DCCA. This paper learns text embeddings
using BERT, the current state-of-the-art in text encoders. We posit that this
highly optimized algorithm dominates over the contribution of other views,
though each view does contribute to the final result. Classification tasks are
carried out on two benchmark datasets and on a new Debate Emotion data set, and
together these demonstrate that the one-Step DCCA outperforms the current
state-of-the-art in learning multi-modal embeddings."
185,"Point-of-Interest (POI) recommendation plays a vital role in various
location-aware services. It has been observed that POI recommendation is driven
by both sequential and geographical influences. However, since there is no
annotated label of the dominant influence during recommendation, existing
methods tend to entangle these two influences, which may lead to sub-optimal
recommendation performance and poor interpretability. In this paper, we address
the above challenge by proposing DisenPOI, a novel Disentangled dual-graph
framework for POI recommendation, which jointly utilizes sequential and
geographical relationships on two separate graphs and disentangles the two
influences with self-supervision. The key novelty of our model compared with
existing approaches is to extract disentangled representations of both
sequential and geographical influences with contrastive learning. To be
specific, we construct a geographical graph and a sequential graph based on the
check-in sequence of a user. We tailor their propagation schemes to become
sequence-/geo-aware to better capture the corresponding influences. Preference
proxies are extracted from check-in sequence as pseudo labels for the two
influences, which supervise the disentanglement via a contrastive loss.
Extensive experiments on three datasets demonstrate the superiority of the
proposed model."
186,"We enhance the autonomy of the continuous active learning method shown by
Cormack and Grossman (SIGIR 2014) to be effective for technology-assisted
review, in which documents from a collection are retrieved and reviewed, using
relevance feedback, until substantially all of the relevant documents have been
reviewed. Autonomy is enhanced through the elimination of topic-specific and
dataset-specific tuning parameters, so that the sole input required by the user
is, at the outset, a short query, topic description, or single relevant
document; and, throughout the review, ongoing relevance assessments of the
retrieved documents. We show that our enhancements consistently yield superior
results to Cormack and Grossman's version of continuous active learning, and
other methods, not only on average, but on the vast majority of topics from
four separate sets of tasks: the legal datasets examined by Cormack and
Grossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and
the construction of the TREC 2002 filtering test collection."
187,"Recently, deep neural networks such as RNN, CNN and Transformer have been
applied in the task of sequential recommendation, which aims to capture the
dynamic preference characteristics from logged user behavior data for accurate
recommendation. However, in online platforms, logged user behavior data is
inevitable to contain noise, and deep recommendation models are easy to overfit
on these logged data. To tackle this problem, we borrow the idea of filtering
algorithms from signal processing that attenuates the noise in the frequency
domain. In our empirical experiments, we find that filtering algorithms can
substantially improve representative sequential recommendation models, and
integrating simple filtering algorithms (eg Band-Stop Filter) with an all-MLP
architecture can even outperform competitive Transformer-based models.
Motivated by it, we propose \textbf{FMLP-Rec}, an all-MLP model with learnable
filters for sequential recommendation task. The all-MLP architecture endows our
model with lower time complexity, and the learnable filters can adaptively
attenuate the noise information in the frequency domain. Extensive experiments
conducted on eight real-world datasets demonstrate the superiority of our
proposed method over competitive RNN, CNN, GNN and Transformer-based methods.
Our code and data are publicly available at the link:
\textcolor{blue}{\url{https://github.com/RUCAIBox/FMLP-Rec}}."
188,"In the rapidly evolving field of artificial intelligence, transformer-based
models have gained significant attention in the context of Sequential
Recommender Systems (SRSs), demonstrating remarkable proficiency in capturing
user-item interactions. However, such attention-based frameworks result in
substantial computational overhead and extended inference time. To address this
problem, this paper proposes a novel efficient sequential recommendation
framework GLINT-RU that leverages dense selective Gated Recurrent Units (GRU)
module to accelerate the inference speed, which is a pioneering work to further
exploit the potential of efficient GRU modules in SRSs. The GRU module lies at
the heart of GLINT-RU, playing a crucial role in substantially reducing both
inference time and GPU memory usage. Through the integration of a dense
selective gate, our framework adeptly captures both long-term and short-term
item dependencies, enabling the adaptive generation of item scores. GLINT-RU
further integrates a mixing block, enriching it with global user-item
interaction information to bolster recommendation quality. Moreover, we design
a gated Multi-layer Perceptron (MLP) for our framework where the information is
deeply filtered. Extensive experiments on three datasets are conducted to
highlight the effectiveness and efficiency of GLINT-RU. Our GLINT-RU achieves
exceptional inference speed and prediction accuracy, outperforming existing
baselines based on Recurrent Neural Network (RNN), Transformer, MLP and State
Space Model (SSM). These results establish a new standard in sequential
recommendation, highlighting the potential of GLINT-RU as a renewing approach
in the realm of recommender systems."
189,"Clinical coding is an administrative process that involves the translation of
diagnostic data from episodes of care into a standard code format such as
ICD10. It has many critical applications such as billing and aetiology
research. The automation of clinical coding is very challenging due to data
sparsity, low interoperability of digital health systems, complexity of
real-life diagnosis coupled with the huge size of ICD10 code space. Related
work suffer from low applicability due to reliance on many data sources,
inefficient modelling and less generalizable solutions. We propose a novel
collaborative residual learning based model to automatically predict ICD10
codes employing only prescriptions data. Extensive experiments were performed
on two real-world clinical datasets (outpatient & inpatient) from Maharaj
Nakorn Chiang Mai Hospital with real case-mix distributions. We obtain
multi-label classification accuracy of 0.71 and 0.57 of average precision, 0.57
and 0.38 of F1-score and 0.73 and 0.44 of accuracy in predicting principal
diagnosis for inpatient and outpatient datasets respectively."
190,"The need for diversification of recommendation lists manifests in a number of
recommender systems use cases. However, an increase in diversity may undermine
the utility of the recommendations, as relevant items in the list may be
replaced by more diverse ones. In this work we propose a novel method for
maximizing the utility of the recommended items subject to the diversity of
user's tastes, and show that an optimal solution to this problem can be found
greedily. We evaluate the proposed method in two online user studies as well as
in an offline analysis incorporating a number of evaluation metrics. The
results of evaluations show the superiority of our method over a number of
baselines."
191,"Prior work on English monolingual retrieval has shown that a cross-encoder
trained using a large number of relevance judgments for query-document pairs
can be used as a teacher to train more efficient, but similarly effective,
dual-encoder student models. Applying a similar knowledge distillation approach
to training an efficient dual-encoder model for Cross-Language Information
Retrieval (CLIR), where queries and documents are in different languages, is
challenging due to the lack of a sufficiently large training collection when
the query and document languages differ. The state of the art for CLIR thus
relies on translating queries, documents, or both from the large English MS
MARCO training set, an approach called Translate-Train. This paper proposes an
alternative, Translate-Distill, in which knowledge distillation from either a
monolingual cross-encoder or a CLIR cross-encoder is used to train a
dual-encoder CLIR student model. This richer design space enables the teacher
model to perform inference in an optimized setting, while training the student
model directly for CLIR. Trained models and artifacts are publicly available on
Huggingface."
192,"Assessing the trustworthiness of artificial intelligence systems requires
knowledge from many different disciplines. These disciplines do not necessarily
share concepts between them and might use words with different meanings, or
even use the same words differently. Additionally, experts from different
disciplines might not be aware of specialized terms readily used in other
disciplines. Therefore, a core challenge of the assessment process is to
identify when experts from different disciplines talk about the same problem
but use different terminologies. In other words, the problem is to group
problem descriptions (a.k.a. issues) with the same semantic meaning but
described using slightly different terminologies.
  In this work, we show how we employed recent advances in natural language
processing, namely sentence embeddings and semantic textual similarity, to
support this identification process and to bridge communication gaps in
interdisciplinary teams of experts assessing the trustworthiness of an
artificial intelligence system used in healthcare."
193,"Recommender systems help users deal with information overload by providing
tailored item suggestions to them. The recommendation of news is often
considered to be challenging, since the relevance of an article for a user can
depend on a variety of factors, including the user's short-term reading
interests, the reader's context, or the recency or popularity of an article.
Previous work has shown that the use of Recurrent Neural Networks is promising
for the next-in-session prediction task, but has certain limitations when only
recorded item click sequences are used as input. In this work, we present a
contextual hybrid, deep learning based approach for session-based news
recommendation that is able to leverage a variety of information types. We
evaluated our approach on two public datasets, using a temporal evaluation
protocol that simulates the dynamics of a news portal in a realistic way. Our
results confirm the benefits of considering additional types of information,
including article popularity and recency, in the proposed way, resulting in
significantly higher recommendation accuracy and catalog coverage than other
session-based algorithms. Additional experiments show that the proposed
parameterizable loss function used in our method also allows us to balance two
usually conflicting quality factors, accuracy and novelty.
  Keywords: Artificial Neural Networks, Context-Aware Recommender Systems,
Hybrid Recommender Systems, News Recommender Systems, Session-based
Recommendation"
194,"This paper describes PinView, a content-based image retrieval system that
exploits implicit relevance feedback collected during a search session. PinView
contains several novel methods to infer the intent of the user. From relevance
feedback, such as eye movements or pointer clicks, and visual features of
images, PinView learns a similarity metric between images which depends on the
current interests of the user. It then retrieves images with a specialized
online learning algorithm that balances the tradeoff between exploring new
images and exploiting the already inferred interests of the user. We have
integrated PinView to the content-based image retrieval system PicSOM, which
enables applying PinView to real-world image databases. With the new algorithms
PinView outperforms the original PicSOM, and in online experiments with real
users the combination of implicit and explicit feedback gives the best results."
195,"A long user history inevitably reflects the transitions of personal interests
over time. The analyses on the user history require the robust sequential model
to anticipate the transitions and the decays of user interests. The user
history is often modeled by various RNN structures, but the RNN structures in
the recommendation system still suffer from the long-term dependency and the
interest drifts. To resolve these challenges, we suggest HCRNN with three
hierarchical contexts of the global, the local, and the temporary interests.
This structure is designed to withhold the global long-term interest of users,
to reflect the local sub-sequence interests, and to attend the temporary
interests of each transition. Besides, we propose a hierarchical context-based
gate structure to incorporate our \textit{interest drift assumption}. As we
suggest a new RNN structure, we support HCRNN with a complementary
\textit{bi-channel attention} structure to utilize hierarchical context. We
experimented the suggested structure on the sequential recommendation tasks
with CiteULike, MovieLens, and LastFM, and our model showed the best
performances in the sequential recommendations."
196,"We present a novel summarization framework for reviews of products and
services by selecting informative and concise text segments from the reviews.
Our method consists of two major steps. First, we identify five frequently
occurring variable-length syntactic patterns and use them to extract candidate
segments. Then we use the output of a joint generative sentiment topic model to
filter out the non-informative segments. We verify the proposed method with
quantitative and qualitative experiments. In a quantitative study, our approach
outperforms previous methods in producing informative segments and summaries
that capture aspects of products and services as expressed in the
user-generated pros and cons lists. Our user study with ninety users resonates
with this result: individual segments extracted and filtered by our method are
rated as more useful by users compared to previous approaches by users."
197,"While entity-oriented neural IR models have advanced significantly, they
often overlook a key nuance: the varying degrees of influence individual
entities within a document have on its overall relevance. Addressing this gap,
we present DREQ, an entity-oriented dense document re-ranking model. Uniquely,
we emphasize the query-relevant entities within a document's representation
while simultaneously attenuating the less relevant ones, thus obtaining a
query-specific entity-centric document representation. We then combine this
entity-centric document representation with the text-centric representation of
the document to obtain a ""hybrid"" representation of the document. We learn a
relevance score for the document using this hybrid representation. Using four
large-scale benchmarks, we show that DREQ outperforms state-of-the-art neural
and non-neural re-ranking methods, highlighting the effectiveness of our
entity-oriented representation approach."
198,"We consider the problem of ranking $N$ objects starting from a set of noisy
pairwise comparisons provided by a crowd of equal workers. We assume that
objects are endowed with intrinsic qualities and that the probability with
which an object is preferred to another depends only on the difference between
the qualities of the two competitors. We propose a class of non-adaptive
ranking algorithms that rely on a least-squares optimization criterion for the
estimation of qualities. Such algorithms are shown to be asymptotically optimal
(i.e., they require $O(\frac{N}{\epsilon^2}\log \frac{N}{\delta})$ comparisons
to be $(\epsilon, \delta)$-PAC). Numerical results show that our schemes are
very efficient also in many non-asymptotic scenarios exhibiting a performance
similar to the maximum-likelihood algorithm. Moreover, we show how they can be
extended to adaptive schemes and test them on real-world datasets."
199,"A significant amount of search queries originate from some real world
information need or tasks. In order to improve the search experience of the end
users, it is important to have accurate representations of tasks. As a result,
significant amount of research has been devoted to extracting proper
representations of tasks in order to enable search systems to help users
complete their tasks, as well as providing the end user with better query
suggestions, for better recommendations, for satisfaction prediction, and for
improved personalization in terms of tasks. Most existing task extraction
methodologies focus on representing tasks as flat structures. However, tasks
often tend to have multiple subtasks associated with them and a more
naturalistic representation of tasks would be in terms of a hierarchy, where
each task can be composed of multiple (sub)tasks. To this end, we propose an
efficient Bayesian nonparametric model for extracting hierarchies of such tasks
\& subtasks. We evaluate our method based on real world query log data both
through quantitative and crowdsourced experiments and highlight the importance
of considering task/subtask hierarchies."
200,"Pretrained contextualized representations offer great success for many
downstream tasks, including document ranking. The multilingual versions of such
pretrained representations provide a possibility of jointly learning many
languages with the same model. Although it is expected to gain big with such
joint training, in the case of cross lingual information retrieval (CLIR), the
models under a multilingual setting are not achieving the same level of
performance as those under a monolingual setting. We hypothesize that the
performance drop is due to the translation gap between query and documents. In
the monolingual retrieval task, because of the same lexical inputs, it is
easier for model to identify the query terms that occurred in documents.
However, in the multilingual pretrained models that the words in different
languages are projected into the same hyperspace, the model tends to translate
query terms into related terms, i.e., terms that appear in a similar context,
in addition to or sometimes rather than synonyms in the target language. This
property is creating difficulties for the model to connect terms that cooccur
in both query and document. To address this issue, we propose a novel Mixed
Attention Transformer (MAT) that incorporates external word level knowledge,
such as a dictionary or translation table. We design a sandwich like
architecture to embed MAT into the recent transformer based deep neural models.
By encoding the translation knowledge into an attention matrix, the model with
MAT is able to focus on the mutually translated words in the input sequence.
Experimental results demonstrate the effectiveness of the external knowledge
and the significant improvement of MAT embedded neural reranking model on CLIR
task."
201,"Multi-Objective Recommender Systems (MORSs) emerged as a paradigm to
guarantee multiple (often conflicting) goals. Besides accuracy, a MORS can
operate at the global level, where additional beyond-accuracy goals are met for
the system as a whole, or at the individual level, meaning that the
recommendations are tailored to the needs of each user. The state-of-the-art
MORSs either operate at the global or individual level, without assuming the
co-existence of the two perspectives. In this study, we show that when global
and individual objectives co-exist, MORSs are not able to meet both types of
goals. To overcome this issue, we present an approach that regulates the
recommendation lists so as to guarantee both global and individual
perspectives, while preserving its effectiveness. Specifically, as individual
perspective, we tackle genre calibration and, as global perspective, provider
fairness. We validate our approach on two real-world datasets, publicly
released with this paper."
202,"Understanding and analyzing big data is firmly recognized as a powerful and
strategic priority. For deeper interpretation of and better intelligence with
big data, it is important to transform raw data (unstructured, semi-structured
and structured data sources, e.g., text, video, image data sets) into curated
data: contextualized data and knowledge that is maintained and made available
for use by end-users and applications. In particular, data curation acts as the
glue between raw data and analytics, providing an abstraction layer that
relieves users from time consuming, tedious and error prone curation tasks. In
this context, the data curation process becomes a vital analytics asset for
increasing added value and insights.
  In this paper, we identify and implement a set of curation APIs and make them
available (on GitHub) to researchers and developers to assist them transforming
their raw data into curated data. The curation APIs enable developers to easily
add features - such as extracting keyword, part of speech, and named entities
such as Persons, Locations, Organizations, Companies, Products, Diseases,
Drugs, etc.; providing synonyms and stems for extracted information items
leveraging lexical knowledge bases for the English language such as WordNet;
linking extracted entities to external knowledge bases such as Google Knowledge
Graph and Wikidata; discovering similarity among the extracted information
items, such as calculating similarity between string, number, date and time
data; classifying, sorting and categorizing data into various types, forms or
any other distinct class; and indexing structured and unstructured data - into
their applications."
203,"We present a study on predicting the factuality of reporting and bias of news
media. While previous work has focused on studying the veracity of claims or
documents, here we are interested in characterizing entire news media. These
are under-studied but arguably important research problems, both in their own
right and as a prior for fact-checking systems. We experiment with a large list
of news websites and with a rich set of features derived from (i) a sample of
articles from the target news medium, (ii) its Wikipedia page, (iii) its
Twitter account, (iv) the structure of its URL, and (v) information about the
Web traffic it attracts. The experimental results show sizable performance
gains over the baselines, and confirm the importance of each feature type."
204,"To make Sequential Recommendation (SR) successful, recent works focus on
designing effective sequential encoders, fusing side information, and mining
extra positive self-supervision signals. The strategy of sampling negative
items at each time step is less explored. Due to the dynamics of users'
interests and model updates during training, considering randomly sampled items
from a user's non-interacted item set as negatives can be uninformative. As a
result, the model will inaccurately learn user preferences toward items.
Identifying informative negatives is challenging because informative negative
items are tied with both dynamically changed interests and model parameters
(and sampling process should also be efficient). To this end, we propose to
Generate Negative Samples (items) for SR (GenNi). A negative item is sampled at
each time step based on the current SR model's learned user preferences toward
items. An efficient implementation is proposed to further accelerate the
generation process, making it scalable to large-scale recommendation tasks.
Extensive experiments on four public datasets verify the importance of
providing high-quality negative samples for SR and demonstrate the
effectiveness and efficiency of GenNi."
205,"Recommender systems recommend items more accurately by analyzing users'
potential interest on different brands' items. In conjunction with users'
rating similarity, the presence of users' implicit feedbacks like clicking
items, viewing items specifications, watching videos etc. have been proved to
be helpful for learning users' embedding, that helps better rating prediction
of users. Most existing recommender systems focus on modeling of ratings and
implicit feedbacks ignoring users' explicit feedbacks. Explicit feedbacks can
be used to validate the reliability of the particular users and can be used to
learn about the users' characteristic. Users' characteristic mean what type of
reviewers they are. In this paper, we explore three different models for
recommendation with more accuracy focusing on users' explicit feedbacks and
implicit feedbacks. First one is RHC-PMF that predicts users' rating more
accurately based on user's three explicit feedbacks (rating, helpfulness score
and centrality) and second one is RV-PMF, where user's implicit feedback (view
relationship) is considered. Last one is RHCV-PMF, where both type of feedbacks
are considered. In this model users' explicit feedbacks' similarity indicate
the similarity of their reliability and characteristic and implicit feedback's
similarity indicates their preference similarity. Extensive experiments on real
world dataset, i.e. Amazon.com online review dataset shows that our models
perform better compare to base-line models in term of users' rating prediction.
RHCV-PMF model also performs better rating prediction compare to baseline
models for cold start users and cold start items."
206,"We present a method for a wine recommendation system that employs
multidimensional clustering and unsupervised learning methods. Our algorithm
first performs clustering on a large corpus of wine reviews. It then uses the
resulting wine clusters as an approximation of the most common flavor palates,
recommending a user a wine by optimizing over a price-quality ratio within
clusters that they demonstrated a preference for."
207,"More than ever, technical inventions are the symbol of our society's advance.
Patents guarantee their creators protection against infringement. For an
invention being patentable, its novelty and inventiveness have to be assessed.
Therefore, a search for published work that describes similar inventions to a
given patent application needs to be performed. Currently, this so-called
search for prior art is executed with semi-automatically composed keyword
queries, which is not only time consuming, but also prone to errors. In
particular, errors may systematically arise by the fact that different keywords
for the same technical concepts may exist across disciplines. In this paper, a
novel approach is proposed, where the full text of a given patent application
is compared to existing patents using machine learning and natural language
processing techniques to automatically detect inventions that are similar to
the one described in the submitted document. Various state-of-the-art
approaches for feature extraction and document comparison are evaluated. In
addition to that, the quality of the current search process is assessed based
on ratings of a domain expert. The evaluation results show that our automated
approach, besides accelerating the search process, also improves the search
results for prior art with respect to their quality."
208,"The problem of disambiguation of company names poses a significant challenge
in extracting useful information from patents. This issue biases research
outcomes as it mostly underestimates the number of patents attributed to
companies, particularly multinational corporations which file patents under a
plethora of names, including alternate spellings of the same entity and,
eventually, companies' subsidiaries. To date, addressing these challenges has
relied on labor-intensive dictionary based or string matching approaches,
leaving the problem of patents' assignee harmonization on large datasets mostly
unresolved. To bridge this gap, this paper describes the Terrorizer algorithm,
a text-based algorithm that leverages natural language processing (NLP),
network theory, and rule-based techniques to harmonize the variants of company
names recorded as patent assignees. In particular, the algorithm follows the
tripartite structure of its antecedents, namely parsing, matching and filtering
stage, adding an original ""knowledge augmentation"" phase which is used to
enrich the information available on each assignee name. We use Terrorizer on a
set of 325'917 companies' names who are assignees of patents granted by the
USPTO from 2005 to 2022. The performance of Terrorizer is evaluated on four
gold standard datasets. This validation step shows us two main things: the
first is that the performance of Terrorizer is similar over different kind of
datasets, proving that our algorithm generalizes well. Second, when comparing
its performance with the one of the algorithm currently used in PatentsView for
the same task (Monath et al., 2021), it achieves a higher F1 score. Finally, we
use the Tree-structured Parzen Estimator (TPE) optimization algorithm for the
hyperparameters' tuning. Our final result is a reduction in the initial set of
names of over 42%."
209,"Searching, browsing, and recommendations are common ways in which the ""choice
overload"" faced by users in the online marketplace can be mitigated. In this
paper we propose the use of hierarchical item categories, obtained from
implicit feedback data, to enable efficient browsing and recommendations. We
present a method of creating hierarchical item categories from implicit
feedback data only i.e., without any other information on the items like name,
genre etc. Categories created in this fashion are based on users'
co-consumption of items. Thus, they can be more useful for users in finding
interesting and relevant items while they are browsing through the hierarchy.
We also show that this item hierarchy can be useful in making category based
recommendations, which makes the recommendations more explainable and increases
the diversity of the recommendations without compromising much on the accuracy.
Item hierarchy can also be useful in the creation of an automatic item taxonomy
skeleton by bypassing manual labeling and annotation. This can especially be
useful for small vendors. Our data-driven hierarchical categories are based on
hierarchical latent tree analysis (HLTA) which has been previously used for
text analysis. We present a scaled up learning algorithm \emph{HLTA-Forest} so
that HLTA can be applied to implicit feedback data."
210,"The integration of Linked Open Data (LOD) content in Web pages is a
challenging and sometimes tedious task for Web developers. At the same moment,
most software packages for blogs, content management systems (CMS), and shop
applications support the consumption of feed formats, namely RSS and Atom. In
this technical report, we demonstrate an on-line tool that fetches e-commerce
data from a SPARQL endpoint and syndicates obtained results as RSS or Atom
feeds. Our approach combines (1) the popularity and broad tooling support of
existing feed formats, (2) the precision of queries against structured data
built upon common Web vocabularies like schema.org, GoodRelations, FOAF, VCard,
and WGS 84, and (3) the ease of integrating content from a large number of Web
sites and other data sources in RDF in general."
211,"Recommendation techniques are important approaches for alleviating
information overload. Being often trained on implicit user feedback, many
recommenders suffer from the sparsity challenge due to the lack of explicitly
negative samples. The GAN-style recommenders (i.e., IRGAN) addresses the
challenge by learning a generator and a discriminator adversarially, such that
the generator produces increasingly difficult samples for the discriminator to
accelerate optimizing the discrimination objective. However, producing samples
from the generator is very time-consuming, and our empirical study shows that
the discriminator performs poor in top-k item recommendation. To this end, a
theoretical analysis is made for the GAN-style algorithms, showing that the
generator of limit capacity is diverged from the optimal generator. This may
interpret the limitation of discriminator's performance. Based on these
findings, we propose a Sampling-Decomposable Generative Adversarial Recommender
(SD-GAR). In the framework, the divergence between some generator and the
optimum is compensated by self-normalized importance sampling; the efficiency
of sample generation is improved with a sampling-decomposable generator, such
that each sample can be generated in O(1) with the Vose-Alias method.
Interestingly, due to decomposability of sampling, the generator can be
optimized with the closed-form solutions in an alternating manner, being
different from policy gradient in the GAN-style algorithms. We extensively
evaluate the proposed algorithm with five real-world recommendation datasets.
The results show that SD-GAR outperforms IRGAN by 12.4% and the SOTA
recommender by 10% on average. Moreover, discriminator training can be 20x
faster on the dataset with more than 120K items."
212,"Providing visual summaries of scientific publications can increase
information access for readers and thereby help deal with the exponential
growth in the number of scientific publications. Nonetheless, efforts in
providing visual publication summaries have been few and far apart, primarily
focusing on the biomedical domain. This is primarily because of the limited
availability of annotated gold standards, which hampers the application of
robust and high-performing supervised learning techniques. To address these
problems we create a new benchmark dataset for selecting figures to serve as
visual summaries of publications based on their abstracts, covering several
domains in computer science. Moreover, we develop a self-supervised learning
approach, based on heuristic matching of inline references to figures with
figure captions. Experiments in both biomedical and computer science domains
show that our model is able to outperform the state of the art despite being
self-supervised and therefore not relying on any annotated training data."
213,"The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a
large scale of in-domain relevance training signals, which are not always
available in real-world ranking scenarios. To democratize the benefits of
Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method
that generalizes Neu-IR models from label-rich source domains to few-shot
target domains. Drawing on source-domain massive relevance supervision,
MetaAdaptRank contrastively synthesizes a large number of weak supervision
signals for target domains and meta-learns to reweight these synthetic ""weak""
data based on their benefits to the target-domain ranking accuracy of Neu-IR
models. Experiments on three TREC benchmarks in the web, news, and biomedical
domains show that MetaAdaptRank significantly improves the few-shot ranking
accuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives
from both its contrastive weak data synthesis and meta-reweighted data
selection. The code and data of this paper can be obtained from
https://github.com/thunlp/MetaAdaptRank."
214,"Providing access to information across languages has been a goal of
Information Retrieval (IR) for decades. While progress has been made on Cross
Language IR (CLIR) where queries are expressed in one language and documents in
another, the multilingual (MLIR) task to create a single ranked list of
documents across many languages is considerably more challenging. This paper
investigates whether advances in neural document translation and pretrained
multilingual neural language models enable improvements in the state of the art
over earlier MLIR techniques. The results show that although combining neural
document translation with neural ranking yields the best Mean Average Precision
(MAP), 98% of that MAP score can be achieved with an 84% reduction in indexing
time by using a pretrained XLM-R multilingual language model to index documents
in their native language, and that 2% difference in effectiveness is not
statistically significant. Key to achieving these results for MLIR is to
fine-tune XLM-R using mixed-language batches from neural translations of MS
MARCO passages."
215,"Collaborative filtering (CF), as a fundamental approach for recommender
systems, is usually built on the latent factor model with learnable parameters
to predict users' preferences towards items. However, designing a proper CF
model for a given data is not easy, since the properties of datasets are highly
diverse. In this paper, motivated by the recent advances in automated machine
learning (AutoML), we propose to design a data-specific CF model by AutoML
techniques. The key here is a new framework that unifies state-of-the-art
(SOTA) CF methods and splits them into disjoint stages of input encoding,
embedding function, interaction function, and prediction function. We further
develop an easy-to-use, robust, and efficient search strategy, which utilizes
random search and a performance predictor for efficient searching within the
above framework. In this way, we can combinatorially generalize data-specific
CF models, which have not been visited in the literature, from SOTA ones.
Extensive experiments on five real-world datasets demonstrate that our method
can consistently outperform SOTA ones for various CF tasks. Further experiments
verify the rationality of the proposed framework and the efficiency of the
search strategy. The searched CF models can also provide insights for exploring
more effective methods in the future"
216,"Session length is a very important aspect in determining a user's
satisfaction with a media streaming service. Being able to predict how long a
session will last can be of great use for various downstream tasks, such as
recommendations and ad scheduling. Most of the related literature on user
interaction duration has focused on dwell time for websites, usually in the
context of approximating post-click satisfaction either in search results, or
display ads. In this work we present the first analysis of session length in a
mobile-focused online service, using a real world data-set from a major music
streaming service. We use survival analysis techniques to show that the
characteristics of the length distributions can differ significantly between
users, and use gradient boosted trees with appropriate objectives to predict
the length of a session using only information available at its beginning. Our
evaluation on real world data illustrates that our proposed technique
outperforms the considered baseline."
217,"In today's data-rich environment, recommender systems play a crucial role in
decision support systems. They provide to users personalized recommendations
and explanations about these recommendations. Embedding-based models, despite
their widespread use, often suffer from a lack of interpretability, which can
undermine trust and user engagement. This paper presents an approach that
combines embedding-based and semantic-based models to generate post-hoc
explanations in recommender systems, leveraging ontology-based knowledge graphs
to improve interpretability and explainability. By organizing data within a
structured framework, ontologies enable the modeling of intricate relationships
between entities, which is essential for generating explanations. By combining
embedding-based and semantic based models for post-hoc explanations in
recommender systems, the framework we defined aims at producing meaningful and
easy-to-understand explanations, enhancing user trust and satisfaction, and
potentially promoting the adoption of recommender systems across the e-commerce
sector."
218,"The classification of television content helps users organise and navigate
through the large list of channels and programs now available. In this paper,
we address the problem of television content classification by exploiting text
information extracted from program transcriptions. We present an analysis which
adapts a model for sentiment that has been widely and successfully applied in
other fields such as music or blog posts. We use a real-world dataset obtained
from the Boxfish API to compare the performance of classifiers trained on a
number of different feature sets. Our experiments show that, over a large
collection of television content, program genres can be represented in a
three-dimensional space of valence, arousal and dominance, and that promising
classification results can be achieved using features based on this
representation. This finding supports the use of the proposed representation of
television content as a feature space for similarity computation and
recommendation generation."
219,"Recommender systems aim to find an accurate and efficient mapping from
historic data of user-preferred items to a new item that is to be liked by a
user. Towards this goal, energy-based sequence generative adversarial nets
(EB-SeqGANs) are adopted for recommendation by learning a generative model for
the time series of user-preferred items. By recasting the energy function as
the feature function, the proposed EB-SeqGANs is interpreted as an instance of
maximum-entropy imitation learning."
220,"Reinforcement learning based recommender systems (RL-based RS) aim at
learning a good policy from a batch of collected data, by casting
recommendations to multi-step decision-making tasks. However, current RL-based
RS research commonly has a large reality gap. In this paper, we introduce the
first open-source real-world dataset, RL4RS, hoping to replace the artificial
datasets and semi-simulated RS datasets previous studies used due to the
resource limitation of the RL-based RS domain. Unlike academic RL research,
RL-based RS suffers from the difficulties of being well-validated before
deployment. We attempt to propose a new systematic evaluation framework,
including evaluation of environment simulation, evaluation on environments,
counterfactual policy evaluation, and evaluation on environments built from
test set. In summary, the RL4RS (Reinforcement Learning for Recommender
Systems), a new resource with special concerns on the reality gaps, contains
two real-world datasets, data understanding tools, tuned simulation
environments, related advanced RL baselines, batch RL baselines, and
counterfactual policy evaluation algorithms. The RL4RS suite can be found at
https://github.com/fuxiAIlab/RL4RS. In addition to the RL-based recommender
systems, we expect the resource to contribute to research in applied
reinforcement learning."
221,"With the prevalence of Internet of Things (IoT)-based social media
applications, the distance among people has been greatly shortened. As a
result, recommender systems in IoT-based social media need to be developed
oriented to groups of users rather than individual users. However, existing
methods were highly dependent on explicit preference feedbacks, ignoring
scenarios of implicit feedback. To remedy such gap, this paper proposes an
implicit feedback-based group recommender system using probabilistic inference
and non-cooperative game(GREPING) for IoT-based social media. Particularly,
unknown process variables can be estimated from observable implicit feedbacks
via Bayesian posterior probability inference. In addition, the globally optimal
recommendation results can be calculated with the aid of non-cooperative game.
Two groups of experiments are conducted to assess the GREPING from two aspects:
efficiency and robustness. Experimental results show obvious promotion and
considerable stability of the GREPING compared to baseline methods."
222,"This paper explores meta-learning in sequential recommendation to alleviate
the item cold-start problem. Sequential recommendation aims to capture user's
dynamic preferences based on historical behavior sequences and acts as a key
component of most online recommendation scenarios. However, most previous
methods have trouble recommending cold-start items, which are prevalent in
those scenarios. As there is generally no side information in the setting of
sequential recommendation task, previous cold-start methods could not be
applied when only user-item interactions are available. Thus, we propose a
Meta-learning-based Cold-Start Sequential Recommendation Framework, namely
Mecos, to mitigate the item cold-start problem in sequential recommendation.
This task is non-trivial as it targets at an important problem in a novel and
challenging context. Mecos effectively extracts user preference from limited
interactions and learns to match the target cold-start item with the potential
user. Besides, our framework can be painlessly integrated with neural
network-based models. Extensive experiments conducted on three real-world
datasets verify the superiority of Mecos, with the average improvement up to
99%, 91%, and 70% in HR@10 over state-of-the-art baseline methods."
223,"Keyphrase provides accurate information of document content that is highly
compact, concise, full of meanings, and widely used for discourse
comprehension, organization, and text retrieval. Though previous studies have
made substantial efforts for automated keyphrase extraction and generation,
surprisingly, few studies have been made for \textit{keyphrase completion}
(KPC). KPC aims to generate more keyphrases for document (e.g. scientific
publication) taking advantage of document content along with a very limited
number of known keyphrases, which can be applied to improve text indexing
system, etc. In this paper, we propose a novel KPC method with an
encoder-decoder framework. We name it \textit{deep keyphrase completion} (DKPC)
since it attempts to capture the deep semantic meaning of the document content
together with known keyphrases via a deep learning framework. Specifically, the
encoder and the decoder in DKPC play different roles to make full use of the
known keyphrases. The former considers the keyphrase-guiding factors, which
aggregates information of known keyphrases into context. On the contrary, the
latter considers the keyphrase-inhibited factor to inhibit semantically
repeated keyphrase generation. Extensive experiments on benchmark datasets
demonstrate the efficacy of our proposed model."
224,"Password authentication is a very important system security procedure to gain
access to user resources. In the Traditional password authentication methods a
server has check the authenticity of the users. In our proposed method users
can freely select their passwords from a predefined character set. They can
also use a graphical image as password. The password may be a character or an
image it will be converted into binary form and the binary values will be
normalized. Associative memories have been used recently for password
authentication in order to overcome drawbacks of the traditional password
authentication methods. In this paper we proposed a method using Bidirectional
Associative Memory algorithm for both alphanumeric (Text) and graphical
password. By doing so the amount of security what we provide for the user can
be enhanced. This paper along with test results show that converting user
password in to Probabilistic values and giving them as input for BAM improves
the security of the system"
225,"Trojan attack on deep neural networks, also known as backdoor attack, is a
typical threat to artificial intelligence. A trojaned neural network behaves
normally with clean inputs. However, if the input contains a particular
trigger, the trojaned model will have attacker-chosen abnormal behavior.
Although many backdoor detection methods exist, most of them assume that the
defender has access to a set of clean validation samples or samples with the
trigger, which may not hold in some crucial real-world cases, e.g., the case
where the defender is the maintainer of model-sharing platforms. Thus, in this
paper, we propose FreeEagle, the first data-free backdoor detection method that
can effectively detect complex backdoor attacks on deep neural networks,
without relying on the access to any clean samples or samples with the trigger.
The evaluation results on diverse datasets and model architectures show that
FreeEagle is effective against various complex backdoor attacks, even
outperforming some state-of-the-art non-data-free backdoor detection methods."
226,"We revisit the well-studied problem of differentially private empirical risk
minimization (ERM). We show that for unconstrained convex generalized linear
models (GLMs), one can obtain an excess empirical risk of $\tilde
O\left(\sqrt{{\texttt{rank}}}/\epsilon n\right)$, where ${\texttt{rank}}$ is
the rank of the feature matrix in the GLM problem, $n$ is the number of data
samples, and $\epsilon$ is the privacy parameter. This bound is attained via
differentially private gradient descent (DP-GD). Furthermore, via the first
lower bound for unconstrained private ERM, we show that our upper bound is
tight. In sharp contrast to the constrained ERM setting, there is no dependence
on the dimensionality of the ambient model space ($p$). (Notice that
${\texttt{rank}}\leq \min\{n, p\}$.) Besides, we obtain an analogous excess
population risk bound which depends on ${\texttt{rank}}$ instead of $p$.
  For the smooth non-convex GLM setting (i.e., where the objective function is
non-convex but preserves the GLM structure), we further show that DP-GD attains
a dimension-independent convergence of $\tilde
O\left(\sqrt{{\texttt{rank}}}/\epsilon n\right)$ to a
first-order-stationary-point of the underlying objective.
  Finally, we show that for convex GLMs, a variant of DP-GD commonly used in
practice (which involves clipping the individual gradients) also exhibits the
same dimension-independent convergence to the minimum of a well-defined
objective. To that end, we provide a structural lemma that characterizes the
effect of clipping on the optimization profile of DP-GD."
227,"In the realm of Artificial Intelligence (AI), the need for privacy and
security in data processing has become paramount. As AI applications continue
to expand, the collection and handling of sensitive data raise concerns about
individual privacy protection. Federated Learning (FL) emerges as a promising
solution to address these challenges by enabling decentralized model training
on local devices, thus preserving data privacy. This paper introduces FLEX: a
FLEXible Federated Learning Framework designed to provide maximum flexibility
in FL research experiments. By offering customizable features for data
distribution, privacy parameters, and communication strategies, FLEX empowers
researchers to innovate and develop novel FL techniques. The framework also
includes libraries for specific FL implementations including: (1) anomalies,
(2) blockchain, (3) adversarial attacks and defences, (4) natural language
processing and (5) decision trees, enhancing its versatility and applicability
in various domains. Overall, FLEX represents a significant advancement in FL
research, facilitating the development of robust and efficient FL applications."
228,"Machine Learning (ML), addresses a multitude of complex issues in multiple
disciplines, including social sciences, finance, and medical research. ML
models require substantial computing power and are only as powerful as the data
utilized. Due to high computational cost of ML methods, data scientists
frequently use Machine Learning-as-a-Service (MLaaS) to outsource computation
to external servers. However, when working with private information, like
financial data or health records, outsourcing the computation might result in
privacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have
enabled ML training and inference over protected data through the use of
Privacy-Preserving Machine Learning (PPML). However, these techniques are still
at a preliminary stage and their application in real-world situations is
demanding. In order to comprehend discrepancy between theoretical research
suggestions and actual applications, this work examines the past and present of
PPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party
Computation (SMPC) applied to ML. This work primarily focuses on the ML model's
training phase, where maintaining user data privacy is of utmost importance. We
provide a solid theoretical background that eases the understanding of current
approaches and their limitations. In addition, we present a SoK of the most
recent PPML frameworks for model training and provide a comprehensive
comparison in terms of the unique properties and performances on standard
benchmarks. Also, we reproduce the results for some of the papers and examine
at what level existing works in the field provide support for open science. We
believe our work serves as a valuable contribution by raising awareness about
the current gap between theoretical advancements and real-world applications in
PPML, specifically regarding open-source availability, reproducibility, and
usability."
229,"Transforming large deep neural network (DNN) models into the multi-exit
architectures can overcome the overthinking issue and distribute a large DNN
model on resource-constrained scenarios (e.g. IoT frontend devices and backend
servers) for inference and transmission efficiency. Nevertheless, intellectual
property (IP) protection for the multi-exit models in the wild is still an
unsolved challenge. Previous efforts to verify DNN model ownership mainly rely
on querying the model with specific samples and checking the responses, e.g.,
DNN watermarking and fingerprinting. However, they are vulnerable to
adversarial settings such as adversarial training and are not suitable for the
IP verification for multi-exit DNN models. In this paper, we propose a novel
approach to fingerprint multi-exit models via inference time rather than
inference predictions. Specifically, we design an effective method to generate
a set of fingerprint samples to craft the inference process with a unique and
robust inference time cost as the evidence for model ownership. We conduct
extensive experiments to prove the uniqueness and robustness of our method on
three structures (ResNet-56, VGG-16, and MobileNet) and three datasets
(CIFAR-10, CIFAR-100, and Tiny-ImageNet) under comprehensive adversarial
settings."
230,"The novel Internet of Things (IoT) paradigm is composed of a growing number
of heterogeneous smart objects and services that are transforming architectures
and applications, increasing systems' complexity, and the need for reliability
and autonomy. In this context, both smart objects and services are often
provided by third parties which do not give full transparency regarding the
security and privacy of the features offered. Although machine-based Service
Level Agreements (SLA) have been recently leveraged to establish and share
policies in Cloud-based scenarios, and also in the IoT context, the issue of
making end users aware of the overall system security levels and the
fulfillment of their privacy requirements through the provision of the
requested service remains a challenging task. To tackle this problem, we
propose a complete framework that defines suitable levels of privacy and
security requirements in the acquisition of services in IoT, according to the
user needs. Through the use of a Reinforcement Learning based solution, a user
agent, inside the environment, is trained to choose the best smart objects
granting access to the target services. Moreover, the solution is designed to
guarantee deadline requirements and user security and privacy needs. Finally,
to evaluate the correctness and the performance of the proposed approach we
illustrate an extensive experimental analysis."
231,"Healthcare systems are increasingly incorporating Artificial Intelligence
into their systems, but it is not a solution for all difficulties. AI's
extraordinary potential is being held back by challenges such as a lack of
medical datasets for training AI models, adversarial attacks, and a lack of
trust due to its black box working style. We explored how blockchain technology
can improve the reliability and trustworthiness of AI-based healthcare. This
paper has conducted a Systematic Literature Review to explore the
state-of-the-art research studies conducted in healthcare applications
developed with different AI techniques and Blockchain Technology. This
systematic literature review proceeds with three different paths as natural
language processing-based healthcare systems, computer vision-based healthcare
systems and acoustic AI-based healthcare systems. We found that 1) Defence
techniques for adversarial attacks on AI are available for specific kind of
attacks and even adversarial training is AI based technique which in further
prone to different attacks. 2) Blockchain can address security and privacy
issues in healthcare fraternity. 3) Medical data verification and user
provenance can be enabled with Blockchain. 4) Blockchain can protect
distributed learning on heterogeneous medical data. 5) The issues like single
point of failure, non-transparency in healthcare systems can be resolved with
Blockchain. Nevertheless, it has been identified that research is at the
initial stage. As a result, we have synthesized a conceptual framework using
Blockchain Technology for AI-based healthcare applications that considers the
needs of each NLP, Computer Vision, and Acoustic AI application. A global
solution for all sort of adversarial attacks on AI based healthcare. However,
this technique has significant limits and challenges that need to be addressed
in future studies."
232,"The paper studies how to release data about a critical infrastructure network
(e.g., the power network or a transportation network) without disclosing
sensitive information that can be exploited by malevolent agents, while
preserving the realism of the network. It proposes a novel obfuscation
mechanism that combines several privacy-preserving building blocks with a
bi-level optimization model to significantly improve accuracy. The obfuscation
is evaluated for both realism and privacy properties on real energy and
transportation networks. Experimental results show the obfuscation mechanism
substantially reduces the potential damage of an attack exploiting the released
data to harm the real network."
233,"In recent years Deep Neural Networks (DNNs) have achieved remarkable results
and even showed super-human capabilities in a broad range of domains. This led
people to trust in DNNs' classifications and resulting actions even in
security-sensitive environments like autonomous driving.
  Despite their impressive achievements, DNNs are known to be vulnerable to
adversarial examples. Such inputs contain small perturbations to intentionally
fool the attacked model.
  In this paper, we present a novel end-to-end framework to detect such attacks
during classification without influencing the target model's performance.
Inspired by recent research in neuron-coverage guided testing we show that
dense layers of DNNs carry security-sensitive information. With a secondary DNN
we analyze the activation patterns of the dense layers during classification
runtime, which enables effective and real-time detection of adversarial
examples.
  Our prototype implementation successfully detects adversarial examples in
image, natural language, and audio processing. Thereby, we cover a variety of
target DNNs, including Long Short Term Memory (LSTM) architectures. In
addition, to effectively defend against state-of-the-art attacks, our approach
generalizes between different sets of adversarial examples. Thus, our method
most likely enables us to detect even future, yet unknown attacks. Finally,
during white-box adaptive attacks, we show our method cannot be easily
bypassed."
234,"The Dendritic Cell Algorithm (DCA) as one of the emerging evolutionary
algorithms is based on the behavior of the specific immune agents; known as
Dendritic Cells (DCs). DCA has several potentially beneficial features for
binary classification problems. In this paper, we aim at providing a new
version of this immune-inspired mechanism acts as a semi-supervised classifier
which can be a defensive shield in network intrusion detection problem. Till
now, no strategy or idea has already been adopted on the GetAntigen() function
on detection phase, but randomly sampling entails the DCA to provide
undesirable results in several cycles in each time. This leads to uncertainty.
Whereas it must be accomplished by biological behaviors of DCs in tissues, we
have proposed a novel strategy which exactly acts based on its immunological
functionalities of dendritic cells. The proposed mechanism focuses on two
items: First, to obviate the challenge of needing to have a preordered antigen
set for computing danger signal, and the second, to provide a novel
immune-inspired idea in order to non-random data sampling. A variable
functional migration threshold is also computed cycle by cycle that shows
necessity of the Migration threshold (MT) flexibility. A significant criterion
so called capability of intrusion detection (CID) used for tests. All of the
tests have been performed in a new benchmark dataset named UNSW-NB15.
Experimental consequences demonstrate that the present schema dominates the
standard DCA and has higher CID in comparison with other approaches found in
literature."
235,"Homomorphic encryption (HE) is a promising cryptographic technique for
enabling secure collaborative machine learning in the cloud. However, support
for homomorphic computation on ciphertexts under multiple keys is inefficient.
Current solutions often require key setup before any computation or incur large
ciphertext size (at best, grow linearly to the number of involved keys). In
this paper, we proposed a new approach that leverages threshold and multi-key
HE to support computations on ciphertexts under different keys. Our new
approach removes the need for key setup between each client and the set of
model owners. At the same time, this approach reduces the number of encrypted
models to be offloaded to the cloud evaluator, and the ciphertext size with a
dimension reduction from (N+1)x2 to 2x2. We present the details of each step
and discuss the complexity and security of our approach."
236,"In this paper we propose a novel methodology to assist in identifying
vulnerabilities in a real-world complex heterogeneous industrial control
systems (ICS) using two evolutionary multiobjective optimisation (EMO)
algorithms, NSGA-II and SPEA2. Our approach is evaluated on a well known
benchmark chemical plant simulator, the Tennessee Eastman (TE) process model.
We identified vulnerabilities in individual components of the TE model and then
made use of these to generate combinatorial attacks to damage the safety of the
system, and to cause economic loss. Results were compared against random
attacks, and the performance of the EMO algorithms were evaluated using
hypervolume, spread and inverted generational distance (IGD) metrics. A defence
against these attacks in the form of a novel intrusion detection system was
developed, using a number of machine learning algorithms. Designed approach was
further tested against the developed detection methods. Results demonstrate
that EMO algorithms are a promising tool in the identification of the most
vulnerable components of ICS, and weaknesses of any existing detection systems
in place to protect the system. The proposed approach can be used by control
and security engineers to design security aware control, and test the
effectiveness of security mechanisms, both during design, and later during
system operation."
237,"Since the number of elderly and patients who are in hospitals and healthcare
centers are growing, providing efficient remote healthcare services seems very
important. Currently, most such systems benefit from the distribution and
autonomy features of multiagent systems and the structure of wireless sensor
networks. On the one hand, securing the data of remote healthcare systems is
one of the most significant concerns; particularly recent types of research
about the security of remote healthcare systems keep them secure from
eavesdropping and data modification. On the other hand, existing remote
healthcare systems are still vulnerable against other common attacks of
healthcare networks such as Denial of Service (DoS) and User to Root (U2R)
attacks, because they are managed remotely and based on the Internet.
Therefore, in this paper, we propose a secure framework for remote healthcare
systems that consists of two phases. First, we design a healthcare system base
on multiagent technology to collect data from a sensor network. Then, in the
second phase, a layered architecture of intrusion detection systems that uses
Support Vector Machine to learn the behavior of network traffic is applied.
Based on our framework, we implement a secure remote healthcare system and
evaluate this system against the frequent attacks of healthcare networks such
as Smurf, Buffer overflow, Neptune, and Pod attacks. In the end, evaluation
parameters of the layered architecture of intrusion detection systems prove the
efficiency and correctness of our proposed framework."
238,"In the recent years, Portable Document Format, commonly known as PDF, has
become a democratized standard for document exchange and dissemination. This
trend has been due to its characteristics such as its flexibility and
portability across platforms. The widespread use of PDF has installed a false
impression of inherent safety among benign users. However, the characteristics
of PDF motivated hackers to exploit various types of vulnerabilities, overcome
security safeguards, thereby making the PDF format one of the most efficient
malicious code attack vectors. Therefore, efficiently detecting malicious PDF
files is crucial for information security. Several analysis techniques has been
proposed in the literature, be it static or dynamic, to extract the main
features that allow the discrimination of malware files from benign ones. Since
classical analysis techniques may be limited in case of zero-days,
machine-learning based techniques have emerged recently as an automatic
PDF-malware detection method that is able to generalize from a set of training
samples. These techniques are themselves facing the challenge of evasion
attacks where a malicious PDF is transformed to look benign. In this work, we
give an overview on the PDF-malware detection problem. We give a perspective on
the new challenges and emerging solutions."
239,"Security vulnerabilities play a vital role in network security system.
Fuzzing technology is widely used as a vulnerability discovery technology to
reduce damage in advance. However, traditional fuzzing techniques have many
challenges, such as how to mutate input seed files, how to increase code
coverage, and how to effectively bypass verification. Machine learning
technology has been introduced as a new method into fuzzing test to alleviate
these challenges. This paper reviews the research progress of using machine
learning technology for fuzzing test in recent years, analyzes how machine
learning improve the fuzz process and results, and sheds light on future work
in fuzzing. Firstly, this paper discusses the reasons why machine learning
techniques can be used for fuzzing scenarios and identifies six different
stages in which machine learning have been used. Then this paper systematically
study the machine learning based fuzzing models from selection of machine
learning algorithm, pre-processing methods, datasets, evaluation metrics, and
hyperparameters setting. Next, this paper assesses the performance of the
machine learning models based on the frequently used evaluation metrics. The
results of the evaluation prove that machine learning technology has an
acceptable capability of categorize predictive for fuzzing. Finally, the
comparison on capability of discovering vulnerabilities between traditional
fuzzing tools and machine learning based fuzzing tools is analyzed. The results
depict that the introduction of machine learning technology can improve the
performance of fuzzing. However, there are still some limitations, such as
unbalanced training samples and difficult to extract the characteristics
related to vulnerabilities."
240,"Recently, with the continuous development of deep learning, the performance
of named entity recognition tasks has been dramatically improved. However, the
privacy and the confidentiality of data in some specific fields, such as
biomedical and military, cause insufficient data to support the training of
deep neural networks. In this paper, we propose an encryption learning
framework to address the problems of data leakage and inconvenient disclosure
of sensitive data in certain domains. We introduce multiple encryption
algorithms to encrypt training data in the named entity recognition task for
the first time. In other words, we train the deep neural network using the
encrypted data. We conduct experiments on six Chinese datasets, three of which
are constructed by ourselves. The experimental results show that the encryption
method achieves satisfactory results. The performance of some models trained
with encrypted data even exceeds the performance of the unencrypted method,
which verifies the effectiveness of the introduced encryption method and solves
the problem of data leakage to a certain extent."
241,"Following OpenAI's introduction of GPTs, a surge in GPT apps has led to the
launch of dedicated LLM app stores. Nevertheless, given its debut, there is a
lack of sufficient understanding of this new ecosystem. To fill this gap, this
paper presents a first comprehensive longitudinal (5-month) study of the
evolution, landscape, and vulnerability of the emerging LLM app ecosystem,
focusing on two GPT app stores: \textit{GPTStore.AI} and the official
\textit{OpenAI GPT Store}. Specifically, we develop two automated tools and a
TriLevel configuration extraction strategy to efficiently gather metadata (\ie
names, creators, descriptions, \etc) and user feedback for all GPT apps across
these two stores, as well as configurations (\ie system prompts, knowledge
files, and APIs) for the top 10,000 popular apps. Our extensive analysis
reveals: (1) the user enthusiasm for GPT apps consistently rises, whereas
creator interest plateaus within three months of GPTs' launch; (2) nearly 90\%
system prompts can be easily accessed due to widespread failure to secure GPT
app configurations, leading to considerable plagiarism and duplication among
apps. Our findings highlight the necessity of enhancing the LLM app ecosystem
by the app stores, creators, and users."
242,"This work assesses the impact of blockchain and smart contract on the
visibility of construction supply chain and in the context of payments
(intersection of cash and product flows). It uses comparative empirical
experiments (Charrette Test Method) to draw comparisons between the visibility
of state-of-practice and blockchain-enabled payment systems in a commercial
construction project. Comparisons were drawn across four levels of granularity.
The findings are twofold: 1) blockchain improved information completeness and
information accuracy respectively by an average 216% and 261% compared with the
digital state-of-practice solution. The improvements were significantly more
pronounced for inquiries that had higher product, trade, and temporal
granularity; 2) blockchain-enabled solution was robust in the face of increased
granularity, while the conventional solution experienced 50% and 66.7% decline
respectively in completeness and accuracy of information. The paper concludes
with a discussion of mechanisms contributing to visibility and technology
adoption based on business objectives."
243,"In this paper, we present a general multiparty modeling paradigm with Privacy
Preserving Principal Component Analysis (PPPCA) for horizontally partitioned
data. PPPCA can accomplish multiparty cooperative execution of PCA under the
premise of keeping plaintext data locally. We also propose implementations
using two techniques, i.e., homomorphic encryption and secret sharing. The
output of PPPCA can be sent directly to data consumer to build any machine
learning models. We conduct experiments on three UCI benchmark datasets and a
real-world fraud detection dataset. Results show that the accuracy of the model
built upon PPPCA is the same as the model with PCA that is built based on
centralized plaintext data."
244,"Top-k predictions are used in many real-world applications such as machine
learning as a service, recommender systems, and web searches. $\ell_0$-norm
adversarial perturbation characterizes an attack that arbitrarily modifies some
features of an input such that a classifier makes an incorrect prediction for
the perturbed input. $\ell_0$-norm adversarial perturbation is easy to
interpret and can be implemented in the physical world. Therefore, certifying
robustness of top-$k$ predictions against $\ell_0$-norm adversarial
perturbation is important. However, existing studies either focused on
certifying $\ell_0$-norm robustness of top-$1$ predictions or $\ell_2$-norm
robustness of top-$k$ predictions. In this work, we aim to bridge the gap. Our
approach is based on randomized smoothing, which builds a provably robust
classifier from an arbitrary classifier via randomizing an input. Our major
theoretical contribution is an almost tight $\ell_0$-norm certified robustness
guarantee for top-$k$ predictions. We empirically evaluate our method on
CIFAR10 and ImageNet. For instance, our method can build a classifier that
achieves a certified top-3 accuracy of 69.2\% on ImageNet when an attacker can
arbitrarily perturb 5 pixels of a testing image."
245,"The consumer Internet of Things (IoT) have developed in recent years. Mass
IoT devices are constructed to build a huge communications network. But these
devices are insecure in reality, it means that the communications network are
exposed by the attacker. Moreover, the IoT communication network also faces
with variety of sudden errors. Therefore, it easily leads to that is vulnerable
with the threat of attacker and system failure. The severe situation of IoT
communication network motivates the development of new techniques to
automatically detect multi-anomaly. In this paper, we propose SS-VTCN, a
semi-supervised network for IoT multiple anomaly detection that works well
effectively for IoT communication network. SS-VTCN is designed to capture the
normal patterns of the IoT traffic data based on the distribution whether it is
labeled or not by learning their representations with key techniques such as
Variational Autoencoders and Temporal Convolutional Network. This network can
use the encode data to predict preliminary result, and reconstruct input data
to determine anomalies by the representations. Extensive evaluation experiments
based on a benchmark dataset and a real consumer smart home dataset demonstrate
that SS-VTCN is more suitable than supervised and unsupervised method with
better performance when compared other state-of-art semi-supervised method."
246,"The Artificial Intelligence (AI) institute for Intelligent
Cyberinfrastructure with Computational Learning in the Environment (ICICLE) is
funded by the NSF to build the next generation of Cyberinfrastructure to render
AI more accessible to everyone and drive its further democratization in the
larger society. We describe our efforts to develop Jupyter Notebooks and Python
command line clients that would access these ICICLE resources and services
using ICICLE authentication mechanisms. To connect our clients, we used Tapis,
which is a framework that supports computational research to enable scientists
to access, utilize, and manage multi-institution resources and services. We
used Neo4j to organize data into a knowledge graph (KG). We then hosted the KG
on a Tapis Pod, which offers persistent data storage with a template made
specifically for Neo4j KGs. In order to demonstrate the capabilities of our
software, we developed several clients: Jupyter notebooks authentication,
Neural Networks (NN) notebook, and command line applications that provide a
convenient frontend to the Tapis API. In addition, we developed a data
processing notebook that can manipulate KGs on the Tapis servers, including
creations of a KG, data upload and modification. In this report we present the
software architecture, design and approach, the successfulness of our client
software, and future work."
247,"Intrusion detection into computer networks has become one of the most
important issues in cybersecurity. Attackers keep on researching and coding to
discover new vulnerabilities to penetrate information security system. In
consequence computer systems must be daily upgraded using up-to-date techniques
to keep hackers at bay. This paper focuses on the design and implementation of
an intrusion detection system based on Deep Learning architectures. As a first
step, a shallow network is trained with labelled log-in [into a computer
network] data taken from the Dataset CICIDS2017. The internal behaviour of this
network is carefully tracked and tuned by using plotting and exploring codes
until it reaches a functional peak in intrusion prediction accuracy. As a
second step, an autoencoder, trained with big unlabelled data, is used as a
middle processor which feeds compressed information and abstract representation
to the original shallow network. It is proven that the resultant deep
architecture has a better performance than any version of the shallow network
alone. The resultant functional code scripts, written in MATLAB, represent a
re-trainable system which has been proved using real data, producing good
precision and fast response."
248,"Autonomous vehicle navigation and healthcare diagnostics are among the many
fields where the reliability and security of machine learning models for image
data are critical. We conduct a comprehensive investigation into the
susceptibility of Convolutional Neural Networks (CNNs), which are widely used
for image data, to white-box adversarial attacks. We investigate the effects of
various sophisticated attacks -- Fast Gradient Sign Method, Basic Iterative
Method, Jacobian-based Saliency Map Attack, Carlini & Wagner, Projected
Gradient Descent, and DeepFool -- on CNN performance metrics, (e.g., loss,
accuracy), the differential efficacy of adversarial techniques in increasing
error rates, the relationship between perceived image quality metrics (e.g.,
ERGAS, PSNR, SSIM, and SAM) and classification performance, and the comparative
effectiveness of iterative versus single-step attacks. Using the MNIST,
CIFAR-10, CIFAR-100, and Fashio_MNIST datasets, we explore the effect of
different attacks on the CNNs performance metrics by varying the
hyperparameters of CNNs. Our study provides insights into the robustness of
CNNs against adversarial threats, pinpoints vulnerabilities, and underscores
the urgent need for developing robust defense mechanisms to protect CNNs and
ensuring their trustworthy deployment in real-world scenarios."
249,"In Autonomous Vehicles (AVs), one fundamental pillar is perception, which
leverages sensors like cameras and LiDARs (Light Detection and Ranging) to
understand the driving environment. Due to its direct impact on road safety,
multiple prior efforts have been made to study its the security of perception
systems. In contrast to prior work that concentrates on camera-based
perception, in this work we perform the first security study of LiDAR-based
perception in AV settings, which is highly important but unexplored. We
consider LiDAR spoofing attacks as the threat model and set the attack goal as
spoofing obstacles close to the front of a victim AV. We find that blindly
applying LiDAR spoofing is insufficient to achieve this goal due to the machine
learning-based object detection process. Thus, we then explore the possibility
of strategically controlling the spoofed attack to fool the machine learning
model. We formulate this task as an optimization problem and design modeling
methods for the input perturbation function and the objective function. We also
identify the inherent limitations of directly solving the problem using
optimization and design an algorithm that combines optimization and global
sampling, which improves the attack success rates to around 75%. As a case
study to understand the attack impact at the AV driving decision level, we
construct and evaluate two attack scenarios that may damage road safety and
mobility. We also discuss defense directions at the AV system, sensor, and
machine learning model levels."
250,"Artificial Intelligence Generated Content (AIGC) is one of the latest
achievements in AI development. The content generated by related applications,
such as text, images and audio, has sparked a heated discussion. Various
derived AIGC applications are also gradually entering all walks of life,
bringing unimaginable impact to people's daily lives. However, the rapid
development of such generative tools has also raised concerns about privacy and
security issues, and even copyright issues in AIGC. We note that advanced
technologies such as blockchain and privacy computing can be combined with AIGC
tools, but no work has yet been done to investigate their relevance and
prospect in a systematic and detailed way. Therefore it is necessary to
investigate how they can be used to protect the privacy and security of data in
AIGC by fully exploring the aforementioned technologies. In this paper, we
first systematically review the concept, classification and underlying
technologies of AIGC. Then, we discuss the privacy and security challenges
faced by AIGC from multiple perspectives and purposefully list the
countermeasures that currently exist. We hope our survey will help researchers
and industry to build a more secure and robust AIGC system."
251,"Differential privacy has gained popularity in machine learning as a strong
privacy guarantee, in contrast to privacy mitigation techniques such as
k-anonymity. However, applying differential privacy to n-gram counts
significantly degrades the utility of derived language models due to their
large vocabularies. We propose a differential privacy mechanism that uses
public data as a prior in a Bayesian setup to provide tighter bounds on the
privacy loss metric epsilon, and thus better privacy-utility trade-offs. It
first transforms the counts to log space, approximating the distribution of the
public and private data as Gaussian. The posterior distribution is then
evaluated and softmax is applied to produce a probability distribution. This
technique achieves up to 85% reduction in KL divergence compared to previously
known mechanisms at epsilon equals 0.1. We compare our mechanism to k-anonymity
in a n-gram language modelling task and show that it offers competitive
performance at large vocabulary sizes, while also providing superior privacy
protection."
252,"Botnet detectors based on machine learning are potential targets for
adversarial evasion attacks. Several research works employ adversarial training
with samples generated from generative adversarial nets (GANs) to make the
botnet detectors adept at recognising adversarial evasions. However, the
synthetic evasions may not follow the original semantics of the input samples.
This paper proposes a novel GAN model leveraged with deep reinforcement
learning (DRL) to explore semantic aware samples and simultaneously harden its
detection. A DRL agent is used to attack the discriminator of the GAN that acts
as a botnet detector. The discriminator is trained on the crafted perturbations
by the agent during the GAN training, which helps the GAN generator converge
earlier than the case without DRL. We name this model RELEVAGAN, i.e. [""relive
a GAN"" or deep REinforcement Learning-based Evasion Generative Adversarial
Network] because, with the help of DRL, it minimises the GAN's job by letting
its generator explore the evasion samples within the semantic limits. During
the GAN training, the attacks are conducted to adjust the discriminator weights
for learning crafted perturbations by the agent. RELEVAGAN does not require
adversarial training for the ML classifiers since it can act as an adversarial
semantic-aware botnet detection model. Code will be available at
https://github.com/rhr407/RELEVAGAN."
253,"As Android has become increasingly popular, so has malware targeting it, thus
pushing the research community to propose different detection techniques.
However, the constant evolution of the Android ecosystem, and of malware
itself, makes it hard to design robust tools that can operate for long periods
of time without the need for modifications or costly re-training. Aiming to
address this issue, we set to detect malware from a behavioral point of view,
modeled as the sequence of abstracted API calls. We introduce MaMaDroid, a
static-analysis based system that abstracts the API calls performed by an app
to their class, package, or family, and builds a model from their sequences
obtained from the call graph of an app as Markov chains. This ensures that the
model is more resilient to API changes and the features set is of manageable
size. We evaluate MaMaDroid using a dataset of 8.5K benign and 35.5K malicious
apps collected over a period of six years, showing that it effectively detects
malware (with up to 0.99 F-measure) and keeps its detection capabilities for
long periods of time (up to 0.87 F-measure two years after training). We also
show that MaMaDroid remarkably outperforms DroidAPIMiner, a state-of-the-art
detection system that relies on the frequency of (raw) API calls. Aiming to
assess whether MaMaDroid's effectiveness mainly stems from the API abstraction
or from the sequencing modeling, we also evaluate a variant of it that uses
frequency (instead of sequences), of abstracted API calls. We find that it is
not as accurate, failing to capture maliciousness when trained on malware
samples that include API calls that are equally or more frequently used by
benign apps."
254,"Maximum order complexity is an important tool for measuring the nonlinearity
of a pseudorandom sequence. There is a lack of tools for predicting the
strength of a pseudorandom binary sequence in an effective and efficient
manner. To this end, this paper proposes a neural-network-based model for
measuring the strength of a pseudorandom binary sequence. Using the Shrinking
Generator (SG) keystream as pseudorandom binary sequences, then calculating the
Unique Window Size (UWS) as a representation of Maximum order complexity, we
demonstrate that the proposed model provides more accurate and efficient
predictions (measurements) than a classical method for predicting the maximum
order complexity."
255,"Increased dependence on networked, software based control has escalated the
vulnerabilities of Cyber Physical Systems (CPSs). Detection and monitoring
components developed leveraging dynamical systems theory are often employed as
lightweight security measures for protecting such safety critical CPSs against
false data injection attacks. However, existing approaches do not correlate
attack scenarios with parameters of detection systems. In the present work, we
propose a Reinforcement Learning (RL) based framework which adaptively sets the
parameters of such detectors based on experience learned from attack scenarios,
maximizing detection rate and minimizing false alarms in the process while
attempting performance preserving control actions."
256,"Privacy poses a significant obstacle to the progress of learning analytics
(LA), presenting challenges like inadequate anonymization and data misuse that
current solutions struggle to address. Synthetic data emerges as a potential
remedy, offering robust privacy protection. However, prior LA research on
synthetic data lacks thorough evaluation, essential for assessing the delicate
balance between privacy and data utility. Synthetic data must not only enhance
privacy but also remain practical for data analytics. Moreover, diverse LA
scenarios come with varying privacy and utility needs, making the selection of
an appropriate synthetic data approach a pressing challenge. To address these
gaps, we propose a comprehensive evaluation of synthetic data, which
encompasses three dimensions of synthetic data quality, namely resemblance,
utility, and privacy. We apply this evaluation to three distinct LA datasets,
using three different synthetic data generation methods. Our results show that
synthetic data can maintain similar utility (i.e., predictive performance) as
real data, while preserving privacy. Furthermore, considering different privacy
and data utility requirements in different LA scenarios, we make customized
recommendations for synthetic data generation. This paper not only presents a
comprehensive evaluation of synthetic data but also illustrates its potential
in mitigating privacy concerns within the field of LA, thus contributing to a
wider application of synthetic data in LA and promoting a better practice for
open science."
257,"Not too long ago, AI security used to mean the research and practice of how
AI can empower cybersecurity, that is, AI for security. Ever since Ian
Goodfellow and his team popularized adversarial attacks on machine learning,
security for AI became an important concern and also part of AI security. It is
imperative to understand the threats to machine learning products and avoid
common pitfalls in AI product development. This article is addressed to
developers, designers, managers and researchers of AI software products."
258,"The binary similarity problem consists in determining if two functions are
similar by only considering their compiled form. Advanced techniques for binary
similarity recently gained momentum as they can be applied in several fields,
such as copyright disputes, malware analysis, vulnerability detection, etc.,
and thus have an immediate practical impact. Current solutions compare
functions by first transforming their binary code in multi-dimensional vector
representations (embeddings), and then comparing vectors through simple and
efficient geometric operations. However, embeddings are usually derived from
binary code using manual feature extraction, that may fail in considering
important function characteristics, or may consider features that are not
important for the binary similarity problem. In this paper we propose SAFE, a
novel architecture for the embedding of functions based on a self-attentive
neural network. SAFE works directly on disassembled binary functions, does not
require manual feature extraction, is computationally more efficient than
existing solutions (i.e., it does not incur in the computational overhead of
building or manipulating control flow graphs), and is more general as it works
on stripped binaries and on multiple architectures. We report the results from
a quantitative and qualitative analysis that show how SAFE provides a
noticeable performance improvement with respect to previous solutions.
Furthermore, we show how clusters of our embedding vectors are closely related
to the semantic of the implemented algorithms, paving the way for further
interesting applications (e.g. semantic-based binary function search)."
259,"We investigate the problem of nodes clustering under privacy constraints when
representing a dataset as a graph. Our contribution is threefold. First we
formally define the concept of differential privacy for structured databases
such as graphs, and give an alternative definition based on a new neighborhood
notion between graphs. This definition is adapted to particular frameworks that
can be met in various application fields such as genomics, world wide web,
population survey, etc. Second, we introduce a new algorithm to tackle the
issue of privately releasing an approximated minimum spanning tree topology for
a simple-undirected-weighted graph. It provides a simple way of producing the
topology of a private almost minimum spanning tree which outperforms, in most
cases, the state of the art ""Laplace mechanism"" in terms of
weight-approximation error.
  Finally, we propose a theoretically motivated method combining a sanitizing
mechanism (such as Laplace or our new algorithm) with a Minimum Spanning Tree
(MST)-based clustering algorithm. It provides an accurate method for nodes
clustering in a graph while keeping the sensitive information contained in the
edges weights of the private graph. We provide some theoretical results on the
robustness of an almost minimum spanning tree construction for Laplace
sanitizing mechanisms. These results exhibit which conditions the graph weights
should respect in order to consider that the nodes form well separated clusters
both for Laplace and our algorithm as sanitizing mechanism. The method has been
experimentally evaluated on simulated data, and preliminary results show the
good behavior of the algorithm while identifying well separated clusters."
260,"Homomorphic Encryption (HE), allowing computations on encrypted data
(ciphertext) without decrypting it first, enables secure but prohibitively slow
Convolutional Neural Network (CNN) inference for privacy-preserving
applications in clouds. To reduce the inference latency, one approach is to
pack multiple messages into a single ciphertext in order to reduce the number
of ciphertexts and support massive parallelism of Homomorphic
Multiply-Accumulate (HMA) operations between ciphertexts. Despite the faster
HECNN inference, the mainstream packing schemes Dense Packing (DensePack) and
Convolution Packing (ConvPack) introduce expensive rotation overhead, which
prolongs the inference latency of HECNN for deeper and wider CNN architectures.
In this paper, we propose a low-rank factorization method named FFConv
dedicated to efficient ciphertext packing for reducing both the rotation
overhead and HMA operations. FFConv approximates a d x d convolution layer with
low-rank factorized convolutions, in which a d x d low-rank convolution with
fewer channels is followed by a 1 x 1 convolution to restore the channels. The
d x d low-rank convolution with DensePack leads to significantly reduced
rotation operations, while the rotation overhead of 1 x 1 convolution with
ConvPack is close to zero. To our knowledge, FFConv is the first work that is
capable of reducing the rotation overhead incurred by DensePack and ConvPack
simultaneously, without introducing additional special blocks into the HECNN
inference pipeline. Compared to prior art LoLa and Falcon, our method reduces
the inference latency by up to 88% and 21%, respectively, with comparable
accuracy on MNIST and CIFAR-10."
261,"Bitcoin has created a new exchange paradigm within which financial
transactions can be trusted without an intermediary. This premise of a free
decentralized transactional network however requires, in its current
implementation, unrestricted access to the ledger for peer-based transaction
verification. A number of studies have shown that, in this pseudonymous
context, identities can be leaked based on transaction features or off-network
information. In this work, we analyze the information revealed by the pattern
of transactions in the neighborhood of a given entity transaction. By
definition, these features which pertain to an extended network are not
directly controllable by the entity, but might enable leakage of information
about transacting entities. We define a number of new features relevant to
entity characterization on the Bitcoin Blockchain and study their efficacy in
practice. We show that even a weak attacker with shallow data mining knowledge
is able to leverage these features to characterize the entity properties."
262,"We present remote Operating System detection as an inference problem: given a
set of observations (the target host responses to a set of tests), we want to
infer the OS type which most probably generated these observations. Classical
techniques used to perform this analysis present several limitations. To
improve the analysis, we have developed tools using neural networks and
Statistics tools. We present two working modules: one which uses DCE-RPC
endpoints to distinguish Windows versions, and another which uses Nmap
signatures to distinguish different version of Windows, Linux, Solaris,
OpenBSD, FreeBSD and NetBSD systems. We explain the details of the topology and
inner workings of the neural networks used, and the fine tuning of their
parameters. Finally we show positive experimental results."
263,"Neural text detectors aim to decide the characteristics that distinguish
neural (machine-generated) from human texts. To challenge such detectors,
adversarial attacks can alter the statistical characteristics of the generated
text, making the detection task more and more difficult. Inspired by the
advances of mutation analysis in software development and testing, in this
paper, we propose character- and word-based mutation operators for generating
adversarial samples to attack state-of-the-art natural text detectors. This
falls under white-box adversarial attacks. In such attacks, attackers have
access to the original text and create mutation instances based on this
original text. The ultimate goal is to confuse machine learning models and
classifiers and decrease their prediction accuracy."
264,"Backdoor attack is a severe security threat to deep neural networks (DNNs).
We envision that, like adversarial examples, there will be a cat-and-mouse game
for backdoor attacks, i.e., new empirical defenses are developed to defend
against backdoor attacks but they are soon broken by strong adaptive backdoor
attacks. To prevent such cat-and-mouse game, we take the first step towards
certified defenses against backdoor attacks. Specifically, in this work, we
study the feasibility and effectiveness of certifying robustness against
backdoor attacks using a recent technique called randomized smoothing.
Randomized smoothing was originally developed to certify robustness against
adversarial examples. We generalize randomized smoothing to defend against
backdoor attacks. Our results show the theoretical feasibility of using
randomized smoothing to certify robustness against backdoor attacks. However,
we also find that existing randomized smoothing methods have limited
effectiveness at defending against backdoor attacks, which highlight the needs
of new theory and methods to certify robustness against backdoor attacks."
265,"In the fields of statistics and unsupervised machine learning a fundamental
and well-studied problem is anomaly detection. Anomalies are difficult to
define, yet many algorithms have been proposed. Underlying the approaches is
the nebulous understanding that anomalies are rare, unusual or inconsistent
with the majority of data. The present work provides a philosophical treatise
to clearly define anomalies and develops an algorithm for their efficient
detection with minimal user intervention. Inspired by the Gestalt School of
Psychology and the Helmholtz principle of human perception, anomalies are
assumed to be observations that are unexpected to occur with respect to certain
groupings made by the majority of the data. Under appropriate random variable
modelling anomalies are directly found in a set of data by a uniform and
independent random assumption of the distribution of constituent elements of
the observations, with anomalies corresponding to those observations where the
expectation of the number of occurrences of the elements in a given view is
$<1$. Starting from fundamental principles of human perception an unsupervised
anomaly detection algorithm is developed that is simple, real-time and
parameter-free. Experiments suggest it as a competing choice for univariate
data with promising results on the detection of global anomalies in
multivariate data."
266,"In the burgeoning field of Large Language Models (LLMs), developing a robust
safety mechanism, colloquially known as ""safeguards"" or ""guardrails"", has
become imperative to ensure the ethical use of LLMs within prescribed
boundaries. This article provides a systematic literature review on the current
status of this critical mechanism. It discusses its major challenges and how it
can be enhanced into a comprehensive mechanism dealing with ethical issues in
various contexts. First, the paper elucidates the current landscape of
safeguarding mechanisms that major LLM service providers and the open-source
community employ. This is followed by the techniques to evaluate, analyze, and
enhance some (un)desirable properties that a guardrail might want to enforce,
such as hallucinations, fairness, privacy, and so on. Based on them, we review
techniques to circumvent these controls (i.e., attacks), to defend the attacks,
and to reinforce the guardrails. While the techniques mentioned above represent
the current status and the active research trends, we also discuss several
challenges that cannot be easily dealt with by the methods and present our
vision on how to implement a comprehensive guardrail through the full
consideration of multi-disciplinary approach, neural-symbolic method, and
systems development lifecycle."
267,"Membership inference attacks seek to infer the membership of individual
training instances of a privately trained model. This paper presents a
membership privacy analysis and evaluation system, called MPLens, with three
unique contributions. First, through MPLens, we demonstrate how membership
inference attack methods can be leveraged in adversarial machine learning.
Second, through MPLens, we highlight how the vulnerability of pre-trained
models under membership inference attack is not uniform across all classes,
particularly when the training data itself is skewed. We show that risk from
membership inference attacks is routinely increased when models use skewed
training data. Finally, we investigate the effectiveness of differential
privacy as a mitigation technique against membership inference attacks. We
discuss the trade-offs of implementing such a mitigation strategy with respect
to the model complexity, the learning task complexity, the dataset complexity
and the privacy parameter settings. Our empirical results reveal that (1)
minority groups within skewed datasets display increased risk for membership
inference and (2) differential privacy presents many challenging trade-offs as
a mitigation technique to membership inference risk."
268,"In recent work, Cheu et al. (Eurocrypt 2019) proposed a protocol for
$n$-party real summation in the shuffle model of differential privacy with
$O_{\epsilon, \delta}(1)$ error and $\Theta(\epsilon\sqrt{n})$ one-bit messages
per party. In contrast, every local model protocol for real summation must
incur error $\Omega(1/\sqrt{n})$, and there exist protocols matching this lower
bound which require just one bit of communication per party. Whether this gap
in number of messages is necessary was left open by Cheu et al.
  In this note we show a protocol with $O(1/\epsilon)$ error and
$O(\log(n/\delta))$ messages of size $O(\log(n))$ per party. This protocol is
based on the work of Ishai et al.\ (FOCS 2006) showing how to implement
distributed summation from secure shuffling, and the observation that this
allows simulating the Laplace mechanism in the shuffle model."
269,"Despite the broad application of Machine Learning models as a Service
(MLaaS), they are vulnerable to model stealing attacks. These attacks can
replicate the model functionality by using the black-box query process without
any prior knowledge of the target victim model. Existing stealing defenses add
deceptive perturbations to the victim's posterior probabilities to mislead the
attackers. However, these defenses are now suffering problems of high inference
computational overheads and unfavorable trade-offs between benign accuracy and
stealing robustness, which challenges the feasibility of deployed models in
practice. To address the problems, this paper proposes Isolation and Induction
(InI), a novel and effective training framework for model stealing defenses.
Instead of deploying auxiliary defense modules that introduce redundant
inference time, InI directly trains a defensive model by isolating the
adversary's training gradient from the expected gradient, which can effectively
reduce the inference computational cost. In contrast to adding perturbations
over model predictions that harm the benign accuracy, we train models to
produce uninformative outputs against stealing queries, which can induce the
adversary to extract little useful knowledge from victim models with minimal
impact on the benign performance. Extensive experiments on several visual
classification datasets (e.g., MNIST and CIFAR10) demonstrate the superior
robustness (up to 48% reduction on stealing accuracy) and speed (up to 25.4x
faster) of our InI over other state-of-the-art methods. Our codes can be found
in https://github.com/DIG-Beihang/InI-Model-Stealing-Defense."
270,"Modern malware evolves various detection avoidance techniques to bypass the
state-of-the-art detection methods. An emerging trend to deal with this issue
is the combination of image transformation and machine learning techniques to
classify and detect malware. However, existing works in this field only perform
simple image transformation methods that limit the accuracy of the detection.
In this paper, we introduce a novel approach to classify malware by using a
deep network on images transformed from binary samples. In particular, we first
develop a novel hybrid image transformation method to convert binaries into
color images that convey the binary semantics. The images are trained by a deep
convolutional neural network that later classifies the test inputs into benign
or malicious categories. Through the extensive experiments, our proposed method
surpasses all baselines and achieves 99.14% in terms of accuracy on the testing
set."
271,"In this paper, we propose a new feature extraction technique for program
execution logs. First, we automatically extract complex patterns from a
program's behavior graph. Then, we embed these patterns into a continuous space
by training an autoencoder. We evaluate the proposed features on a real-world
malicious software detection task. We also find that the embedding space
captures interpretable structures in the space of pattern parts."
272,"Advanced persistent threats (APTs) have novel features such as multi-stage
penetration, highly-tailored intention, and evasive tactics. APTs defense
requires fusing multi-dimensional Cyber threat intelligence data to identify
attack intentions and conducts efficient knowledge discovery strategies by
data-driven machine learning to recognize entity relationships. However,
data-driven machine learning lacks generalization ability on fresh or unknown
samples, reducing the accuracy and practicality of the defense model. Besides,
the private deployment of these APT defense models on heterogeneous
environments and various network devices requires significant investment in
context awareness (such as known attack entities, continuous network states,
and current security strategies). In this paper, we propose a few-shot
multi-domain knowledge rearming (FMKR) scheme for context-aware defense against
APTs. By completing multiple small tasks that are generated from different
network domains with meta-learning, the FMKR firstly trains a model with good
discrimination and generalization ability for fresh and unknown APT attacks. In
each FMKR task, both threat intelligence and local entities are fused into the
support/query sets in meta-learning to identify possible attack stages.
Secondly, to rearm current security strategies, an finetuning-based deployment
mechanism is proposed to transfer learned knowledge into the student model,
while minimizing the defense cost. Compared to multiple model replacement
strategies, the FMKR provides a faster response to attack behaviors while
consuming less scheduling cost. Based on the feedback from multiple real users
of the Industrial Internet of Things (IIoT) over 2 months, we demonstrate that
the proposed scheme can improve the defense satisfaction rate."
273,"Malware analysis has been extensively investigated as the number and types of
malware has increased dramatically. However, most previous studies use
end-to-end systems to detect whether a sample is malicious, or to identify its
malware family. In this paper, we propose a neural network framework composed
of an embedder, an encoder, and a filter to learn malware representations from
characteristic execution sequences for malware family classification. The
embedder uses BERT and Sent2Vec, state-of-the-art embedding modules, to capture
relations within a single API call and among consecutive API calls in an
execution trace. The encoder comprises gated recurrent units (GRU) to preserve
the ordinal position of API calls and a self-attention mechanism for comparing
intra-relations among different positions of API calls. The filter identifies
representative API calls to build the malware representation. We conduct broad
experiments to determine the influence of individual framework components. The
results show that the proposed framework outperforms the baselines, and also
demonstrates that considering Sent2Vec to learn complete API call embeddings
and GRU to explicitly preserve ordinal information yields more information and
thus significant improvements. Also, the proposed approach effectively
classifies new malicious execution traces on the basis of similarities with
previously collected families."
274,"Blockchain offers a decentralized, immutable, transparent system of records.
It offers a peer-to-peer network of nodes with no centralised governing entity
making it unhackable and therefore, more secure than the traditional
paper-based or centralised system of records like banks etc. While there are
certain advantages to the paper-based recording approach, it does not work well
with digital relationships where the data is in constant flux. Unlike
traditional channels, governed by centralized entities, blockchain offers its
users a certain level of anonymity by providing capabilities to interact
without disclosing their personal identities and allows them to build trust
without a third-party governing entity. Due to the aforementioned
characteristics of blockchain, more and more users around the globe are
inclined towards making a digital transaction via blockchain than via
rudimentary channels. Therefore, there is a dire need for us to gain insight on
how these transactions are processed by the blockchain and how much time it may
take for a peer to confirm a transaction and add it to the blockchain network.
This paper presents a novel approach that would allow one to estimate the time,
in block time or otherwise, it would take for a mining node to accept and
confirm a transaction to a block using machine learning. The paper also aims to
compare the predictive accuracy of two machine learning regression models-
Random Forest Regressor and Multilayer Perceptron against previously proposed
statistical regression model under a set evaluation criterion. The objective is
to determine whether machine learning offers a more accurate predictive model
than conventional statistical models. The proposed model results in improved
accuracy in prediction."
275,"To this date, CAPTCHAs have served as the first line of defense preventing
unauthorized access by (malicious) bots to web-based services, while at the
same time maintaining a trouble-free experience for human visitors. However,
recent work in the literature has provided evidence of sophisticated bots that
make use of advancements in machine learning (ML) to easily bypass existing
CAPTCHA-based defenses. In this work, we take the first step to address this
problem. We introduce CAPTURE, a novel CAPTCHA scheme based on adversarial
examples. While typically adversarial examples are used to lead an ML model
astray, with CAPTURE, we attempt to make a ""good use"" of such mechanisms. Our
empirical evaluations show that CAPTURE can produce CAPTCHAs that are easy to
solve by humans while at the same time, effectively thwarting ML-based bot
solvers."
276,"Deep learning models have achieved high performance on many tasks, and thus
have been applied to many security-critical scenarios. For example, deep
learning-based face recognition systems have been used to authenticate users to
access many security-sensitive applications like payment apps. Such usages of
deep learning systems provide the adversaries with sufficient incentives to
perform attacks against these systems for their adversarial purposes. In this
work, we consider a new type of attacks, called backdoor attacks, where the
attacker's goal is to create a backdoor into a learning-based authentication
system, so that he can easily circumvent the system by leveraging the backdoor.
Specifically, the adversary aims at creating backdoor instances, so that the
victim learning system will be misled to classify the backdoor instances as a
target label specified by the adversary. In particular, we study backdoor
poisoning attacks, which achieve backdoor attacks using poisoning strategies.
Different from all existing work, our studied poisoning strategies can apply
under a very weak threat model: (1) the adversary has no knowledge of the model
and the training set used by the victim system; (2) the attacker is allowed to
inject only a small amount of poisoning samples; (3) the backdoor key is hard
to notice even by human beings to achieve stealthiness. We conduct evaluation
to demonstrate that a backdoor adversary can inject only around 50 poisoning
samples, while achieving an attack success rate of above 90%. We are also the
first work to show that a data poisoning attack can create physically
implementable backdoors without touching the training process. Our work
demonstrates that backdoor poisoning attacks pose real threats to a learning
system, and thus highlights the importance of further investigation and
proposing defense strategies against them."
277,"Our computer systems for decades have been threatened by various types of
hardware and software attacks of which Malwares have been one of them. This
malware has the ability to steal, destroy, contaminate, gain unintended access,
or even disrupt the entire system. There have been techniques to detect malware
by performing static and dynamic analysis of malware files, but, stealthy
malware has circumvented the static analysis method and for dynamic analysis,
there have been previous works that propose different methods to detect malware
but, in this work we propose a novel technique to detect malware. We use
malware binary images and then extract different features from the same and
then employ different ML-classifiers on the dataset thus obtained. We show that
this technique is successful in differentiating classes of malware based on the
features extracted."
278,"The continued growth in the deployment of Internet-of-Things (IoT) devices
has been fueled by the increased connectivity demand, particularly in
industrial environments. However, this has led to an increase in the number of
network related attacks due to the increased number of potential attack
surfaces. Industrial IoT (IIoT) devices are prone to various network related
attacks that can have severe consequences on the manufacturing process as well
as on the safety of the workers in the manufacturing plant. One promising
solution that has emerged in recent years for attack detection is Machine
learning (ML). More specifically, ensemble learning models have shown great
promise in improving the performance of the underlying ML models. Accordingly,
this paper proposes a framework based on the combined use of Bayesian
Optimization-Gaussian Process (BO-GP) with an ensemble tree-based learning
model to improve the performance of intrusion and attack detection in IIoT
environments. The proposed framework's performance is evaluated using the
Windows 10 dataset collected by the Cyber Range and IoT labs at University of
New South Wales. Experimental results illustrate the improvement in detection
accuracy, precision, and F-score when compared to standard tree and ensemble
tree models."
279,"Ethereum smart contracts are automated decentralized applications on the
blockchain that describe the terms of the agreement between buyers and sellers,
reducing the need for trusted intermediaries and arbitration. However, the
deployment of smart contracts introduces new attack vectors into the
cryptocurrency systems. In particular, programming flaws in smart contracts can
be and have already been exploited to gain enormous financial profits. It is
thus an emerging yet crucial issue to detect vulnerabilities of different
classes in contracts in an efficient manner. Existing machine learning-based
vulnerability detection methods are limited and only inspect whether the smart
contract is vulnerable, or train individual classifiers for each specific
vulnerability, or demonstrate multi-class vulnerability detection without
extensibility consideration. To overcome the scalability and generalization
limitations of existing works, we propose ESCORT, the first Deep Neural Network
(DNN)-based vulnerability detection framework for Ethereum smart contracts that
support lightweight transfer learning on unseen security vulnerabilities, thus
is extensible and generalizable. ESCORT leverages a multi-output NN
architecture that consists of two parts: (i) A common feature extractor that
learns the semantics of the input contract; (ii) Multiple branch structures
where each branch learns a specific vulnerability type based on features
obtained from the feature extractor. Experimental results show that ESCORT
achieves an average F1-score of 95% on six vulnerability types and the
detection time is 0.02 seconds per contract. When extended to new vulnerability
types, ESCORT yields an average F1-score of 93%. To the best of our knowledge,
ESCORT is the first framework that enables transfer learning on new
vulnerability types with minimal modification of the DNN model architecture and
re-training overhead."
280,"Clustering is an essential technique for network analysis, with applications
in a diverse range of fields. Although spectral clustering is a popular and
effective method, it fails to consider higher-order structure and can perform
poorly on directed networks. One approach is to capture and cluster
higher-order structures using motif adjacency matrices. However, current
formulations fail to take edge weights into account, and thus are somewhat
limited when weight is a key component of the network under study.
  We address these shortcomings by exploring motif-based weighted spectral
clustering methods. We present new and computationally useful matrix formulae
for motif adjacency matrices on weighted networks, which can be used to
construct efficient algorithms for any anchored or non-anchored motif on three
nodes. In a very sparse regime, our proposed method can handle graphs with a
million nodes and tens of millions of edges. We further use our framework to
construct a motif-based approach for clustering bipartite networks.
  We provide comprehensive experimental results, demonstrating (i) the
scalability of our approach, (ii) advantages of higher-order clustering on
synthetic examples, and (iii) the effectiveness of our techniques on a variety
of real world data sets; and compare against several techniques from the
literature. We conclude that motif-based spectral clustering is a valuable tool
for analysis of directed and bipartite weighted networks, which is also
scalable and easy to implement."
281,"One of the longstanding open problems in spectral graph clustering (SGC) is
the so-called model order selection problem: automated selection of the correct
number of clusters. This is equivalent to the problem of finding the number of
connected components or communities in an undirected graph. We propose
automated model order selection (AMOS), a solution to the SGC model selection
problem under a random interconnection model (RIM) using a novel selection
criterion that is based on an asymptotic phase transition analysis. AMOS can
more generally be applied to discovering hidden block diagonal structure in
symmetric non-negative matrices. Numerical experiments on simulated graphs
validate the phase transition analysis, and real-world network data is used to
validate the performance of the proposed model selection procedure."
282,"The ubiquitous proliferation of online social networks has led to the
widescale emergence of relational graphs expressing unique patterns in link
formation and descriptive user node features. Matrix Factorization and
Completion have become popular methods for Link Prediction due to the low rank
nature of mutual node friendship information, and the availability of parallel
computer architectures for rapid matrix processing. Current Link Prediction
literature has demonstrated vast performance improvement through the
utilization of sparsity in addition to the low rank matrix assumption. However,
the majority of research has introduced sparsity through the limited L1 or
Frobenius norms, instead of considering the more detailed distributions which
led to the graph formation and relationship evolution. In particular, social
networks have been found to express either Pareto, or more recently discovered,
Log Normal distributions. Employing the convexity-inducing Lovasz Extension, we
demonstrate how incorporating specific degree distribution information can lead
to large scale improvements in Matrix Completion based Link prediction. We
introduce Log-Normal Matrix Completion (LNMC), and solve the complex
optimization problem by employing Alternating Direction Method of Multipliers.
Using data from three popular social networks, our experiments yield up to 5%
AUC increase over top-performing non-structured sparsity based methods."
283,"In early 2020, the Corona Virus Disease 2019 (COVID-19) pandemic swept the
world.In China, COVID-19 has caused severe consequences. Moreover, online
rumors during the COVID-19 pandemic increased people's panic about public
health and social stability. At present, understanding and curbing the spread
of online rumors is an urgent task. Therefore, we analyzed the rumor spreading
mechanism and propose a method to quantify a rumors' influence by the speed of
new insiders. The search frequency of the rumor is used as an observation
variable of new insiders. The peak coefficient and the attenuation coefficient
are calculated for the search frequency, which conforms to the exponential
distribution. We designed several rumor features and used the above two
coefficients as predictable labels. A 5-fold cross-validation experiment using
the mean square error (MSE) as the loss function showed that the decision tree
was suitable for predicting the peak coefficient, and the linear regression
model was ideal for predicting the attenuation coefficient. Our feature
analysis showed that precursor features were the most important for the
outbreak coefficient, while location information and rumor entity information
were the most important for the attenuation coefficient. Meanwhile, features
that were conducive to the outbreak were usually harmful to the continued
spread of rumors. At the same time, anxiety was a crucial rumor causing factor.
Finally, we discuss how to use deep learning technology to reduce the forecast
loss by using the Bidirectional Encoder Representations from Transformers
(BERT) model."
284,"With the spiraling pandemic of the Coronavirus Disease 2019 (COVID-19), it
has becoming inherently important to disseminate accurate and timely
information about the disease. Due to the ubiquity of Internet connectivity and
smart devices, social sensing is emerging as a dynamic AI-driven sensing
paradigm to extract real-time observations from online users. In this paper, we
propose CovidSens, a vision of social sensing based risk alert systems to
spontaneously obtain and analyze social data to infer COVID-19 propagation.
CovidSens can actively help to keep the general public informed about the
COVID-19 spread and identify risk-prone areas. The CovidSens concept is
motivated by three observations: 1) people actively share their experience of
COVID-19 via online social media, 2) official warning channels and news
agencies are relatively slower than people reporting on social media, and 3)
online users are frequently equipped with powerful mobile devices that can
perform data processing and analytics. We envision unprecedented opportunities
to leverage posts generated by ordinary people to build real-time sensing and
analytic system for gathering and circulating COVID-19 propagation data.
Specifically, the vision of CovidSens attempts to answer the questions: How to
distill reliable information on COVID-19 with prevailing rumors and
misinformation? How to inform the general public about the state of the spread
timely and effectively? How to leverage the computational power on edge devices
to construct fully integrated edge-based social sensing platforms? In this
vision paper, we discuss the roles of CovidSens and identify potential
challenges in developing reliable social sensing based risk alert systems. We
envision that approaches originating from multiple disciplines can be effective
in addressing the challenges. Finally, we outline a few research directions for
future work in CovidSens."
285,"A lack of information exists about the health issues of lesbian, gay,
bisexual, transgender, and queer (LGBTQ) people who are often excluded from
national demographic assessments, health studies, and clinical trials. As a
result, medical experts and researchers lack a holistic understanding of the
health disparities facing these populations. Fortunately, publicly available
social media data such as Twitter data can be utilized to support the decisions
of public health policy makers and managers with respect to LGBTQ people. This
research employs a computational approach to collect tweets from gay users on
health-related topics and model these topics. To determine the nature of
health-related information shared by men who have sex with men on Twitter, we
collected thousands of tweets from 177 active users. We sampled these tweets
using a framework that can be applied to other LGBTQ sub-populations in future
research. We found 11 diseases in 7 categories based on ICD 10 that are in line
with the published studies and official reports."
286,"Recent advances in specialized hardware for solving optimization problems
such quantum computers, quantum annealers, and CMOS annealers give rise to new
ways for solving real-word complex problems. However, given current and
near-term hardware limitations, the number of variables required to express a
large real-world problem easily exceeds the hardware capabilities, thus hybrid
methods are usually developed in order to utilize the hardware. In this work,
we advocate for the development of hybrid methods that are built on top of the
frameworks of existing state-of-art heuristics, thereby improving these
methods. We demonstrate this by building on the so called Louvain method, which
is one of the most popular algorithms for the Community detection problem and
develop and Ising-based Louvain method. The proposed method outperforms two
state-of-the-art community detection algorithms in clustering several small to
large-scale graphs. The results show promise in adapting the same optimization
approach to other unsupervised learning heuristics to improve their
performance."
287,"Trust plays an essential role in an individual's decision-making. Traditional
trust prediction models rely on pairwise correlations to infer potential
relationships between users. However, in the real world, interactions between
users are usually complicated rather than pairwise only. Hypergraphs offer a
flexible approach to modeling these complex high-order correlations (not just
pairwise connections), since hypergraphs can leverage hyperedeges to link more
than two nodes. However, most hypergraph-based methods are generic and cannot
be well applied to the trust prediction task. In this paper, we propose an
Adaptive Hypergraph Network for Trust Prediction (AHNTP), a novel approach that
improves trust prediction accuracy by using higher-order correlations. AHNTP
utilizes Motif-based PageRank to capture high-order social influence
information. In addition, it constructs hypergroups from both node-level and
structure-level attributes to incorporate complex correlation information.
Furthermore, AHNTP leverages adaptive hypergraph Graph Convolutional Network
(GCN) layers and multilayer perceptrons (MLPs) to generate comprehensive user
embeddings, facilitating trust relationship prediction. To enhance model
generalization and robustness, we introduce a novel supervised contrastive
learning loss for optimization. Extensive experiments demonstrate the
superiority of our model over the state-of-the-art approaches in terms of trust
prediction accuracy. The source code of this work can be accessed via
https://github.com/Sherry-XU1995/AHNTP."
288,"Users increasingly rely on social media feeds for consuming daily
information. The items in a feed, such as news, questions, songs, etc., usually
result from the complex interplay of a user's social contacts, her interests
and her actions on the platform. The relationship of the user's own behavior
and the received feed is often puzzling, and many users would like to have a
clear explanation on why certain items were shown to them. Transparency and
explainability are key concerns in the modern world of cognitive overload,
filter bubbles, user tracking, and privacy risks. This paper presents FAIRY, a
framework that systematically discovers, ranks, and explains relationships
between users' actions and items in their social media feeds. We model the
user's local neighborhood on the platform as an interaction graph, a form of
heterogeneous information network constructed solely from information that is
easily accessible to the concerned user. We posit that paths in this
interaction graph connecting the user and her feed items can act as pertinent
explanations for the user. These paths are scored with a learning-to-rank model
that captures relevance and surprisal. User studies on two social platforms
demonstrate the practical viability and user benefits of the FAIRY method."
289,"Efforts by foreign actors to influence public opinion have gained
considerable attention because of their potential to impact democratic
elections. Thus, the ability to identify and counter sources of disinformation
is increasingly becoming a top priority for government entities in order to
protect the integrity of democratic processes. This study presents a method of
identifying Russian disinformation bots on Twitter using centering resonance
analysis and Clauset-Newman-Moore community detection. The data reflect a
significant degree of discursive dissimilarity between known Russian
disinformation bots and a control set of Twitter users during the timeframe of
the 2016 U.S. Presidential Election. The data also demonstrate statistically
significant classification capabilities (MCC = 0.9070) based on community
clustering. The prediction algorithm is very effective at identifying true
positives (bots), but is not able to resolve true negatives (non-bots) because
of the lack of discursive similarity between control users. This leads to a
highly sensitive means of identifying propagators of disinformation with a high
degree of discursive similarity on Twitter, with implications for limiting the
spread of disinformation that could impact democratic processes."
290,"Real-world networks usually have community structure, that is, nodes are
grouped into densely connected communities. Community detection is one of the
most popular and best-studied research topics in network science and has
attracted attention in many different fields, including computer science,
statistics, social sciences, among others. Numerous approaches for community
detection have been proposed in literature, from ad-hoc algorithms to
systematic model-based approaches. The large number of available methods leads
to a fundamental question: whether a certain method can provide consistent
estimates of community labels. The stochastic blockmodel (SBM) and its variants
provide a convenient framework for the study of such problems. This article is
a survey on the recent theoretical advances of community detection. The authors
review a number of community detection methods and their theoretical
properties, including graph cut methods, profile likelihoods, the
pseudo-likelihood method, the variational method, belief propagation, spectral
clustering, and semidefinite relaxations of the SBM. The authors also briefly
discuss other research topics in community detection such as robust community
detection, community detection with nodal covariates and model selection, as
well as suggest a few possible directions for future research."
291,"With the widespread use of information technologies, information networks are
becoming increasingly popular to capture complex relationships across various
disciplines, such as social networks, citation networks, telecommunication
networks, and biological networks. Analyzing these networks sheds light on
different aspects of social life such as the structure of societies,
information diffusion, and communication patterns. In reality, however, the
large scale of information networks often makes network analytic tasks
computationally expensive or intractable. Network representation learning has
been recently proposed as a new learning paradigm to embed network vertices
into a low-dimensional vector space, by preserving network topology structure,
vertex content, and other side information. This facilitates the original
network to be easily handled in the new vector space for further analysis. In
this survey, we perform a comprehensive review of the current literature on
network representation learning in the data mining and machine learning field.
We propose new taxonomies to categorize and summarize the state-of-the-art
network representation learning techniques according to the underlying learning
mechanisms, the network information intended to preserve, as well as the
algorithmic designs and methodologies. We summarize evaluation protocols used
for validating network representation learning including published benchmark
datasets, evaluation methods, and open source algorithms. We also perform
empirical studies to compare the performance of representative algorithms on
common datasets, and analyze their computational complexity. Finally, we
suggest promising research directions to facilitate future study."
292,"Cyberbullying is a pervasive problem in online communities. To identify
cyberbullying cases in large-scale social networks, content moderators depend
on machine learning classifiers for automatic cyberbullying detection. However,
existing models remain unfit for real-world applications, largely due to a
shortage of publicly available training data and a lack of standard criteria
for assigning ground truth labels. In this study, we address the need for
reliable data using an original annotation framework. Inspired by social
sciences research into bullying behavior, we characterize the nuanced problem
of cyberbullying using five explicit factors to represent its social and
linguistic aspects. We model this behavior using social network and
language-based features, which improve classifier performance. These results
demonstrate the importance of representing and modeling cyberbullying as a
social phenomenon."
293,"With the significant increase in users on social media platforms, a new means
of political campaigning has appeared. Twitter and Facebook are now notable
campaigning tools during elections. Indeed, the candidates and their parties
now take to the internet to interact and spread their ideas. In this paper, we
aim to identify political communities formed on Twitter during the 2022 French
presidential election and analyze each respective community. We create a
large-scale Twitter dataset containing 1.2 million users and 62.6 million
tweets that mention keywords relevant to the election. We perform community
detection on a retweet graph of users and propose an in-depth analysis of the
stance of each community. Finally, we attempt to detect offensive tweets and
automatic bots, comparing across communities in order to gain insight into each
candidate's supporter demographics and online campaign strategy."
294,"Deep graph embedding is an important approach for community discovery. Deep
graph neural network with self-supervised mechanism can obtain the
low-dimensional embedding vectors of nodes from unlabeled and unstructured
graph data. The high-order information of graph can provide more abundant
structure information for the representation learning of nodes. However, most
self-supervised graph neural networks only use adjacency matrix as the input
topology information of graph and cannot obtain too high-order information
since the number of layers of graph neural network is fairly limited. If there
are too many layers, the phenomenon of over smoothing will appear. Therefore
how to obtain and fuse high-order information of graph by a shallow graph
neural network is an important problem. In this paper, a deep graph embedding
algorithm with self-supervised mechanism for community discovery is proposed.
The proposed algorithm uses self-supervised mechanism and different high-order
information of graph to train multiple deep graph convolution neural networks.
The outputs of multiple graph convolution neural networks are fused to extract
the representations of nodes which include the attribute and structure
information of a graph. In addition, data augmentation and negative sampling
are introduced into the training process to facilitate the improvement of
embedding result. The proposed algorithm and the comparison algorithms are
conducted on the five experimental data sets. The experimental results show
that the proposed algorithm outperforms the comparison algorithms on the most
experimental data sets. The experimental results demonstrate that the proposed
algorithm is an effective algorithm for community discovery."
295,"Recent work in graph models has found that probabilistic hyperedge
replacement grammars (HRGs) can be extracted from graphs and used to generate
new random graphs with graph properties and substructures close to the
original. In this paper, we show how to add latent variables to the model,
trained using Expectation-Maximization, to generate still better graphs, that
is, ones that generalize better to the test data. We evaluate the new method by
separating training and test graphs, building the model on the former and
measuring the likelihood of the latter, as a more stringent test of how well
the model can generalize to new graphs. On this metric, we find that our
latent-variable HRGs consistently outperform several existing graph models and
provide interesting insights into the building blocks of real world networks."
296,"Edge streams are commonly used to capture interactions in dynamic networks,
such as email, social, or computer networks. The problem of detecting anomalies
or rare events in edge streams has a wide range of applications. However, it
presents many challenges due to lack of labels, a highly dynamic nature of
interactions, and the entanglement of temporal and structural changes in the
network. Current methods are limited in their ability to address the above
challenges and to efficiently process a large number of interactions. Here, we
propose F-FADE, a new approach for detection of anomalies in edge streams,
which uses a novel frequency-factorization technique to efficiently model the
time-evolving distributions of frequencies of interactions between node-pairs.
The anomalies are then determined based on the likelihood of the observed
frequency of each incoming interaction. F-FADE is able to handle in an online
streaming setting a broad variety of anomalies with temporal and structural
changes, while requiring only constant memory. Our experiments on one synthetic
and six real-world dynamic networks show that F-FADE achieves state of the art
performance and may detect anomalies that previous methods are unable to find."
297,"With the great success of graph embedding model on both academic and industry
area, the robustness of graph embedding against adversarial attack inevitably
becomes a central problem in graph learning domain. Regardless of the fruitful
progress, most of the current works perform the attack in a white-box fashion:
they need to access the model predictions and labels to construct their
adversarial loss. However, the inaccessibility of model predictions in real
systems makes the white-box attack impractical to real graph learning system.
This paper promotes current frameworks in a more general and flexible sense --
we demand to attack various kinds of graph embedding model with black-box
driven. To this end, we begin by investigating the theoretical connections
between graph signal processing and graph embedding models in a principled way
and formulate the graph embedding model as a general graph signal process with
corresponding graph filter. As such, a generalized adversarial attacker:
GF-Attack is constructed by the graph filter and feature matrix. Instead of
accessing any knowledge of the target classifiers used in graph embedding,
GF-Attack performs the attack only on the graph filter in a black-box attack
fashion. To validate the generalization of GF-Attack, we construct the attacker
on four popular graph embedding models. Extensive experimental results validate
the effectiveness of our attacker on several benchmark datasets. Particularly
by using our attack, even small graph perturbations like one-edge flip is able
to consistently make a strong attack in performance to different graph
embedding models."
298,"This work investigates the problem of learning temporal interaction networks.
A temporal interaction network consists of a series of chronological
interactions between users and items. Previous methods tackle this problem by
using different variants of recurrent neural networks to model sequential
interactions, which fail to consider the structural information of temporal
interaction networks and inevitably lead to sub-optimal results. To this end,
we propose a novel Deep Structural Point Process termed as DSPP for learning
temporal interaction networks. DSPP simultaneously incorporates the topological
structure and long-range dependency structure into our intensity function to
enhance model expressiveness. To be specific, by using the topological
structure as a strong prior, we first design a topological fusion encoder to
obtain node embeddings. An attentive shift encoder is then developed to learn
the long-range dependency structure between users and items in continuous time.
The proposed two modules enable our model to capture the user-item correlation
and dynamic influence in temporal interaction networks. DSPP is evaluated on
three real-world datasets for both tasks of item prediction and time
prediction. Extensive experiments demonstrate that our model achieves
consistent and significant improvements over state-of-the-art baselines."
299,"Detecting anomalies from a series of temporal networks has many applications,
including road accidents in transport networks and suspicious events in social
networks. While there are many methods for network anomaly detection,
statistical methods are under utilised in this space even though they have a
long history and proven capability in handling temporal dependencies. In this
paper, we introduce \textit{oddnet}, a feature-based network anomaly detection
method that uses time series methods to model temporal dependencies. We
demonstrate the effectiveness of oddnet on synthetic and real-world datasets.
The R package oddnet implements this algorithm."
300,"Public opinion is a crucial factor in shaping political decision-making.
Nowadays, social media has become an essential platform for individuals to
engage in political discussions and express their political views, presenting
researchers with an invaluable resource for analyzing public opinion. In this
paper, we focus on the 2020 US presidential election and create a large-scale
dataset from Twitter. To detect political opinions in tweets, we build a
user-tweet bipartite graph based on users' posting and retweeting behaviors and
convert the task into a Graph Neural Network (GNN)-based node classification
problem. Then, we introduce a novel skip aggregation mechanism that makes tweet
nodes aggregate information from second-order neighbors, which are also tweet
nodes due to the graph's bipartite nature, effectively leveraging user
behavioral information. The experimental results show that our proposed model
significantly outperforms several competitive baselines. Further analyses
demonstrate the significance of user behavioral information and the
effectiveness of skip aggregation."
301,"Do higher-order network structures aid graph semi-supervised learning? Given
a graph and a few labeled vertices, labeling the remaining vertices is a
high-impact problem with applications in several tasks, such as recommender
systems, fraud detection and protein identification. However, traditional
methods rely on edges for spreading labels, which is limited as all edges are
not equal. Vertices with stronger connections participate in higher-order
structures in graphs, which calls for methods that can leverage these
structures in the semi-supervised learning tasks.
  To this end, we propose Higher-Order Label Spreading (HOLS) to spread labels
using higher-order structures. HOLS has strong theoretical guarantees and
reduces to standard label spreading in the base case. Via extensive
experiments, we show that higher-order label spreading using triangles in
addition to edges is up to 4.7% better than label spreading using edges alone.
Compared to prior traditional and state-of-the-art methods, the proposed method
leads to statistically significant accuracy gains in all-but-one cases, while
remaining fast and scalable to large graphs."
302,"Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media."
303,"Multi-view networks are broadly present in real-world applications. In the
meantime, network embedding has emerged as an effective representation learning
approach for networked data. Therefore, we are motivated to study the problem
of multi-view network embedding with a focus on the optimization objectives
that are specific and important in embedding this type of network. In our
practice of embedding real-world multi-view networks, we explicitly identify
two such objectives, which we refer to as preservation and collaboration. The
in-depth analysis of these two objectives is discussed throughout this paper.
In addition, the mvn2vec algorithms are proposed to (i) study how varied extent
of preservation and collaboration can impact embedding learning and (ii)
explore the feasibility of achieving better embedding quality by modeling them
simultaneously. With experiments on a series of synthetic datasets, a
large-scale internal Snapchat dataset, and two public datasets, we confirm the
validity and importance of preservation and collaboration as two objectives for
multi-view network embedding. These experiments further demonstrate that better
embedding can be obtained by simultaneously modeling the two objectives, while
not over-complicating the model or requiring additional supervision. The code
and the processed datasets are available at
http://yushi2.web.engr.illinois.edu/."
304,"Finding meaningful communities in social network has attracted the attentions
of many researchers. The community structure of complex networks reveals both
their organization and hidden relations among their constituents. Most of the
researches in the field of community detection mainly focus on the topological
structure of the network without performing any content analysis. Nowadays,
real world social networks are containing a vast range of information including
shared objects, comments, following information, etc. In recent years, a number
of researches have proposed approaches which consider both the contents that
are interchanged in the networks and the topological structures of the networks
in order to find more meaningful communities. In this research, the effect of
topic analysis in finding more meaningful communities in social networking
sites in which the users express their feelings toward different objects (like
movies) by the means of rating is demonstrated by performing extensive
experiments."
305,"How can we recognise social roles of people, given a completely unlabelled
social network? We present a transfer learning approach to network role
classification based on feature transformations from each network's local
feature distribution to a global feature space. Experiments are carried out on
real-world datasets. (See manuscript for the full abstract.)"
306,"We discuss a variant of `blind' community detection, in which we aim to
partition an unobserved network from the observation of a (dynamical) graph
signal defined on the network. We consider a scenario where our observed graph
signals are obtained by filtering white noise input, and the underlying network
is different for every observation. In this fashion, the filtered graph signals
can be interpreted as defined on a time-varying network. We model each of the
underlying network realizations as generated by an independent draw from a
latent stochastic blockmodel (SBM). To infer the partition of the latent SBM,
we propose a simple spectral algorithm for which we provide a theoretical
analysis and establish consistency guarantees for the recovery. We illustrate
our results using numerical experiments on synthetic and real data,
highlighting the efficacy of our approach."
307,"The discourse around conspiracy theories is currently thriving amidst the
rampant misinformation in online environments. Research in this field has been
focused on detecting conspiracy theories on social media, often relying on
limited datasets. In this study, we present a novel methodology for
constructing a Twitter dataset that encompasses accounts engaged in
conspiracy-related activities throughout the year 2022. Our approach centers on
data collection that is independent of specific conspiracy theories and
information operations. Additionally, our dataset includes a control group
comprising randomly selected users who can be fairly compared to the
individuals involved in conspiracy activities. This comprehensive collection
effort yielded a total of 15K accounts and 37M tweets extracted from their
timelines. We conduct a comparative analysis of the two groups across three
dimensions: topics, profiles, and behavioral characteristics. The results
indicate that conspiracy and control users exhibit similarity in terms of their
profile metadata characteristics. However, they diverge significantly in terms
of behavior and activity, particularly regarding the discussed topics, the
terminology used, and their stance on trending subjects. In addition, we find
no significant disparity in the presence of bot users between the two groups.
Finally, we develop a classifier to identify conspiracy users using features
borrowed from bot, troll and linguistic literature. The results demonstrate a
high accuracy level (with an F1 score of 0.94), enabling us to uncover the most
discriminating features associated with conspiracy-related accounts."
308,"In this work, we study the utility of graph embeddings to generate latent
user representations for trust-based collaborative filtering. In a cold-start
setting, on three publicly available datasets, we evaluate approaches from four
method families: (i) factorization-based, (ii) random walk-based, (iii) deep
learning-based, and (iv) the Large-scale Information Network Embedding (LINE)
approach. We find that across the four families, random-walk-based approaches
consistently achieve the best accuracy. Besides, they result in highly novel
and diverse recommendations. Furthermore, our results show that the use of
graph embeddings in trust-based collaborative filtering significantly improves
user coverage."
309,"Networks found in the real-world are numerous and varied. A common type of
network is the heterogeneous network, where the nodes (and edges) can be of
different types. Accordingly, there have been efforts at learning
representations of these heterogeneous networks in low-dimensional space.
However, most of the existing heterogeneous network embedding methods suffer
from the following two drawbacks: (1) The target space is usually Euclidean.
Conversely, many recent works have shown that complex networks may have
hyperbolic latent anatomy, which is non-Euclidean. (2) These methods usually
rely on meta-paths, which require domain-specific prior knowledge for meta-path
selection. Additionally, different down-streaming tasks on the same network
might require different meta-paths in order to generate task-specific
embeddings. In this paper, we propose a novel self-guided random walk method
that does not require meta-path for embedding heterogeneous networks into
hyperbolic space. We conduct thorough experiments for the tasks of network
reconstruction and link prediction on two public datasets, showing that our
model outperforms a variety of well-known baselines across all tasks."
310,"Bipartite graph embedding has recently attracted much attention due to the
fact that bipartite graphs are widely used in various application domains. Most
previous methods, which adopt random walk-based or reconstruction-based
objectives, are typically effective to learn local graph structures. However,
the global properties of bipartite graph, including community structures of
homogeneous nodes and long-range dependencies of heterogeneous nodes, are not
well preserved. In this paper, we propose a bipartite graph embedding called
BiGI to capture such global properties by introducing a novel local-global
infomax objective. Specifically, BiGI first generates a global representation
which is composed of two prototype representations. BiGI then encodes sampled
edges as local representations via the proposed subgraph-level attention
mechanism. Through maximizing the mutual information between local and global
representations, BiGI enables nodes in bipartite graph to be globally relevant.
Our model is evaluated on various benchmark datasets for the tasks of top-K
recommendation and link prediction. Extensive experiments demonstrate that BiGI
achieves consistent and significant improvements over state-of-the-art
baselines. Detailed analyses verify the high effectiveness of modeling the
global properties of bipartite graph."
311,"We propose a method for simultaneously detecting shared and unshared
communities in heterogeneous multilayer weighted and undirected networks. The
multilayer network is assumed to follow a generative probabilistic model that
takes into account the similarities and dissimilarities between the
communities. We make use of a variational Bayes approach for jointly inferring
the shared and unshared hidden communities from multilayer network
observations. We show that our approach outperforms state-of-the-art algorithms
in detecting disparate (shared and private) communities on synthetic data as
well as on real genome-wide fibroblast proliferation dataset."
312,"Many works have been proposed in the literature to capture the dynamics of
diffusion in networks. While some of them define graphical markovian models to
extract temporal relationships between node infections in networks, others
consider diffusion episodes as sequences of infections via recurrent neural
models. In this paper we propose a model at the crossroads of these two
extremes, which embeds the history of diffusion in infected nodes as hidden
continuous states. Depending on the trajectory followed by the content before
reaching a given node, the distribution of influence probabilities may vary.
However, content trajectories are usually hidden in the data, which induces
challenging learning problems. We propose a topological recurrent neural model
which exhibits good experimental performances for diffusion modelling and
prediction."
313,"Inferring latent attributes of people online is an important social computing
task, but requires integrating the many heterogeneous sources of information
available on the web. We propose learning individual representations of people
using neural nets to integrate rich linguistic and network evidence gathered
from social media. The algorithm is able to combine diverse cues, such as the
text a person writes, their attributes (e.g. gender, employer, education,
location) and social relations to other people. We show that by integrating
both textual and network evidence, these representations offer improved
performance at four important tasks in social media inference on Twitter:
predicting (1) gender, (2) occupation, (3) location, and (4) friendships for
users. Our approach scales to large datasets and the learned representations
can be used as general features in and have the potential to benefit a large
number of downstream tasks including link prediction, community detection, or
probabilistic reasoning over social networks."
314,"In this short paper, we evaluate the performance of three well-known Machine
Learning techniques for predicting the impact of a post in Facebook. Social
medias have a huge influence in the social behaviour. Therefore to develop an
automatic model for predicting the impact of posts in social medias can be
useful to the society. In this article, we analyze the efficiency for
predicting the post impact of three popular techniques: Support Vector
Regression (SVR), Echo State Network (ESN) and Adaptive Network Fuzzy Inject
System (ANFIS). The evaluation was done over a public and well-known benchmark
dataset."
315,"An identity denotes the role an individual or a group plays in highly
differentiated contemporary societies. In this paper, our goal is to classify
Twitter users based on their role identities. We first collect a coarse-grained
public figure dataset automatically, then manually label a more fine-grained
identity dataset. We propose a hierarchical self-attention neural network for
Twitter user role identity classification. Our experiments demonstrate that the
proposed model significantly outperforms multiple baselines. We further propose
a transfer learning scheme that improves our model's performance by a large
margin. Such transfer learning also greatly reduces the need for a large amount
of human labeled data."
316,"In this work we propose Lasagne, a methodology to learn locality and
structure aware graph node embeddings in an unsupervised way. In particular, we
show that the performance of existing random-walk based approaches depends
strongly on the structural properties of the graph, e.g., the size of the
graph, whether the graph has a flat or upward-sloping Network Community Profile
(NCP), whether the graph is expander-like, whether the classes of interest are
more k-core-like or more peripheral, etc. For larger graphs with flat NCPs that
are strongly expander-like, existing methods lead to random walks that expand
rapidly, touching many dissimilar nodes, thereby leading to lower-quality
vector representations that are less useful for downstream tasks. Rather than
relying on global random walks or neighbors within fixed hop distances, Lasagne
exploits strongly local Approximate Personalized PageRank stationary
distributions to more precisely engineer local information into node
embeddings. This leads, in particular, to more meaningful and more useful
vector representations of nodes in poorly-structured graphs. We show that
Lasagne leads to significant improvement in downstream multi-label
classification for larger graphs with flat NCPs, that it is comparable for
smaller graphs with upward-sloping NCPs, and that is comparable to existing
methods for link prediction tasks."
317,"On 26 January 2021, India witnessed a national embarrassment from the
demographic least expected from - farmers. People across the nation watched in
horror as a pseudo-patriotic mob of farmers stormed capital Delhi and
vandalized the national pride- Red Fort. Investigations that followed the event
revealed the existence of a social media trail that led to the likes of such an
event. Consequently, it became essential and necessary to archive this trail
for social media analysis - not only to understand the bread-crumbs that are
dispersed across the trail but also to visualize the role played by
misinformation and fake news in this event. In this paper, we propose the
tractor2twitter dataset which contains around 0.05 million tweets that were
posted before, during, and after this event. Also, we benchmark our dataset
with an Explainable AI ML model for classification of each tweet into either of
the three categories - disinformation, misinformation, and opinion."
318,"Social networks are often associated with rich side information, such as
texts and images. While numerous methods have been developed to identify
communities from pairwise interactions, they usually ignore such side
information. In this work, we study an extension of the Stochastic Block Model
(SBM), a widely used statistical framework for community detection, that
integrates vectorial edges covariates: the Vectorial Edges Covariates
Stochastic Block Model (VEC-SBM). We propose a novel algorithm based on
iterative refinement techniques and show that it optimally recovers the latent
communities under the VEC-SBM. Furthermore, we rigorously assess the added
value of leveraging edge's side information in the community detection process.
We complement our theoretical results with numerical experiments on synthetic
and semi-synthetic data."
319,"In standard graph clustering/community detection, one is interested in
partitioning the graph into more densely connected subsets of nodes. In
contrast, the ""search"" problem of this paper aims to only find the nodes in a
""single"" such community, the target, out of the many communities that may
exist. To do so , we are given suitable side information about the target; for
example, a very small number of nodes from the target are labeled as such.
  We consider a general yet simple notion of side information: all nodes are
assumed to have random weights, with nodes in the target having higher weights
on average. Given these weights and the graph, we develop a variant of the
method of moments that identifies nodes in the target more reliably, and with
lower computation, than generic community detection methods that do not use
side information and partition the entire graph. Our empirical results show
significant gains in runtime, and also gains in accuracy over other graph
clustering algorithms."
320,"We consider the problem of learning the weighted edges of a graph by
observing the noisy times of infection for multiple epidemic cascades on this
graph. Past work has considered this problem when the cascade information,
i.e., infection times, are known exactly. Though the noisy setting is well
motivated by many epidemic processes (e.g., most human epidemics), to the best
of our knowledge, very little is known about when it is solvable. Previous work
on the no-noise setting critically uses the ordering information. If noise can
reverse this -- a node's reported (noisy) infection time comes after the
reported infection time of some node it infected -- then we are unable to see
how previous results can be extended.
  We therefore tackle two versions of the noisy setting: the limited-noise
setting, where we know noisy times of infections, and the extreme-noise
setting, in which we only know whether or not a node was infected. We provide a
polynomial time algorithm for recovering the structure of bidirectional trees
in the extreme-noise setting, and show our algorithm almost matches lower
bounds established in the no-noise setting, and hence is optimal up to
log-factors. We extend our results for general degree-bounded graphs, where
again we show that our (poly-time) algorithm can recover the structure of the
graph with optimal sample complexity. We also provide the first efficient
algorithm to learn the weights of the bidirectional tree in the limited-noise
setting. Finally, we give a polynomial time algorithm for learning the weights
of general bounded-degree graphs in the limited-noise setting. This algorithm
extends to general graphs (at the price of exponential running time), proving
the problem is solvable in the general case. All our algorithms work for any
noise distribution, without any restriction on the variance."
321,"Closeness Centrality (CC) and Betweenness Centrality (BC) are crucial metrics
in network analysis, providing essential reference for discerning the
significance of nodes within complex networks. These measures find wide
applications in critical tasks, such as community detection and network
dismantling. However, their practical implementation on extensive networks
remains computationally demanding due to their high time complexity. To
mitigate these computational challenges, numerous approximation algorithms have
been developed to expedite the computation of CC and BC. Nevertheless, even
these approximations still necessitate substantial processing time when applied
to large-scale networks. Furthermore, their output proves sensitive to even
minor perturbations within the network structure.
  In this work, We redefine the CC and BC node ranking problem as a machine
learning problem and propose the CNCA-IGE model, which is an encoder-decoder
model based on inductive graph neural networks designed to rank nodes based on
specified CC or BC metrics. We incorporate the MLP-Mixer model as the decoder
in the BC ranking prediction task to enhance the model's robustness and
capacity. Our approach is evaluated on diverse synthetic and real-world
networks of varying scales, and the experimental results demonstrate that the
CNCA-IGE model outperforms state-of-the-art baseline models, significantly
reducing execution time while improving performance."
322,"We propose a detailed analysis of the online-learning problem for Independent
Cascade (IC) models under node-level feedback. These models have widespread
applications in modern social networks. Existing works for IC models have only
shed light on edge-level feedback models, where the agent knows the explicit
outcome of every observed edge. Little is known about node-level feedback
models, where only combined outcomes for sets of edges are observed; in other
words, the realization of each edge is censored. This censored information,
together with the nonlinear form of the aggregated influence probability, make
both parameter estimation and algorithm design challenging. We establish the
first confidence-region result under this setting. We also develop an online
algorithm achieving a cumulative regret of $\mathcal{O}( \sqrt{T})$, matching
the theoretical regret bound for IC models with edge-level feedback."
323,"Much of the complexity of social, biological, and engineered systems arises
from a network of complex interactions connecting many basic components.
Network analysis tools have been successful at uncovering latent structure
termed communities in such networks. However, some of the most interesting
structure can be difficult to uncover because it is obscured by the more
dominant structure. Our previous work proposes a general structure
amplification technique called HICODE that uncovers many layers of functional
hidden structure in complex networks. HICODE incrementally weakens dominant
structure through randomization allowing the hidden functionality to emerge,
and uncovers these hidden structure in real-world networks that previous
methods rarely uncover. In this work, we conduct a comprehensive and systematic
theoretical analysis on the hidden community structure. In what follows, we
define multi-layer stochastic block model, and provide theoretical support
using the model on why the existence of hidden structure will make the
detection of dominant structure harder compared with equivalent random noise.
We then provide theoretical proofs that the iterative reducing methods could
help promote the uncovering of hidden structure as well as boosting the
detection quality of dominant structure."
324,"Existing network embedding approaches tackle the problem of learning
low-dimensional node representations. However, networks can also be seen in the
light of edges interlinking pairs of nodes. The broad goal of this paper is to
introduce edge-centric network embeddings. We present an approach called ECNE,
which instead of computing node embeddings directly, computes edge embeddings
by relying on the notion of line graph coupled with an edge weighting mechanism
to preserve the dynamic of the original graph in the line graph. We also
present a link prediction framework called ECNE-LP, which given a target link
(u,v) first collects paths between nodes u and v, then directly embeds the
edges in these paths, and finally aggregates them toward predicting the
existence of a link. We show that both ECNE and ECNE-LP bring benefit wrt the
state-of-the-art."
325,"A collection of $U \: (\in \mathbb{N})$ data vectors is called a $U$-tuple,
and the association strength among the vectors of a tuple is termed as the
\emph{hyperlink weight}, that is assumed to be symmetric with respect to
permutation of the entries in the index. We herein propose Bregman hyperlink
regression (BHLR), which learns a user-specified symmetric similarity function
such that it predicts the tuple's hyperlink weight from data vectors stored in
the $U$-tuple. BHLR is a simple and general framework for hyper-relational
learning, that minimizes Bregman-divergence (BD) between the hyperlink weights
and estimated similarities defined for the corresponding tuples; BHLR
encompasses various existing methods, such as logistic regression ($U=1$),
Poisson regression ($U=1$), link prediction ($U=2$), and those for
representation learning, such as graph embedding ($U=2$), matrix factorization
($U=2$), tensor factorization ($U \geq 2$), and their variants equipped with
arbitrary BD. Nonlinear functions (e.g., neural networks), can be employed for
the similarity functions. However, there are theoretical challenges such that
some of different tuples of BHLR may share data vectors therein, unlike the
i.i.d. setting of classical regression. We address these theoretical issues,
and proved that BHLR equipped with arbitrary BD and $U \in \mathbb{N}$ is (P-1)
statistically consistent, that is, it asymptotically recovers the underlying
true conditional expectation of hyperlink weights given data vectors, and (P-2)
computationally tractable, that is, it is efficiently computed by stochastic
optimization algorithms using a novel generalized minibatch sampling procedure
for hyper-relational data. Consequently, theoretical guarantees for BHLR
including several existing methods, that have been examined experimentally, are
provided in a unified manner."
326,"Though current researches often study the properties of online social
relationship from an objective view, we also need to understand individuals'
subjective opinions on their interrelationships in social computing studies.
Inspired by the theories from sociolinguistics, the latest work indicates that
interactive language can reveal individuals' asymmetric opinions on their
interrelationship. In this work, in order to explain the opinions' asymmetry on
interrelationship with more latent factors, we extend the investigation from
single relationship to the structural context in online social network. We
analyze the correlation between interactive language features and the
structural context of interrelationships. The structural context of vertex,
edges and triangles in social network are considered. With statistical analysis
on Enron email dataset, we find that individuals' opinions (measured by
interactive language features) on their interrelationship are related to some
of their important structural context in social network. This result can help
us to understand and measure the individuals' opinions on their
interrelationship with more intrinsic information."
327,"Link prediction is widely used in a variety of industrial applications, such
as merchant recommendation, fraudulent transaction detection, and so on.
However, it's a great challenge to train and deploy a link prediction model on
industrial-scale graphs with billions of nodes and edges. In this work, we
present a scalable and distributed framework for semi-supervised link
prediction problem (named DSSLP), which is able to handle industrial-scale
graphs. Instead of training model on the whole graph, DSSLP is proposed to
train on the \emph{$k$-hops neighborhood} of nodes in a mini-batch setting,
which helps reduce the scale of the input graph and distribute the training
procedure. In order to generate negative examples effectively, DSSLP contains a
distributed batched runtime sampling module. It implements uniform and dynamic
sampling approaches, and is able to adaptively construct positive and negative
examples to guide the training process. Moreover, DSSLP proposes a model-split
strategy to accelerate the speed of inference process of the link prediction
task. Experimental results demonstrate that the effectiveness and efficiency of
DSSLP in serval public datasets as well as real-world datasets of
industrial-scale graphs."
328,"We explore a method to influence or even control the diversity of opinions
within a polarised social group. We leverage the voter model in which users
hold binary opinions and repeatedly update their beliefs based on others they
connect with. Stubborn agents who never change their minds (""zealots"") are also
disseminated through the network, which is modelled by a connected graph.
Building on earlier results, we provide a closed-form expression for the
average opinion of the group at equilibrium. This leads us to a strategy to
inject zealots into a polarised network in order to shift the average opinion
towards any target value. We account for the possible presence of a backfire
effect, which may lead the group to react negatively and reinforce its level of
polarisation in response. Our results are supported by numerical experiments on
synthetic data."
329,"Arabic Twitter space is crawling with bots that fuel political feuds, spread
misinformation, and proliferate sectarian rhetoric. While efforts have long
existed to analyze and detect English bots, Arabic bot detection and
characterization remains largely understudied. In this work, we contribute new
insights into the role of bots in spreading religious hatred on Arabic Twitter
and introduce a novel regression model that can accurately identify Arabic
language bots. Our assessment shows that existing tools that are highly
accurate in detecting English bots don't perform as well on Arabic bots. We
identify the possible reasons for this poor performance, perform a thorough
analysis of linguistic, content, behavioral and network features, and report on
the most informative features that distinguish Arabic bots from humans as well
as the differences between Arabic and English bots. Our results mark an
important step toward understanding the behavior of malicious bots on Arabic
Twitter and pave the way for a more effective Arabic bot detection tools."
330,"We present a novel active learning algorithm for community detection on
networks. Our proposed algorithm uses a Maximal Expected Model Change (MEMC)
criterion for querying network nodes label assignments. MEMC detects nodes that
maximally change the community assignment likelihood model following a query.
Our method is inspired by detection in the benchmark Stochastic Block Model
(SBM), where we provide sample complexity analysis and empirical study with SBM
and real network data for binary as well as for the multi-class settings. The
analysis also covers the most challenging case of sparse degree and
below-detection-threshold SBMs, where we observe a super-linear error
reduction. MEMC is shown to be superior to the random selection baseline and
other state-of-the-art active learners."
331,"Using an intuitive concept of what constitutes a meaningful community, a
novel metric is formulated for detecting non-overlapping communities in
undirected, weighted heterogeneous networks. This metric, modularity density,
is shown to be superior to the versions of modularity density in present
literature. Compared to the previous versions of modularity density,
maximization of our metric is proven to be free from bias and better detect
weakly-separated communities particularly in heterogeneous networks. In
addition to these characteristics, the computational running time of our
modularity density is found to be on par or faster than that of the previous
variants. Our findings further reveal that community detection by maximization
of our metric is mathematically related to partitioning a network by
minimization of the normalized cut criterion."
332,"Generative models for graphs have been typically committed to strong prior
assumptions concerning the form of the modeled distributions. Moreover, the
vast majority of currently available models are either only suitable for
characterizing some particular network properties (such as degree distribution
or clustering coefficient), or they are aimed at estimating joint probability
distributions, which is often intractable in large-scale networks. In this
paper, we first propose a novel network statistic, based on the Laplacian
spectrum of graphs, which allows to dispense with any parametric assumption
concerning the modeled network properties. Second, we use the defined statistic
to develop the Fiedler random graph model, switching the focus from the
estimation of joint probability distributions to a more tractable conditional
estimation setting. After analyzing the dependence structure characterizing
Fiedler random graphs, we evaluate them experimentally in edge prediction over
several real-world networks, showing that they allow to reach a much higher
prediction accuracy than various alternative statistical models."
333,"We consider the problem of signal recovery on graphs as graphs model data
with complex structure as signals on a graph. Graph signal recovery implies
recovery of one or multiple smooth graph signals from noisy, corrupted, or
incomplete measurements. We propose a graph signal model and formulate signal
recovery as a corresponding optimization problem. We provide a general solution
by using the alternating direction methods of multipliers. We next show how
signal inpainting, matrix completion, robust principal component analysis, and
anomaly detection all relate to graph signal recovery, and provide
corresponding specific solutions and theoretical analysis. Finally, we validate
the proposed methods on real-world recovery problems, including online blog
classification, bridge condition identification, temperature estimation,
recommender system, and expert opinion combination of online blog
classification."
334,"Using different methods for laying out a graph can lead to very different
visual appearances, with which the viewer perceives different information.
Selecting a ""good"" layout method is thus important for visualizing a graph. The
selection can be highly subjective and dependent on the given task. A common
approach to selecting a good layout is to use aesthetic criteria and visual
inspection. However, fully calculating various layouts and their associated
aesthetic metrics is computationally expensive. In this paper, we present a
machine learning approach to large graph visualization based on computing the
topological similarity of graphs using graph kernels. For a given graph, our
approach can show what the graph would look like in different layouts and
estimate their corresponding aesthetic metrics. An important contribution of
our work is the development of a new framework to design graph kernels. Our
experimental study shows that our estimation calculation is considerably faster
than computing the actual layouts and their aesthetic metrics. Also, our graph
kernels outperform the state-of-the-art ones in both time and accuracy. In
addition, we conducted a user study to demonstrate that the topological
similarity computed with our graph kernel matches perceptual similarity
assessed by human users."
335,"Community detection is one of the most important and interesting issues in
social network analysis. In recent years, simultaneous considering of nodes'
attributes and topological structures of social networks in the process of
community detection has attracted the attentions of many scholars, and this
consideration has been recently used in some community detection methods to
increase their efficiencies and to enhance their performances in finding
meaningful and relevant communities. But the problem is that most of these
methods tend to find non-overlapping communities, while many real-world
networks include communities that often overlap to some extent. In order to
solve this problem, an evolutionary algorithm called MOBBO-OCD, which is based
on multi-objective biogeography-based optimization (BBO), is proposed in this
paper to automatically find overlapping communities in a social network with
node attributes with synchronously considering the density of connections and
the similarity of nodes' attributes in the network. In MOBBO-OCD, an extended
locus-based adjacency representation called OLAR is introduced to encode and
decode overlapping communities. Based on OLAR, a rank-based migration operator
along with a novel two-phase mutation strategy and a new double-point crossover
are used in the evolution process of MOBBO-OCD to effectively lead the
population into the evolution path. In order to assess the performance of
MOBBO-OCD, a new metric called alpha_SAEM is proposed in this paper, which is
able to evaluate the goodness of both overlapping and non-overlapping
partitions with considering the two aspects of node attributes and linkage
structure. Quantitative evaluations reveal that MOBBO-OCD achieves favorable
results which are quite superior to the results of 15 relevant community
detection algorithms in the literature."
336,"Natural language programming is a promising approach to enable end users to
instruct new tasks for intelligent agents. However, our formative study found
that end users would often use unclear, ambiguous or vague concepts when
naturally instructing tasks in natural language, especially when specifying
conditionals. Existing systems have limited support for letting the user teach
agents new concepts or explaining unclear concepts. In this paper, we describe
a new multi-modal domain-independent approach that combines natural language
programming and programming-by-demonstration to allow users to first naturally
describe tasks and associated conditions at a high level, and then collaborate
with the agent to recursively resolve any ambiguities or vagueness through
conversations and demonstrations. Users can also define new procedures and
concepts by demonstrating and referring to contents within GUIs of existing
mobile apps. We demonstrate this approach in PUMICE, an end-user programmable
agent that implements this approach. A lab study with 10 users showed its
usability."
337,"This research report presents a proof-of-concept study on the application of
machine learning techniques to video and speech data collected during
diagnostic cognitive assessments of children with a neurodevelopmental
disorder. The study utilised a dataset of 39 video recordings, capturing
extensive sessions where clinicians administered, among other things, four
cognitive assessment tests. From the first 40 minutes of each clinical session,
covering the administration of the Wechsler Intelligence Scale for Children
(WISC-V), we extracted head positions and speech turns of both clinician and
child. Despite the limited sample size and heterogeneous recording styles, the
analysis successfully extracted path signatures as features from the recorded
data, focusing on patient-clinician interactions. Importantly, these features
quantify the interpersonal dynamics of the assessment process (dialogue and
movement patterns). Results suggest that these features exhibit promising
potential for predicting all cognitive tests scores of the entire session
length and for prototyping a predictive model as a clinical decision support
tool. Overall, this proof of concept demonstrates the feasibility of leveraging
machine learning techniques for clinical video and speech data analysis in
order to potentially enhance the efficiency of cognitive assessments for
neurodevelopmental disorders in children."
338,"We explore the expression of personality and adaptivity through the gestures
of virtual agents in a storytelling task. We conduct two experiments using four
different dialogic stories. We manipulate agent personality on the extraversion
scale, whether the agents adapt to one another in their gestural performance
and agent gender. Our results show that subjects are able to perceive the
intended variation in extraversion between different virtual agents,
independently of the story they are telling and the gender of the agent. A
second study shows that subjects also prefer adaptive to nonadaptive virtual
agents."
339,"Human-AI collaboration has the potential to transform various domains by
leveraging the complementary strengths of human experts and Artificial
Intelligence (AI) systems. However, unobserved confounding can undermine the
effectiveness of this collaboration, leading to biased and unreliable outcomes.
In this paper, we propose a novel solution to address unobserved confounding in
human-AI collaboration by employing the marginal sensitivity model (MSM). Our
approach combines domain expertise with AI-driven statistical modeling to
account for potential confounders that may otherwise remain hidden. We present
a deferral collaboration framework for incorporating the MSM into policy
learning from observational data, enabling the system to control for the
influence of unobserved confounding factors. In addition, we propose a
personalized deferral collaboration system to leverage the diverse expertise of
different human decision-makers. By adjusting for potential biases, our
proposed solution enhances the robustness and reliability of collaborative
outcomes. The empirical and theoretical analyses demonstrate the efficacy of
our approach in mitigating unobserved confounding and improving the overall
performance of human-AI collaborations."
340,"Recent work has explored how complementary strengths of humans and artificial
intelligence (AI) systems might be productively combined. However, successful
forms of human-AI partnership have rarely been demonstrated in real-world
settings. We present the iterative design and evaluation of Lumilo, smart
glasses that help teachers help their students in AI-supported classrooms by
presenting real-time analytics about students' learning, metacognition, and
behavior. Results from a field study conducted in K-12 classrooms indicate that
students learn more when teachers and AI tutors work together during class. We
discuss implications of this research for the design of human-AI partnerships.
We argue for more participatory approaches to research and design in this area,
in which practitioners and other stakeholders are deeply, meaningfully involved
throughout the process. Furthermore, we advocate for theory-building and for
principled approaches to the study of human-AI decision-making in real-world
contexts."
341,"The proliferation of text messaging for mobile health is generating a large
amount of patient-doctor conversations that can be extremely valuable to health
care professionals. We present ConVIScope, a visual text analytic system that
tightly integrates interactive visualization with natural language processing
in analyzing patient-doctor conversations. ConVIScope was developed in
collaboration with healthcare professionals following a user-centered iterative
design. Case studies with six domain experts suggest the potential utility of
ConVIScope and reveal lessons for further developments."
342,"The rise of Artificial Intelligence (AI) and Generative Artificial
Intelligence (GenAI) in higher education necessitates assessment reform. This
study addresses a critical gap by exploring student and academic staff
experiences with AI and GenAI tools, focusing on their familiarity and comfort
with current and potential future applications in learning and assessment. An
online survey collected data from 35 academic staff and 282 students across two
universities in Vietnam and one in Singapore, examining GenAI familiarity,
perceptions of its use in assessment marking and feedback, knowledge checking
and participation, and experiences of GenAI text detection.
  Descriptive statistics and reflexive thematic analysis revealed a generally
low familiarity with GenAI among both groups. GenAI feedback was viewed
negatively; however, it was viewed more positively when combined with
instructor feedback. Academic staff were more accepting of GenAI text detection
tools and grade adjustments based on detection results compared to students.
Qualitative analysis identified three themes: unclear understanding of text
detection tools, variability in experiences with GenAI detectors, and mixed
feelings about GenAI's future impact on educational assessment. These findings
have major implications regarding the development of policies and practices for
GenAI-enabled assessment and feedback in higher education."
343,"From deciding on a PhD program to buying a new camera, unfamiliar
decisions--decisions without domain knowledge--are frequent and significant.
The complexity and uncertainty of such decisions demand unique approaches to
information seeking, understanding, and decision-making. Our formative study
highlights that users want to start by discovering broad and relevant domain
information evenly and simultaneously, quickly address emerging inquiries, and
gain personalized standards to assess information found. We present
ChoiceMates, an interactive multi-agent system designed to address these needs
by enabling users to engage with a dynamic set of LLM agents each presenting a
unique experience in the domain. Unlike existing multi-agent systems that
automate tasks with agents, the user orchestrates agents to assist their
decision-making process. Our user evaluation (n=12) shows that ChoiceMates
enables a more confident, satisfactory decision-making with better situation
understanding than web search, and higher decision quality and confidence than
a commercial multi-agent framework. This work provides insights into designing
a more controllable and collaborative multi-agent system."
344,"Data visualization and interaction with large data sets is known to be
essential and critical in many businesses today, and the same applies to
research and teaching, in this case, when exploring large and complex
mathematical objects. GAP is a computer algebra system for computational
discrete algebra with an emphasis on computational group theory. The existing
XGAP package for GAP works exclusively on the X Window System. It lacks
abstraction between its mathematical and graphical cores, making it difficult
to extend, maintain, or port. In this paper, we present Francy, a graphical
semantics package for GAP. Francy is responsible for creating a
representational structure that can be rendered using many GUI frameworks
independent from any particular programming language or operating system.
Building on this, we use state of the art web technologies that take advantage
of an improved REPL environment, which is currently under development for GAP.
The integration of this project with Jupyter provides a rich graphical
environment full of features enhancing the usability and accessibility of GAP."
345,"The think aloud method is an important and commonly used tool for usability
optimization. However, analyzing think aloud data could be time consuming. In
this paper, we put forth an automatic analysis of verbal protocols and test the
link between spoken feedback and the stimulus using eye tracking and mouse
tracking. The gained data - user feedback linked to a specific area of the
stimulus - could be used to let an expert review the feedback on specific web
page elements or to visualize on which parts of the web page the feedback was
given. Specifically, we test if participants fixate on or point with the mouse
to the content of the webpage that they are verbalizing. During the testing,
participants were shown three websites and asked to verbally give their
opinion. The verbal responses, along with the eye and cursor movements were
recorded. We compared the hit rate, defined as the percentage of verbally
mentioned areas of interest (AOIs) that were fixated with gaze or pointed to
with the mouse. The results revealed a significantly higher hit rate for the
gaze compared to the mouse data. Further investigation revealed that, while the
mouse was mostly used passively to scroll, the gaze was often directed towards
relevant AOIs, thus establishing a strong association between spoken words and
stimuli. Therefore, eye tracking data possibly provides more detailed
information and more valuable insights about the verbalizations compared to the
mouse data."
346,"Game balancing is an important part of the (computer) game design process, in
which designers adapt a game prototype so that the resulting gameplay is as
entertaining as possible. In industry, the evaluation of a game is often based
on costly playtests with human players. It suggests itself to automate this
process using surrogate models for the prediction of gameplay and outcome. In
this paper, the feasibility of automatic balancing using simulation- and
deck-based objectives is investigated for the card game top trumps.
Additionally, the necessity of a multi-objective approach is asserted by a
comparison with the only known (single-objective) method. We apply a
multi-objective evolutionary algorithm to obtain decks that optimise
objectives, e.g. win rate and average number of tricks, developed to express
the fairness and the excitement of a game of top trumps. The results are
compared with decks from published top trumps decks using simulation-based
objectives. The possibility to generate decks better or at least as good as
decks from published top trumps decks in terms of these objectives is
demonstrated. Our results indicate that automatic balancing with the presented
approach is feasible even for more complex games such as real-time strategy
games."
347,"While Machine learning gives rise to astonishing results in automated
systems, it is usually at the cost of large data requirements. This makes many
successful algorithms from machine learning unsuitable for human-machine
interaction, where the machine must learn from a small number of training
samples that can be provided by a user within a reasonable time frame.
Fortunately, the user can tailor the training data they create to be as useful
as possible, severely limiting its necessary size -- as long as they know about
the machine's requirements and limitations. Of course, acquiring this knowledge
can in turn be cumbersome and costly. This raises the question of how easy
machine learning algorithms are to interact with. In this work, we address this
issue by analyzing the intuitiveness of certain algorithms when they are
actively taught by users. After developing a theoretical framework of
intuitiveness as a property of algorithms, we introduce an active teaching
paradigm involving a prototypical two-dimensional spatial learning task as a
method to judge the efficacy of human-machine interactions. Finally, we present
and discuss the results of a large-scale user study into the performance and
teaching strategies of 800 users interacting with two prominent machine
learning algorithms in our system, providing first evidence for the role of
intuition as an important factor impacting human-machine interaction."
348,"When performing a task alone, humans achieve a certain level of performance.
When humans are assisted by a tool or automation to perform the same task,
performance is enhanced (augmented). Recently developed cognitive systems are
able to perform cognitive processing at or above the level of a human in some
domains. When humans work collaboratively with such cogs in a human/cog
ensemble, we expect augmentation of cognitive processing to be evident and
measurable. This paper shows the degree of cognitive augmentation depends on
the nature of the information the cog contributes to the ensemble. Results of
an experiment are reported showing conceptual information is the most effective
type of information resulting in increases in cognitive accuracy, cognitive
precision, and cognitive power."
349,"Advances in artificial intelligence and human-computer interaction will
likely lead to extended reality (XR) becoming pervasive. While XR can provide
users with interactive, engaging, and immersive experiences, non-player
characters are often utilized in pre-scripted and conventional ways. This paper
argues for using large language models (LLMs) in XR by embedding them in
avatars or as narratives to facilitate inclusion through prompt engineering and
fine-tuning the LLMs. We argue that this inclusion will promote diversity for
XR use. Furthermore, the versatile conversational capabilities of LLMs will
likely increase engagement in XR, helping XR become ubiquitous. Lastly, we
speculate that combining the information provided to LLM-powered spaces by
users and the biometric data obtained might lead to novel privacy invasions.
While exploring potential privacy breaches, examining user privacy concerns and
preferences is also essential. Therefore, despite challenges, LLM-powered XR is
a promising area with several opportunities."
350,"Short videos on social media are the dominant way young people consume
content. News outlets aim to reach audiences through news reels -- short videos
conveying news -- but struggle to translate traditional journalistic formats
into short, entertaining videos. To translate news into social media reels, we
support journalists in reframing the narrative. In literature, narrative
framing is a high-level structure that shapes the overall presentation of a
story. We identified three narrative framings for reels that adapt social media
norms but preserve news value, each with a different balance of information and
entertainment. We introduce ReelFramer, a human-AI co-creative system that
helps journalists translate print articles into scripts and storyboards.
ReelFramer supports exploring multiple narrative framings to find one
appropriate to the story. AI suggests foundational narrative details, including
characters, plot, setting, and key information. ReelFramer also supports visual
framing; AI suggests character and visual detail designs before generating a
full storyboard. Our studies show that narrative framing introduces the
necessary diversity to translate various articles into reels, and establishing
foundational details helps generate scripts that are more relevant and
coherent. We also discuss the benefits of using narrative framing and
foundational details in content retargeting."
351,"Human and AI are increasingly interacting and collaborating to accomplish
various complex tasks in the context of diverse application domains (e.g.,
healthcare, transportation, and creative design). Two dynamic, learning
entities (AI and human) have distinct mental model, expertise, and ability;
such fundamental difference/mismatch offers opportunities for bringing new
perspectives to achieve better results. However, this mismatch can cause
unexpected failure and result in serious consequences. While recent research
has paid much attention to enhancing interpretability or explainability to
allow machine to explain how it makes a decision for supporting humans, this
research argues that there is urging the need for both human and AI should
develop specific, corresponding ability to interact and collaborate with each
other to form a human-AI team to accomplish superior results. This research
introduces a conceptual framework called ""Co-Learning,"" in which people can
learn with/from and grow with AI partners over time. We characterize three key
concepts of co-learning: ""mutual understanding,"" ""mutual benefits,"" and ""mutual
growth"" for facilitating human-AI collaboration on complex problem solving. We
will present proof-of-concepts to investigate whether and how our approach can
help human-AI team to understand and benefit each other, and ultimately improve
productivity and creativity on creative problem domains. The insights will
contribute to the design of Human-AI collaboration."
352,"Recently, research in human-robot interaction began to consider a robot's
influence at the group level. Despite the recent growth in research
investigating the effects of robots within groups of people, our overall
understanding of what happens when robots are placed within groups or teams of
people is still limited. This paper investigates several key problems for
social robots that manage conversations in a group setting, where the number of
participants is more than two. In a group setting, the conversation dynamics
are a lot more complicated than the conventional one-to-one conversation, thus,
there are more challenges need to be solved."
353,"Millions of people participate in online peer-to-peer support sessions, yet
there has been little prior research on systematic psychology-based evaluations
of fine-grained peer-counselor behavior in relation to client satisfaction.
This paper seeks to bridge this gap by mapping peer-counselor chat-messages to
motivational interviewing (MI) techniques. We annotate 14,797 utterances from
734 chat conversations using 17 MI techniques and introduce four new
interviewing codes such as chit-chat and inappropriate to account for the
unique conversational patterns observed on online platforms. We automate the
process of labeling peer-counselor responses to MI techniques by fine-tuning
large domain-specific language models and then use these automated measures to
investigate the behavior of the peer counselors via correlational studies.
Specifically, we study the impact of MI techniques on the conversation ratings
to investigate the techniques that predict clients' satisfaction with their
counseling sessions. When counselors use techniques such as reflection and
affirmation, clients are more satisfied. Examining volunteer counselors' change
in usage of techniques suggest that counselors learn to use more introduction
and open questions as they gain experience. This work provides a deeper
understanding of the use of motivational interviewing techniques on
peer-to-peer counselor platforms and sheds light on how to build better
training programs for volunteer counselors on online platforms."
354,"Explicitly alerting users is not always an optimal intervention, especially
when they are not motivated to obey. For example, in video-based learning,
learners who are distracted from the video would not follow an alert asking
them to pay attention. Inspired by the concept of Mindless Computing, we
propose a novel intervention approach, Mindless Attractor, that leverages the
nature of human speech communication to help learners refocus their attention
without relying on their motivation. Specifically, it perturbs the voice in the
video to direct their attention without consuming their conscious awareness.
Our experiments not only confirmed the validity of the proposed approach but
also emphasized its advantages in combination with a machine learning-based
sensing module. Namely, it would not frustrate users even though the
intervention is activated by false-positive detection of their attentive state.
Our intervention approach can be a reliable way to induce behavioral change in
human-AI symbiosis."
355,"We present AceWiki, a prototype of a new kind of semantic wiki using the
controlled natural language Attempto Controlled English (ACE) for representing
its content. ACE is a subset of English with a restricted grammar and a formal
semantics. The use of ACE has two important advantages over existing semantic
wikis. First, we can improve the usability and achieve a shallow learning
curve. Second, ACE is more expressive than the formal languages of existing
semantic wikis. Our evaluation shows that people who are not familiar with the
formal foundations of the Semantic Web are able to deal with AceWiki after a
very short learning phase and without the help of an expert."
356,"Despite the widespread use of artificial intelligence (AI), designing user
experiences (UX) for AI-powered systems remains challenging. UX designers face
hurdles understanding AI technologies, such as pre-trained language models, as
design materials. This limits their ability to ideate and make decisions about
whether, where, and how to use AI. To address this problem, we bridge the
literature on AI design and AI transparency to explore whether and how
frameworks for transparent model reporting can support design ideation with
pre-trained models. By interviewing 23 UX practitioners, we find that
practitioners frequently work with pre-trained models, but lack support for
UX-led ideation. Through a scenario-based design task, we identify common goals
that designers seek model understanding for and pinpoint their model
transparency information needs. Our study highlights the pivotal role that UX
designers can play in Responsible AI and calls for supporting their
understanding of AI limitations through model transparency and interrogation."
357,"In recent years, the CHI community has seen significant growth in research on
Human-Centered Responsible Artificial Intelligence. While different research
communities may use different terminology to discuss similar topics, all of
this work is ultimately aimed at developing AI that benefits humanity while
being grounded in human rights and ethics, and reducing the potential harms of
AI. In this special interest group, we aim to bring together researchers from
academia and industry interested in these topics to map current and future
research trends to advance this important area of research by fostering
collaboration and sharing ideas."
358,"Self-guided mental health interventions, such as ""do-it-yourself"" tools to
learn and practice coping strategies, show great promise to improve access to
mental health care. However, these interventions are often cognitively
demanding and emotionally triggering, creating accessibility barriers that
limit their wide-scale implementation and adoption. In this paper, we study how
human-language model interaction can support self-guided mental health
interventions. We take cognitive restructuring, an evidence-based therapeutic
technique to overcome negative thinking, as a case study. In an IRB-approved
randomized field study on a large mental health website with 15,531
participants, we design and evaluate a system that uses language models to
support people through various steps of cognitive restructuring. Our findings
reveal that our system positively impacts emotional intensity for 67% of
participants and helps 65% overcome negative thoughts. Although adolescents
report relatively worse outcomes, we find that tailored interventions that
simplify language model generations improve overall effectiveness and equity."
359,"Given the rapid advance in ITS technologies, future mobility is pointing to
vehicular autonomy. However, there is still a long way before full automation,
and human intervention is required. This work sheds light on understanding
human drivers' intervention behavior involved in the operation of autonomous
vehicles (AVs) and utilizes this knowledge to improve the perception of
critical driving scenarios. Experiment environments were implemented where the
virtual reality (VR) and traffic micro-simulation are integrated, and tests
were carried out under typical and diverse traffic scenes. Performance
indicators such as the probability of intervention, accident rates are defined
and used to quantify and compare the risk levels. By offering novel insights
into drivers' intervention behavior, this work will help improve the
performances of the automated control under similar scenarios. Furthermore,
such an integrated and immersive tool for autonomous driving studies will be
valuable for research on human-to-automation trust. To the best knowledge of
the authors, this work is among the pioneer works making efforts into such
types of tools."
360,"This paper explores the links between Knowledge Management and new
community-based models of the organization from both a theoretical and an
empirical perspective. From a theoretical standpoint, we look at Communities of
Practice (CoPs) and Knowledge Management (KM) and explore the links between the
two as they relate to the use of information systems to manage knowledge. We
begin by reviewing technologically supported approaches to KM and introduce the
idea of ""Systemes d'Aide a la Gestion des Connaissances"" SAGC (Systems to aid
the Management of Knowledge). Following this we examine the contribution that
communal structures such as CoPs can make to intraorganizational KM and
highlight some of 'success factors' for this approach to KM that are found in
the literature. From an empirical standpoint, we present the results of a
survey involving the Chief Knowledge Officers (CKOs) of twelve large French
businesses; the objective of this study was to identify the factors that might
influence the success of such approaches. The survey was analysed using
thematic content analysis and the results are presented here with some short
illustrative quotes from the CKOs. Finally, the paper concludes with some brief
reflections on what can be learnt from looking at this problem from these two
perspectives."
361,"Graphical User Interface (GUI) agents are designed to automate complex tasks
on digital devices, such as smartphones and desktops. Most existing GUI agents
interact with the environment through extracted structured data, which can be
notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops).
To alleviate this issue, we propose a novel visual GUI agent -- SeeClick, which
only relies on screenshots for task automation. In our preliminary study, we
have discovered a key challenge in developing visual GUI agents: GUI grounding
-- the capacity to accurately locate screen elements based on instructions. To
tackle this challenge, we propose to enhance SeeClick with GUI grounding
pre-training and devise a method to automate the curation of GUI grounding
data. Along with the efforts above, we have also created ScreenSpot, the first
realistic GUI grounding benchmark that encompasses mobile, desktop, and web
environments. After pre-training, SeeClick demonstrates significant improvement
in ScreenSpot over various baselines. Moreover, comprehensive evaluations on
three widely used benchmarks consistently support our finding that advancements
in GUI grounding directly correlate with enhanced performance in downstream GUI
agent tasks. The model, data and code are available at
https://github.com/njucckevin/SeeClick."
362,"This whitepaper reports on Project CLAI (Command Line AI), which aims to
bring the power of AI to the command line interface (CLI). The CLAI platform
sets up the CLI as a new environment for AI researchers to conquer by surfacing
the command line as a generic environment that researchers can interface to
using a simple sense-act API, much like the traditional AI agent architecture.
In this paper, we discuss the design and implementation of the platform in
detail, through illustrative use cases of new end user interaction patterns
enabled by this design, and through quantitative evaluation of the system
footprint of a CLAI-enabled terminal. We also report on some early user
feedback on CLAI's features from an internal survey."
363,"Human-swarm interaction (HSI) is an active research challenge in the realms
of swarm robotics and human-factors engineering. Here we apply a cognitive
systems engineering perspective and introduce a neuro-inspired joint systems
theory of HSI. The mindset defines predictions for adaptive, robust and
scalable HSI dynamics and therefore has the potential to inform human-swarm
loop design."
364,"Exploration has been one of the greatest challenges in reinforcement learning
(RL), which is a large obstacle in the application of RL to robotics. Even with
state-of-the-art RL algorithms, building a well-learned agent often requires
too many trials, mainly due to the difficulty of matching its actions with
rewards in the distant future. A remedy for this is to train an agent with
real-time feedback from a human observer who immediately gives rewards for some
actions. This study tackles a series of challenges for introducing such a
human-in-the-loop RL scheme. The first contribution of this work is our
experiments with a precisely modeled human observer: binary, delay,
stochasticity, unsustainability, and natural reaction. We also propose an RL
method called DQN-TAMER, which efficiently uses both human feedback and distant
rewards. We find that DQN-TAMER agents outperform their baselines in Maze and
Taxi simulated environments. Furthermore, we demonstrate a real-world
human-in-the-loop RL application where a camera automatically recognizes a
user's facial expressions as feedback to the agent while the agent explores a
maze."
365,"Brain-Computer interfaces (BCI) are widely used in reading brain signals and
converting them into real-world motion. However, the signals produced from the
BCI are noisy and hard to analyze. This paper looks specifically towards
combining the BCI's latest technology with ultrasonic sensors to provide a
hands-free wheelchair that can efficiently navigate through crowded
environments. This combination provides safety and obstacle avoidance features
necessary for the BCI Navigation system to gain more confidence and operate the
wheelchair at a relatively higher velocity. A population of six human subjects
tested the BCI-controller and obstacle avoidance features. Subjects were able
to mentally control the destination of the wheelchair, by moving the target
from the starting position to a predefined position, in an average of 287.12
seconds and a standard deviation of 48.63 seconds after 10 minutes of training.
The wheelchair successfully avoided all obstacles placed by the subjects during
the test."
366,"The smart textile and wearables sector is looking towards advancing
technologies to meet both industry, consumer and new emerging innovative
textile application demands, within a fast paced textile industry. In parallel
inspiration based on the biological neural workings of the human brain is
driving the next generation of artificial intelligence. Artificial intelligence
inspired hardware (neuromorphic computing) and software modules mimicking the
processing capabilities and properties of neural networks and the human nervous
system are taking shape. The textile sector needs to actively look at such
emerging and new technologies taking inspiration from their workings and
processing methods in order to stimulate new and innovative embedded
intelligence advancements in the etextile world. This emerging next generation
of Artificial intelligence(AI) is rapidly gaining interest across varying
industries (textile, medical, automotive, aerospace, military). How such
properties can inspire and drive advancements within the etextiles sector needs
to be considered. This paper will provide an insight into current
nanotechnology and artificial intelligence advancements in the etextiles domain
before focusing specifically on the future vision and direction around the
potential application of neuromorphic computing and spiking neural network
inspired AI technologies within the textile sector. We investigate the core
architectural elements of artificial neural networks, neuromorphic computing
and how such neuroscience inspired technologies could impact and inspire change
and new research developments within the e-textile sector."
367,"Large Language Models (LLMs) are notorious for blending fact with fiction and
generating non-factual content, known as hallucinations. To address this
challenge, we propose an interactive system that helps users gain insight into
the reliability of the generated text. Our approach is based on the idea that
the self-consistency of multiple samples generated by the same LLM relates to
its confidence in individual claims in the generated texts. Using this idea, we
design RELIC, an interactive system that enables users to investigate and
verify semantic-level variations in multiple long-form responses. This allows
users to recognize potentially inaccurate information in the generated text and
make necessary corrections. From a user study with ten participants, we
demonstrate that our approach helps users better verify the reliability of the
generated text. We further summarize the design implications and lessons
learned from this research for future studies of reliable human-LLM
interactions."
368,"This text analyses the papers accepted for the workshop ""Reuse of designs: an
interdisciplinary cognitive approach"". Several dimensions and questions
considered as important (by the authors and/or by us) are addressed: What about
the ""interdisciplinary cognitive"" character of the approaches adopted by the
authors? Is design indeed a domain where the use of CBR is particularly
suitable? Are there important distinctions between CBR and other approaches?
Which types of knowledge -other than cases- is being, or might be, used in CBR
systems? With respect to cases: are there different ""types"" of case and
different types of case use? which formats are adopted for their
representation? do cases have ""components""? how are cases organised in the case
memory? Concerning their retrieval: which types of index are used? on which
types of relation is retrieval based? how does one retrieve only a selected
number of cases, i.e., how does one retrieve only the ""best"" cases? which
processes and strategies are used, by the system and by its user? Finally, some
important aspects of CBR system development are shortly discussed: should CBR
systems be assistance or autonomous systems? how can case knowledge be
""acquired""? what about the empirical evaluation of CBR systems? The conclusion
points out some lacking points: not much attention is paid to the user, and few
papers have indeed adopted an interdisciplinary cognitive approach."
369,"SimDialog is a visual editor for dialog in computer games. This paper
presents the design of SimDialog, illustrating how script writers and
non-programmers can easily create dialog for video games with complex branching
structures and dynamic response characteristics. The system creates dialog as a
directed graph. This allows for play using the dialog with a state-based cause
and effect system that controls selection of non-player character responses and
can provide a basic scoring mechanism for games."
370,"Large language models (LLMs) have facilitated significant strides in
generating conversational agents, enabling seamless, contextually relevant
dialogues across diverse topics. However, the existing LLM-driven
conversational agents have fixed personalities and functionalities, limiting
their adaptability to individual user needs. Creating personalized agent
personas with distinct expertise or traits can address this issue. Nonetheless,
we lack knowledge of how people customize and interact with agent personas. In
this research, we investigated how users customize agent personas and their
impact on interaction quality, diversity, and dynamics. To this end, we
developed CloChat, an interface supporting easy and accurate customization of
agent personas in LLMs. We conducted a study comparing how participants
interact with CloChat and ChatGPT. The results indicate that participants
formed emotional bonds with the customized agents, engaged in more dynamic
dialogues, and showed interest in sustaining interactions. These findings
contribute to design implications for future systems with conversational agents
using LLMs."
371,"We present CharacterChat, a concept and chatbot to support writers in
creating fictional characters. Concretely, writers progressively turn the bot
into their imagined character through conversation. We iteratively developed
CharacterChat in a user-centred approach, starting with a survey on character
creation with writers (N=30), followed by two qualitative user studies (N=7 and
N=8). Our prototype combines two modes: (1) Guided prompts help writers define
character attributes (e.g. User: ""Your name is Jane.""), including suggestions
for attributes (e.g. Bot: ""What is my main motivation?"") and values, realised
as a rule-based system with a concept network. (2) Open conversation with the
chatbot helps writers explore their character and get inspiration, realised
with a language model that takes into account the defined character attributes.
Our user studies reveal benefits particularly for early stages of character
creation, and challenges due to limited conversational capabilities. We
conclude with lessons learned and ideas for future work."
372,"One of the most crucial issues in data mining is to model human behaviour in
order to provide personalisation, adaptation and recommendation. This usually
involves implicit or explicit knowledge, either by observing user interactions,
or by asking users directly. But these sources of information are always
subject to the volatility of human decisions, making utilised data uncertain to
a particular extent. In this contribution, we elaborate on the impact of this
human uncertainty when it comes to comparative assessments of different data
mining approaches. In particular, we reveal two problems: (1) biasing effects
on various metrics of model-based prediction and (2) the propagation of
uncertainty and its thus induced error probabilities for algorithm rankings.
For this purpose, we introduce a probabilistic view and prove the existence of
those problems mathematically, as well as provide possible solution strategies.
We exemplify our theory mainly in the context of recommender systems along with
the metric RMSE as a prominent example of precision quality measures."
373,"Model explanations such as saliency maps can improve user trust in AI by
highlighting important features for a prediction. However, these become
distorted and misleading when explaining predictions of images that are subject
to systematic error (bias). Furthermore, the distortions persist despite model
fine-tuning on images biased by different factors (blur, color temperature,
day/night). We present Debiased-CAM to recover explanation faithfulness across
various bias types and levels by training a multi-input, multi-task model with
auxiliary tasks for explanation and bias level predictions. In simulation
studies, the approach not only enhanced prediction accuracy, but also generated
highly faithful explanations about these predictions as if the images were
unbiased. In user studies, debiased explanations improved user task
performance, perceived truthfulness and perceived helpfulness. Debiased
training can provide a versatile platform for robust performance and
explanation faithfulness for a wide range of applications with data biases."
374,"There is increasing interest in the adoption of LLMs in HCI research.
However, LLMs may often be regarded as a panacea because of their powerful
capabilities with an accompanying oversight on whether they are suitable for
their intended tasks. We contend that LLMs should be adopted in a critical
manner following rigorous evaluation. Accordingly, we present the evaluation of
an LLM in identifying logical fallacies that will form part of a digital
misinformation intervention. By comparing to a labeled dataset, we found that
GPT-4 achieves an accuracy of 0.79, and for our intended use case that excludes
invalid or unidentified instances, an accuracy of 0.90. This gives us the
confidence to proceed with the application of the LLM while keeping in mind the
areas where it still falls short. The paper describes our evaluation approach,
results and reflections on the use of the LLM for our intended task."
375,"Recent advancements in artificial intelligence (AI) and its sub-branch
machine learning (ML) promise machines that go beyond the boundaries of
automation and behave autonomously. Applications of these machines in creative
practices such as art and design entail relationships between users and
machines that have been described as a form of collaboration or co-creation
between computational and human agents. This paper uses examples from art and
design to argue that this frame is incomplete as it fails to acknowledge the
socio-technical nature of AI systems, and the different human agencies involved
in their design, implementation, and operation. Situating applications of
AI-enabled tools in creative practices in a spectrum between automation and
autonomy, this paper distinguishes different kinds of human engagement elicited
by systems deemed automated or autonomous. Reviewing models of artistic
collaboration during the late 20th century, it suggests that collaboration is
at the core of these artistic practices. We build upon the growing literature
of machine learning and art to look for the human agencies inscribed in works
of computational creativity, and expand the co-creation frame to incorporate
emerging forms of human-human collaboration mediated through technical
artifacts such as algorithms and data."
376,"We present results from a set of experiments in this pilot study to
investigate the causal influence of user activity on various environmental
parameters monitored by occupant carried multi-purpose sensors. Hypotheses with
respect to each type of measurements are verified, including temperature,
humidity, and light level collected during eight typical activities: sitting in
lab / cubicle, indoor walking / running, resting after physical activity,
climbing stairs, taking elevators, and outdoor walking. Our main contribution
is the development of features for activity and location recognition based on
environmental measurements, which exploit location- and activity-specific
characteristics and capture the trends resulted from the underlying
physiological process. The features are statistically shown to have good
separability and are also information-rich. Fusing environmental sensing
together with acceleration is shown to achieve classification accuracy as high
as 99.13%. For building applications, this study motivates a sensor fusion
paradigm for learning individualized activity, location, and environmental
preferences for energy management and user comfort."
377,"Interaction is a fundamental part of using any computer system but it is
still an issue for people with special needs. In order to improve this
situation, this paper describes a new device-interaction model based on
adaptation rules for user models. The aim is the adaptation at the interaction
level, taking into account the interaction device features in order to improve
the usability through the user experience in the education sector. In the
evaluation process, several students from a special education center have
participated. These students have either a physical or sensory disability or
autism. The results are promising enough to consider that this model will be
able to help students with disabilities to interact with a computer system
which will inevitably provide tremendous benefits to their academic and
personal development."
378,"When seeking information not covered in patient-friendly documents, like
medical pamphlets, healthcare consumers may turn to the research literature.
Reading medical papers, however, can be a challenging experience. To improve
access to medical papers, we introduce a novel interactive interface-Paper
Plain-with four features powered by natural language processing: definitions of
unfamiliar terms, in-situ plain language section summaries, a collection of key
questions that guide readers to answering passages, and plain language
summaries of the answering passages. We evaluate Paper Plain, finding that
participants who use Paper Plain have an easier time reading and understanding
research papers without a loss in paper comprehension compared to those who use
a typical PDF reader. Altogether, the study results suggest that guiding
readers to relevant passages and providing plain language summaries, or
""gists,"" alongside the original paper content can make reading medical papers
easier and give readers more confidence to approach these papers."
379,"We propose a text editor to help users plan, structure and reflect on their
writing process. It provides continuously updated paragraph-wise summaries as
margin annotations, using automatic text summarization. Summary levels range
from full text, to selected (central) sentences, down to a collection of
keywords. To understand how users interact with this system during writing, we
conducted two user studies (N=4 and N=8) in which people wrote analytic essays
about a given topic and article. As a key finding, the summaries gave users an
external perspective on their writing and helped them to revise the content and
scope of their drafted paragraphs. People further used the tool to quickly gain
an overview of the text and developed strategies to integrate insights from the
automated summaries. More broadly, this work explores and highlights the value
of designing AI tools for writers, with Natural Language Processing (NLP)
capabilities that go beyond direct text generation and correction."
380,"Image generation using generative AI is rapidly becoming a major new source
of visual media, with billions of AI generated images created using diffusion
models such as Stable Diffusion and Midjourney over the last few years. In this
paper we collect and analyse over 3 million prompts and the images they
generate. Using natural language processing, topic analysis and visualisation
methods we aim to understand collectively how people are using text prompts,
the impact of these systems on artists, and more broadly on the visual cultures
they promote. Our study shows that prompting focuses largely on surface
aesthetics, reinforcing cultural norms, popular conventional representations
and imagery. We also find that many users focus on popular topics (such as
making colouring books, fantasy art, or Christmas cards), suggesting that the
dominant use for the systems analysed is recreational rather than artistic."
381,"NL2VIS (natural language to visualization) is a promising and recent research
area that involves interpreting natural language queries and translating them
into visualizations that accurately represent the underlying data. As we
navigate the era of big data, NL2VIS holds considerable application potential
since it greatly facilitates data exploration by non-expert users. Following
the increasingly widespread usage of generative AI in NL2VIS applications, in
this paper we present V-RECS, the first LLM-based Visual Recommender augmented
with explanations(E), captioning(C), and suggestions(S) for further data
exploration. V-RECS' visualization narratives facilitate both response
verification and data exploration by non-expert users. Furthermore, our
proposed solution mitigates computational, controllability, and cost issues
associated with using powerful LLMs by leveraging a methodology to effectively
fine-tune small models. To generate insightful visualization narratives, we use
Chain-of-Thoughts (CoT), a prompt engineering technique to help LLM identify
and generate the logical steps to produce a correct answer. Since CoT is
reported to perform poorly with small LLMs, we adopted a strategy in which a
large LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to
fine-tune a small model, Llama-2-7B, which plays the role of a Student.
Extensive experiments-based on a framework for the quantitative evaluation of
AI-based visualizations and on manual assessment by a group of
participants-show that V-RECS achieves performance scores comparable to GPT-4,
at a much lower cost. The efficacy of the V-RECS teacher-student paradigm is
also demonstrated by the fact that the un-tuned Llama fails to perform the task
in the vast majority of test cases. We release V-RECS for the visualization
community to assist visualization designers throughout the entire visualization
generation process."
382,"As we become increasingly entangled with digital technologies, the boundary
between human and machine is progressively blurring. Adopting a performative,
posthumanist perspective resolves this ambiguity by proposing that such
boundaries are not predetermined, rather they are enacted within a certain
material configuration. Using this approach, dubbed `Entanglement HCI', this
paper presents \emph{Message Ritual} -- a novel, integrated AI system that
encourages the re-framing of memory through machine generated poetics. Embodied
within a domestic table lamp, the system listens in on conversations occurring
within the home, drawing out key topics and phrases of the day and
reconstituting them through machine generated poetry, delivered to household
members via SMS upon waking each morning. Participants across four households
were asked to live with the lamp over a two week period. We present a
diffractive analysis exploring how the lamp \emph{becomes with} participants
and discuss the implications of this method for future HCI research."
383,"HCI researchers' and practitioners' awareness of intersectionality has been
expanding, producing knowledge, recommendations, and prototypes for supporting
intersectional populations. However, doing intersectional HCI work is uniquely
expensive: it leads to a combinatorial explosion of empirical work (expense 1),
and little of the work on one intersectional population can be leveraged to
serve another (expense 2). In this paper, we explain how representations
employed by certain analytical design methods correspond to type abstractions,
and use that correspondence to identify a (de)compositional model in which a
population's diverse identity properties can be joined and split. We formally
prove the model's correctness, and show how it enables HCI designers to harness
existing analytical HCI methods for use on new intersectional populations of
interest. We illustrate through four design use-cases, how the model can reduce
the amount of expense 1 and enable designers to leverage prior work to new
intersectional populations, addressing expense 2."
384,"We present an algorithm for effectively generating binary sequences which
would be rated by people as highly likely to have been generated by a random
process, such as flipping a fair coin."
385,"Steady-State Visual Evoked Potential (SSVEP) spellers are a promising
communication tool for individuals with disabilities. This Brain-Computer
Interface utilizes scalp potential data from (electroencephalography) EEG
electrodes on a subject's head to decode specific letters or arbitrary targets
the subject is looking at on a screen. However, deep neural networks for SSVEP
spellers often suffer from low accuracy and poor generalizability to unseen
subjects, largely due to the high variability in EEG data. In this study, we
propose a hybrid approach combining data augmentation and language modeling to
enhance the performance of SSVEP spellers. Using the Benchmark dataset from
Tsinghua University, we explore various data augmentation techniques, including
frequency masking, time masking, and noise injection, to improve the robustness
of deep learning models. Additionally, we integrate a language model (CharRNN)
with EEGNet to incorporate linguistic context, significantly enhancing
word-level decoding accuracy. Our results demonstrate accuracy improvements of
up to 2.9 percent over the baseline, with time masking and language modeling
showing the most promise. This work paves the way for more accurate and
generalizable SSVEP speller systems, offering improved communication solutions
for individuals with disabilities."
386,"Deep learning is one of the fastest growing technologies in computer science
with a plethora of applications. But this unprecedented growth has so far been
limited to the consumption of deep learning experts. The primary challenge
being a steep learning curve for learning the programming libraries and the
lack of intuitive systems enabling non-experts to consume deep learning.
Towards this goal, we study the effectiveness of a no-code paradigm for
designing deep learning models. Particularly, a visual drag-and-drop interface
is found more efficient when compared with the traditional programming and
alternative visual programming paradigms. We conduct user studies of different
expertise levels to measure the entry level barrier and the developer load
across different programming paradigms. We obtain a System Usability Scale
(SUS) of 90 and a NASA Task Load index (TLX) score of 21 for the proposed
visual programming compared to 68 and 52, respectively, for the traditional
programming methods."
387,"Distance teaching has become popular these years because of the COVID-19
epidemic. However, both students and teachers face several challenges in
distance teaching, like being easy to distract. We proposed Focus+, a system
designed to detect learners' status with the latest AI technology from their
web camera to solve such challenges. By doing so, teachers can know students'
status, and students can regulate their learning experience. In this research,
we will discuss the expected model's design for training and evaluating the AI
detection model of Focus+."
388,"News archives are an invaluable primary source for placing current events in
historical context. But current search engine tools do a poor job at uncovering
broad themes and narratives across documents. We present Rookie: a practical
software system which uses natural language processing (NLP) to help readers,
reporters and editors uncover broad stories in news archives. Unlike prior
work, Rookie's design emerged from 18 months of iterative development in
consultation with editors and computational journalists. This process lead to a
dramatically different approach from previous academic systems with similar
goals. Our efforts offer a generalizable case study for others building
real-world journalism software using NLP."
389,"As LLMs make their way into many aspects of our lives, one place that
warrants increased scrutiny with LLM usage is scientific research. Using LLMs
for generating or analyzing data for research purposes is gaining popularity.
But when such application is marred with ad-hoc decisions and engineering
solutions, we need to be concerned about how it may affect that research, its
findings, or any future works based on that research. We need a more scientific
approach to using LLMs in our research. While there are several active efforts
to support more systematic construction of prompts, they are often focused more
on achieving desirable outcomes rather than producing replicable and
generalizable knowledge with sufficient transparency, objectivity, or rigor.
This article presents a new methodology inspired by codebook construction
through qualitative methods to address that. Using humans in the loop and a
multi-phase verification processes, this methodology lays a foundation for more
systematic, objective, and trustworthy way of applying LLMs for analyzing data.
Specifically, we show how a set of researchers can work through a rigorous
process of labeling, deliberating, and documenting to remove subjectivity and
bring transparency and replicability to prompt generation process. A set of
experiments are presented to show how this methodology can be put in practice."
390,"The paper presents a novel model-based method for intelligent tutoring, with
particular emphasis on the problem of selecting teaching interventions in
interaction with humans. Whereas previous work has focused on either
personalization of teaching or optimization of teaching intervention sequences,
the proposed individualized model-based planning approach represents
convergence of these two lines of research. Model-based planning picks the best
interventions via interactive learning of a user memory model's parameters. The
approach is novel in its use of a cognitive model that can account for several
key individual- and material-specific characteristics related to
recall/forgetting, along with a planning technique that considers users'
practice schedules. Taking a rule-based approach as a baseline, the authors
evaluated the method's benefits in a controlled study of artificial teaching in
second-language vocabulary learning (N=53)."
391,"Large-scale labeled dataset is the indispensable fuel that ignites the AI
revolution as we see today. Most such datasets are constructed using
crowdsourcing services such as Amazon Mechanical Turk which provides noisy
labels from non-experts at a fair price. The sheer size of such datasets
mandates that it is only feasible to collect a few labels per data point. We
formulate the problem of test-time label aggregation as a statistical
estimation problem of inferring the expected voting score. By imitating workers
with supervised learners and using them in a doubly robust estimation
framework, we prove that the variance of estimation can be substantially
reduced, even if the learner is a poor approximation. Synthetic and real-world
experiments show that by combining the doubly robust approach with adaptive
worker/item selection rules, we often need much lower label cost to achieve
nearly the same accuracy as in the ideal world where all workers label all data
points."
392,"We propose a statistical procedure to characterize and extract features from
a waveform that can be applied as a pre-processing signal stage in a pattern
recognition task using Artificial Neural Networks. Such a procedure is based on
measuring a 30-parameters set of moments and cumulants from the waveform, its
derivative, and its integral. The technique is presented as an extension of the
Statistical Signal Characterization method existing in the literature.
  As a testing methodology, we used the procedure to distinguish a pulse-like
signal from different versions of itself with frequency spectrum alterations or
deformations. The recognition task was performed by single feed-forward
back-propagation networks trained for the case Sinc-, Gaussian-, and
Chirp-pulse waveform. Because of the success obtained in these examples, we can
conclude that the proposed extended statistical signal characterization method
is an effective tool for pattern-recognition applications. In particular, we
can use it as a fast pre-processing stage in embedded systems with limited
memory or computational capability."
393,"Channel charting (CC) has been proposed recently to enable logical
positioning of user equipments (UEs) in the neighborhood of a multi-antenna
base-station solely from channel-state information (CSI). CC relies on
dimensionality reduction of high-dimensional CSI features in order to construct
a channel chart that captures spatial and radio geometries so that UEs close in
space are close in the channel chart. In this paper, we demonstrate that
autoencoder (AE)-based CC can be augmented with side information that is
obtained during the CSI acquisition process. More specifically, we propose to
include pairwise representation constraints into AEs with the goal of improving
the quality of the learned channel charts. We show that such
representation-constrained AEs recover the global geometry of the learned
channel charts, which enables CC to perform approximate positioning without
global navigation satellite systems or supervised learning methods that rely on
extensive and expensive measurement campaigns."
394,"The notion of graph filters can be used to define generative models for graph
data. In fact, the data obtained from many examples of network dynamics may be
viewed as the output of a graph filter. With this interpretation, classical
signal processing tools such as frequency analysis have been successfully
applied with analogous interpretation to graph data, generating new insights
for data science. What follows is a user guide on a specific class of graph
data, where the generating graph filters are low-pass, i.e., the filter
attenuates contents in the higher graph frequencies while retaining contents in
the lower frequencies. Our choice is motivated by the prevalence of low-pass
models in application domains such as social networks, financial markets, and
power systems. We illustrate how to leverage properties of low-pass graph
filters to learn the graph topology or identify its community structure;
efficiently represent graph data through sampling, recover missing
measurements, and de-noise graph data; the low-pass property is also used as
the baseline to detect anomalies."
395,"Electroencephalograms (EEG) are often contaminated by artifacts which make
interpreting them more challenging for clinicians. Hence, automated artifact
recognition systems have the potential to aid the clinical workflow. In this
abstract, we share the first results on applying various machine learning
algorithms to the recently released world's largest open-source artifact
recognition dataset. We envision that these results will serve as a benchmark
for researchers who might work with this dataset in future."
396,"In the present study, six meta-heuristic schemes are hybridized with
artificial neural network (ANN), adaptive neuro-fuzzy interface system (ANFIS),
and support vector machine (SVM), to predict monthly groundwater level (GWL),
evaluate uncertainty analysis of predictions and spatial variation analysis.
The six schemes, including grasshopper optimization algorithm (GOA), cat swarm
optimization (CSO), weed algorithm (WA), genetic algorithm (GA), krill
algorithm (KA), and particle swarm optimization (PSO), were used to hybridize
for improving the performance of ANN, SVM, and ANFIS models. Groundwater level
(GWL) data of Ardebil plain (Iran) for a period of 144 months were selected to
evaluate the hybrid models. The pre-processing technique of principal component
analysis (PCA) was applied to reduce input combinations from monthly time
series up to 12-month prediction intervals. The results showed that the
ANFIS-GOA was superior to the other hybrid models for predicting GWL in the
first piezometer and third piezometer in the testing stage. The performance of
hybrid models with optimization algorithms was far better than that of
classical ANN, ANFIS, and SVM models without hybridization. The percent of
improvements in the ANFIS-GOA versus standalone ANFIS in piezometer 10 were
14.4%, 3%, 17.8%, and 181% for RMSE, MAE, NSE, and PBIAS in the training stage
and 40.7%, 55%, 25%, and 132% in testing stage, respectively. The improvements
for piezometer 6 in train step were 15%, 4%, 13%, and 208% and in the test step
were 33%, 44.6%, 16.3%, and 173%, respectively, that clearly confirm the
superiority of developed hybridization schemes in GWL modeling. Uncertainty
analysis showed that ANFIS-GOA and SVM had, respectively, the best and worst
performances among other models. In general, GOA enhanced the accuracy of the
ANFIS, ANN, and SVM models."
397,"In this paper we propose a one-dimensional convolutional neural network
(CNN)-based state of charge estimation algorithm for electric vehicles. The CNN
is trained using two publicly available battery datasets. The influence of
different types of noises on the estimation capabilities of the CNN model has
been studied. Moreover, a transfer learning mechanism is proposed in order to
make the developed algorithm generalize better and estimate with an acceptable
accuracy when a battery with different chemical characteristics than the one
used for training the model, is used. It has been observed that using transfer
learning, the model can learn sufficiently well with significantly less amount
of battery data. The proposed method fares well in terms of estimation
accuracy, learning speed and generalization capability."
398,"Emotion recognition based on EEG (electroencephalography) has been widely
used in human-computer interaction, distance education and health care.
However, the conventional methods ignore the adjacent and symmetrical
characteristics of EEG signals, which also contain salient information related
to emotion. In this paper, a spatial folding ensemble network (SFE-Net) is
presented for EEG feature extraction and emotion recognition. Firstly, for the
undetected area between EEG electrodes, an improved Bicubic-EEG interpolation
algorithm is developed for EEG channels information completion, which allows us
to extract a wider range of adjacent space features. Then, motivated by the
spatial symmetric mechanism of human brain, we fold the input EEG channels data
with five different symmetrical strategies, which enable the proposed network
to extract the information of space features of EEG signals more effectively.
Finally, a 3DCNN-based spatial, temporal extraction, and a multi-voting
strategy of ensemble learning are integrated to model a new neural network.
With this network, the spatial features of different symmetric folding signals
can be extracted simultaneously, which greatly improves the robustness and
accuracy of emotion recognition. The experimental results on DEAP and SEED
datasets show that the proposed algorithm has comparable performance in terms
of recognition accuracy."
399,"With millimeter wave wireless communications, the resulting radiation
reflects on most visible objects, creating rich multipath environments, namely
in urban scenarios. The radiation captured by a listening device is thus shaped
by the obstacles encountered, which carry latent information regarding their
relative positions. In this paper, a system to convert the received millimeter
wave radiation into the device's position is proposed, making use of the
aforementioned hidden information. Using deep learning techniques and a
pre-established codebook of beamforming patterns transmitted by a base station,
the simulations show that average estimation errors below 10 meters are
achievable in realistic outdoors scenarios that contain mostly
non-line-of-sight positions, paving the way for new positioning systems."
400,"For Device-to-device (D2D) communication of Internet-of-Things (IoT) enabled
5G system, there is a limit to allocating resources considering a complicated
interference between different links in a centralized manner. If D2D link is
controlled by an enhanced node base station (eNB), and thus, remains a burden
on the eNB and it causes delayed latency. This paper proposes a fully
autonomous power allocation method for IoT-D2D communication underlaying
cellular networks using deep learning. In the proposed scheme, an IoT-D2D
transmitter decides the transmit power independently from an eNB and other
IoT-D2D devices. In addition, the power set can be nearly optimized by deep
learning with distributed manner to achieve higher cell throughput. We present
a distributed deep learning architecture in which the devices are trained as a
group but operate independently. The deep learning can attain near optimal cell
throughput while suppressing interference to eNB."
401,"Deep learning methods achieve great success in many areas due to their
powerful feature extraction capabilities and end-to-end training mechanism, and
recently they are also introduced for radio signal modulation classification.
In this paper, we propose a novel deep learning framework called SigNet, where
a signal-to-matrix (S2M) operator is adopted to convert the original signal
into a square matrix first and is co-trained with a follow-up CNN architecture
for classification. This model is further accelerated by integrating 1D
convolution operators, leading to the upgraded model SigNet2.0. The simulations
on two signal datasets show that both SigNet and SigNet2.0 outperform a number
of well-known baselines. More interestingly, our proposed models behave
extremely well in small-sample learning when only a small training dataset is
provided. They can achieve a relatively high accuracy even when 1\% training
data are kept, while other baseline models may lose their effectiveness much
more quickly as the datasets get smaller. Such result suggests that
SigNet/SigNet2.0 could be extremely useful in the situations where labeled
signal data are difficult to obtain. The visualization of the output features
of our models demonstrates that our model can well divide different modulation
types of signals in the feature hyper-space."
402,"Most of the Brain-Computer Interface (BCI) publications, which propose
artificial neural networks for Motor Imagery (MI) Electroencephalography (EEG)
signal classification, are presented using one of the BCI Competition datasets.
However, these databases contain MI EEG data from less than or equal to 10
subjects . In addition, these algorithms usually include only bandpass
filtering to reduce noise and increase signal quality. In this article, we
compared 5 well-known neural networks (Shallow ConvNet, Deep ConvNet, EEGNet,
EEGNet Fusion, MI-EEGNet) using open-access databases with many subjects next
to the BCI Competition 4 2a dataset to acquire statistically significant
results. We removed artifacts from the EEG using the FASTER algorithm as a
signal processing step. Moreover, we investigated whether transfer learning can
further improve the classification results on artifact filtered data. We aimed
to rank the neural networks; therefore, next to the classification accuracy, we
introduced two additional metrics: the accuracy improvement from chance level
and the effect of transfer learning. The former can be used with different
class-numbered databases, while the latter can highlight neural networks with
sufficient generalization abilities. Our metrics showed that the researchers
should not avoid Shallow ConvNet and Deep ConvNet because they can perform
better than the later published ones from the EEGNet family."
403,"The objective of this paper is to provide a temporal dynamic model for
resting state functional Magnetic Resonance Imaging (fMRI) trajectory to
predict future brain images based on the given sequence. To this end, we came
up with the model that takes advantage of representation learning and Neural
Ordinary Differential Equation (Neural ODE) to compress the fMRI image data
into latent representation and learn to predict the trajectory following
differential equation. Latent space was analyzed by Gaussian Mixture Model. The
learned fMRI trajectory embedding can be used to explain the variance of the
trajectory and predict human traits for each subject. This method achieves
average 0.5 spatial correlation for the whole predicted trajectory, and provide
trained ODE parameter for further analysis."
404,"This paper explores the potential of conversion-based neuromorphic algorithms
for highly accurate and energy-efficient single-snapshot multidimensional
harmonic retrieval (MHR). By casting the MHR problem as a sparse recovery
problem, we devise the currently proposed, deep-unrolling-based Structured
Learned Iterative Shrinkage and Thresholding (S-LISTA) algorithm to solve it
efficiently using complex-valued convolutional neural networks with
complex-valued activations, which are trained using a supervised regression
objective. Afterward, a novel method for converting the complex-valued
convolutional layers and activations into spiking neural networks (SNNs) is
developed. At the heart of this method lies the recently proposed Few Spikes
(FS) conversion, which is extended by modifying the neuron model's parameters
and internal dynamics to account for the inherent coupling between real and
imaginary parts in complex-valued computations. Finally, the converted SNNs are
mapped onto the SpiNNaker2 neuromorphic board, and a comparison in terms of
estimation accuracy and power efficiency between the original CNNs deployed on
an NVIDIA Jetson Xavier and the SNNs is being conducted. The measurement
results show that the converted SNNs achieve almost five-fold power efficiency
at moderate performance loss compared to the original CNNs."
405,"Ubiquitous use of lithium-ion batteries across multiple industries presents
an opportunity to explore cost saving initiatives as the price to performance
ratio continually decreases in a competitive environment. Manufacturers using
lithium-ion batteries ranging in applications from mobile phones to electric
vehicles need to know how long batteries will last for a given service life. To
understand this, expensive testing is required.
  This paper utilizes the data and methods implemented by Kristen A. Severson,
et al, to explore the methodologies that the research team used and presents
another method to compare predicted results vs. actual test data for battery
capacity fade. The fundamental effort is to find out if machine learning
techniques may be trained to use early life cycle data in order to accurately
predict battery capacity over the battery life cycle. Results show comparison
of methods between Gaussian Process Regression (GPR) and Elastic Net Regression
(ENR) and highlight key data features used from the extensive dataset found in
the work of Severson, et al."
406,"Social distancing and temperature screening have been widely employed to
counteract the COVID-19 pandemic, sparking great interest from academia,
industry and public administrations worldwide. While most solutions have dealt
with these aspects separately, their combination would greatly benefit the
continuous monitoring of public spaces and help trigger effective
countermeasures. This work presents milliTRACE-IR, a joint mmWave radar and
infrared imaging sensing system performing unobtrusive and privacy preserving
human body temperature screening and contact tracing in indoor spaces.
milliTRACE-IR combines, via a robust sensor fusion approach, mmWave radars and
infrared thermal cameras. It achieves fully automated measurement of distancing
and body temperature, by jointly tracking the subjects's faces in the thermal
camera image plane and the human motion in the radar reference system.
Moreover, milliTRACE-IR performs contact tracing: a person with high body
temperature is reliably detected by the thermal camera sensor and subsequently
traced across a large indoor area in a non-invasive way by the radars. When
entering a new room, a subject is re-identified among several other individuals
by computing gait-related features from the radar reflections through a deep
neural network and using a weighted extreme learning machine as the final
re-identification tool. Experimental results, obtained from a real
implementation of milliTRACE-IR, demonstrate decimeter-level accuracy in
distance/trajectory estimation, inter-personal distance estimation (effective
for subjects getting as close as 0.2 m), and accurate temperature monitoring
(max. errors of 0.5{\deg}C). Furthermore, milliTRACE-IR provides contact
tracing through highly accurate (95%) person re-identification, in less than 20
seconds."
407,"In this paper, a deep learning approach is presented for direction of arrival
estimation using automotive-grade ultrasonic sensors which are used for driving
assistance systems such as automatic parking. A study and implementation of the
state of the art deterministic direction of arrival estimation algorithms is
used as a benchmark for the performance of the proposed approach. Analysis of
the performance of the proposed algorithms against the existing algorithms is
carried out over simulation data as well as data from a measurement campaign
done using automotive-grade ultrasonic sensors. Both sets of results clearly
show the superiority of the proposed approach under realistic conditions such
as noise from the environment as well as eventual errors in measurements. It is
demonstrated as well how the proposed approach can overcome some of the known
limitations of the existing algorithms such as precision dilution of
triangulation and aliasing."
408,"Overhead lines are generally used for electrical energy transmission. Also,
XLPE underground cable lines are generally used in the city center and the
crowded areas to provide electrical safety, so high voltage underground cable
lines are used together with overhead line in the transmission lines, and these
lines are called as the mixed lines. The distance protection relays are used to
determine the impedance based fault location according to the current and
voltage magnitudes in the transmission lines. However, the fault location
cannot be correctly detected in mixed transmission lines due to different
characteristic impedance per unit length because the characteristic impedance
of high voltage cable line is significantly different from overhead line. Thus,
determinations of the fault section and location with the distance protection
relays are difficult in the mixed transmission lines. In this study, 154 kV
overhead transmission line and underground cable line are examined as the mixed
transmission line for the distance protection relays. Phase to ground faults
are created in the mixed transmission line, and overhead line section and
underground cable section are simulated by using PSCAD. The short circuit fault
images are generated in the distance protection relay for the overhead
transmission line and underground cable transmission line faults. The images
include the RX impedance diagram of the fault, and the RX impedance diagram
have been detected by applying image processing steps. The regression methods
are used for prediction of the fault location, and the results of image
processing are used as the input parameters for the training process of the
regression methods. The results of regression methods are compared to select
the most suitable method at the end of this study for forecasting of the fault
location in transmission lines."
409,"The high demand for data rate in the next generation of wireless
communication could be ensured by Non-Orthogonal Multiple Access (NOMA)
approach in the millimetre-wave (mmW) frequency band. Joint power allocation
and beamforming of mmW-NOMA systems is mandatory which could be met by
optimization approaches. To this end, we have exploited Deep Reinforcement
Learning (DRL) approach due to policy generation leading to an optimized
sum-rate of users. Actor-critic phenomena are utilized to measure the immediate
reward and provide the new action to maximize the overall Q-value of the
network. The immediate reward has been defined based on the summation of the
rate of two users regarding the minimum guaranteed rate for each user and the
sum of consumed power as the constraints. The simulation results represent the
superiority of the proposed approach rather than the Time-Division Multiple
Access (TDMA) and another NOMA optimized strategy in terms of sum-rate of
users."
410,"Robustly determining the optimal number of clusters in a data set is an
essential factor in a wide range of applications. Cluster enumeration becomes
challenging when the true underlying structure in the observed data is
corrupted by heavy-tailed noise and outliers. Recently, Bayesian cluster
enumeration criteria have been derived by formulating cluster enumeration as
maximization of the posterior probability of candidate models. This article
generalizes robust Bayesian cluster enumeration so that it can be used with any
arbitrary Real Elliptically Symmetric (RES) distributed mixture model. Our
framework also covers the case of M-estimators that allow for mixture models,
which are decoupled from a specific probability distribution. Examples of
Huber's and Tukey's M-estimators are discussed. We derive a robust criterion
for data sets with finite sample size, and also provide an asymptotic
approximation to reduce the computational cost at large sample sizes. The
algorithms are applied to simulated and real-world data sets, including
radar-based person identification, and show a significant robustness
improvement in comparison to existing methods."
411,"This study suggests a new prediction model for chaotic time series inspired
by the brain emotional learning of mammals. We describe the structure and
function of this model, which is referred to as BELPM (Brain Emotional
Learning-Based Prediction Model). Structurally, the model mimics the connection
between the regions of the limbic system, and functionally it uses weighted k
nearest neighbors to imitate the roles of those regions. The learning algorithm
of BELPM is defined using steepest descent (SD) and the least square estimator
(LSE). Two benchmark chaotic time series, Lorenz and Henon, have been used to
evaluate the performance of BELPM. The obtained results have been compared with
those of other prediction methods. The results show that BELPM has the
capability to achieve a reasonable accuracy for long-term prediction of chaotic
time series, using a limited amount of training data and a reasonably low
computational time."
412,"Sensor-based human activity recognition is important in daily scenarios such
as smart healthcare and homes due to its non-intrusive privacy and low cost
advantages, but the problem of out-of-domain generalization caused by
differences in focusing individuals and operating environments can lead to
significant accuracy degradation on cross-person behavior recognition due to
the inconsistent distributions of training and test data. To address the above
problems, this paper proposes a new method, Multi-channel Time Series
Decomposition Network (MTSDNet). Firstly, MTSDNet decomposes the original
signal into a combination of multiple polynomials and trigonometric functions
by the trainable parameterized temporal decomposition to learn the low-rank
representation of the original signal for improving the extraterritorial
generalization ability of the model. Then, the different components obtained by
the decomposition are classified layer by layer and the layer attention is used
to aggregate components to obtain the final classification result. Extensive
evaluation on DSADS, OPPORTUNITY, PAMAP2, UCIHAR and UniMib public datasets
shows the advantages in predicting accuracy and stability of our method
compared with other competing strategies, including the state-of-the-art ones.
And the visualization is conducted to reveal MTSDNet's interpretability and
layer-by-layer characteristics."
413,"Many applications require accurate indoor localization. Fingerprint-based
localization methods propose a solution to this problem, but rely on a radio
map that is effort-intensive to acquire. We automate the radio map acquisition
phase using a software-defined radio (SDR) and a wheeled robot. Furthermore, we
open-source a radio map acquired with our automated tool for a 3GPP Long-Term
Evolution (LTE) wireless link. To the best of our knowledge, this is the first
publicly available radio map containing channel state information (CSI).
Finally, we describe first localization experiments on this radio map using a
convolutional neural network to regress for location coordinates."
414,"Air Quality Multi-sensors Systems (AQMS) are IoT devices based on low cost
chemical microsensors array that recently have showed capable to provide
relatively accurate air pollutant quantitative estimations. Their availability
permits to deploy pervasive Air Quality Monitoring (AQM) networks that will
solve the geographical sparseness issue that affect the current network of AQ
Regulatory Monitoring Systems (AQRMS). Unfortunately their accuracy have shown
limited in long term field deployments due to negative influence of several
technological issues including sensors poisoning or ageing, non target gas
interference, lack of fabrication repeatability, etc. Seasonal changes in
probability distribution of priors, observables and hidden context variables
(i.e. non observable interferents) challenge field data driven calibration
models which short to mid term performances recently rose to the attention of
Urban authorithies and monitoring agencies. In this work, we address this non
stationary framework with adaptive learning strategies in order to prolong the
validity of multisensors calibration models enabling continuous learning.
Relevant parameters influence in different network and note-to-node
recalibration scenario is analyzed. Results are hence useful for pervasive
deployment aimed to permanent high resolution AQ mapping in urban scenarios as
well as for the use of AQMS as AQRMS backup systems providing data when AQRMS
data are unavailable due to faults or scheduled mainteinance."
415,"CANDECOMP/PARAFAC (CP) decomposition is the mostly used model to formulate
the received tensor signal in a massive MIMO system, as the receiver generally
sums the components from different paths or users. To achieve accurate and
low-latency channel estimation, good and fast CP decomposition (CPD) algorithms
are desired. The CP alternating least squares (CPALS) is the workhorse
algorithm for calculating the CPD. However, its performance depends on the
initializations, and good starting values can lead to more efficient solutions.
Existing initialization strategies are decoupled from the CPALS and are not
necessarily favorable for solving the CPD. This paper proposes a
deep-learning-aided CPALS (DL-CPALS) method that uses a deep neural network
(DNN) to generate favorable initializations. The proposed DL-CPALS integrates
the DNN and CPALS to a model-based deep learning paradigm, where it trains the
DNN to generate an initialization that facilitates fast and accurate CPD.
Moreover, benefiting from the CP low-rankness, the proposed method is trained
using noisy data and does not require paired clean data. The proposed DL-CPALS
is applied to millimeter wave MIMO-OFDM channel estimation. Experimental
results demonstrate the significant improvements of the proposed method in
terms of both speed and accuracy for CPD and channel estimation."
416,"Nonnegative matrix factorization (NMF) is a widely used linear dimensionality
reduction technique for nonnegative data. NMF requires that each data point is
approximated by a convex combination of basis elements. Archetypal analysis
(AA), also referred to as convex NMF, is a well-known NMF variant imposing that
the basis elements are themselves convex combinations of the data points. AA
has the advantage to be more interpretable than NMF because the basis elements
are directly constructed from the data points. However, it usually suffers from
a high data fitting error because the basis elements are constrained to be
contained in the convex cone of the data points. In this letter, we introduce
near-convex archetypal analysis (NCAA) which combines the advantages of both AA
and NMF. As for AA, the basis vectors are required to be linear combinations of
the data points and hence are easily interpretable. As for NMF, the additional
flexibility in choosing the basis elements allows NCAA to have a low data
fitting error. We show that NCAA compares favorably with a state-of-the-art
minimum-volume NMF method on synthetic datasets and on a real-world
hyperspectral image."
417,"Objective: A novel ECG classification algorithm is proposed for continuous
cardiac monitoring on wearable devices with limited processing capacity.
Methods: The proposed solution employs a novel architecture consisting of
wavelet transform and multiple LSTM recurrent neural networks. Results:
Experimental evaluations show superior ECG classification performance compared
to previous works. Measurements on different hardware platforms show the
proposed algorithm meets timing requirements for continuous and real-time
execution on wearable devices. Conclusion: In contrast to many
compute-intensive deep-learning based approaches, the proposed algorithm is
lightweight, and therefore, brings continuous monitoring with accurate
LSTM-based ECG classification to wearable devices. Significance: The proposed
algorithm is both accurate and lightweight. The source code is available online
[1]."
418,"Details of Monarch butterfly migration from the U.S. to Mexico remain a
mystery due to lack of a proper localization technology to accurately localize
and track butterfly migration. In this paper, we propose a deep learning based
butterfly localization algorithm that can estimate a butterfly's daily location
by analyzing a light and temperature sensor data log continuously obtained from
an ultra-low power, mm-scale sensor attached to the butterfly. To train and
test the proposed neural network based multi-sensor fusion localization
algorithm, we collected over 1500 days of real world sensor measurement data
with 82 volunteers all over the U.S. The proposed algorithm exhibits a mean
absolute error of <1.5 degree in latitude and <0.5 degree in longitude Earth
coordinate, satisfying our target goal for the Monarch butterfly migration
study."
419,"Estimating $T_2$ relaxation time distributions from multi-echo $T_2$-weighted
MRI ($T_2W$) data can provide valuable biomarkers for assessing inflammation,
demyelination, edema, and cartilage composition in various pathologies,
including neurodegenerative disorders, osteoarthritis, and tumors. Deep neural
network (DNN) based methods have been proposed to address the complex inverse
problem of estimating $T_2$ distributions from MRI data, but they are not yet
robust enough for clinical data with low Signal-to-Noise ratio (SNR) and are
highly sensitive to distribution shifts such as variations in echo-times (TE)
used during acquisition. Consequently, their application is hindered in
clinical practice and large-scale multi-institutional trials with heterogeneous
acquisition protocols. We propose a physically-primed DNN approach, called
$P_2T_2$, that incorporates the signal decay forward model in addition to the
MRI signal into the DNN architecture to improve the accuracy and robustness of
$T_2$ distribution estimation. We evaluated our $P_2T_2$ model in comparison to
both DNN-based methods and classical methods for $T_2$ distribution estimation
using 1D and 2D numerical simulations along with clinical data. Our model
improved the baseline model's accuracy for low SNR levels ($SNR<80$) which are
common in the clinical setting. Further, our model achieved a $\sim$35\%
improvement in robustness against distribution shifts in the acquisition
process compared to previously proposed DNN models. Finally, Our $P_2T_2$ model
produces the most detailed Myelin-Water fraction maps compared to baseline
approaches when applied to real human MRI data. Our $P_2T_2$ model offers a
reliable and precise means of estimating $T_2$ distributions from MRI data and
shows promise for use in large-scale multi-institutional trials with
heterogeneous acquisition protocols."
420,"In current clinical practice, electroencephalograms (EEG) are reviewed and
analyzed by well-trained neurologists to provide supports for therapeutic
decisions. The way of manual reviewing is labor-intensive and error prone.
Automatic and accurate seizure/nonseizure classification methods are needed.
One major problem is that the EEG signals for seizure state and nonseizure
state exhibit considerable variations. In order to capture essential seizure
features, this paper integrates an emerging deep learning model, the
independently recurrent neural network (IndRNN), with a dense structure and an
attention mechanism to exploit temporal and spatial discriminating features and
overcome seizure variabilities. The dense structure is to ensure maximum
information flow between layers. The attention mechanism is to capture spatial
features. Evaluations are performed in cross-validation experiments over the
noisy CHB-MIT data set. The obtained average sensitivity, specificity and
precision of 88.80%, 88.60% and 88.69% are better than using the current
state-of-the-art methods. In addition, we explore how the segment length
affects the classification performance. Thirteen different segment lengths are
assessed, showing that the classification performance varies over the segment
lengths, and the maximal fluctuating margin is more than 4%. Thus, the segment
length is an important factor influencing the classification performance."
421,"One of the primary goals in spectrum occupancy mapping is to create a system
that is robust to assumptions about the number of sensors, occupancy threshold
(in dBm), sensor noise, number of emitters and the propagation environment. We
show that such a system may be designed with neural networks using a process of
aggregation to allow a variable number of sensors during training and testing.
This process transforms the variable number of measurements into approximate
log-likelihood ratios (LLRs), which are fed as a fixed-resolution image into a
neural network. The use of LLR's provides robustness to the effects of noise
and occupancy threshold. In other words, a system may be trained for a nominal
number of sensors, threshold and noise levels, and still operate well at
various other levels without retraining. Our system operates without knowledge
of the number of emitters and does not explicitly attempt to estimate their
number or power. Receiver operating curves with realistic propagation
environments using topographic maps with commercial network design tools show
how performance of the neural network varies with the environment. The use of
very low-resolution sensors in this system can still yield good performance."
422,"Freezing of gait (FoG) is a common gait disability in Parkinson's disease,
that usually appears in its advanced stage. Freeze episodes are associated with
falls, injuries, and psychological consequences, negatively affecting the
patients' quality of life. For detecting FoG episodes automatically, a highly
accurate detection method is necessary. This paper presents an approach for
detecting FoG episodes utilizing a deep recurrent neural network (RNN) on
3D-accelerometer measurements. We investigate suitable features and feature
combinations extracted from the sensors' time series data. Specifically, for
detecting FoG episodes, we apply a deep RNN with Long Short-Term Memory cells.
In our experiments, we perform both user dependent and user independent
experiments, to detect freeze episodes. Our experimental results show that the
frequency domain features extracted from the trunk sensor are the most
informative feature group in the subject independent method, achieving an
average AUC score of 93%, Specificity of 90% and Sensitivity of 81%. Moreover,
frequency and statistical features of all the sensors are identified as the
best single input for the subject dependent method, achieving an average AUC
score of 97%, Specificity of 96% and Sensitivity of 87%. Overall, in a
comparison to state-of-the-art approaches from literature as baseline methods,
our proposed approach outperforms these significantly."
423,"Beam alignment (BA) is to ensure the transmitter and receiver beams are
accurately aligned to establish a reliable communication link in
millimeter-wave (mmwave) systems. Existing BA methods search the entire beam
space to identify the optimal transmit-receive beam pair, which incurs
significant BA latency on the order of seconds in the worst case. In this
paper, we develop a learning algorithm to reduce BA latency, namely
Hierarchical Beam Alignment (HBA) algorithm. We first formulate the BA problem
as a stochastic multi-armed bandit problem with the objective to maximize the
cumulative received signal strength within a certain period. The proposed
algorithm takes advantage of the correlation structure among beams such that
the information from nearby beams is extracted to identify the optimal beam,
instead of searching the entire beam space. Furthermore, the prior knowledge on
the channel fluctuation is incorporated in the proposed algorithm to further
accelerate the BA process. Theoretical analysis indicates that the proposed
algorithm is asymptotically optimal. Extensive simulation results demonstrate
that the proposed algorithm can identify the optimal beam with a high
probability and reduce the BA latency from hundreds of milliseconds to a few
milliseconds in the multipath channel, as compared to the existing BA method in
IEEE 802.11ad."
424,"Plug-and-play priors (PnP) is a popular framework for regularized signal
reconstruction by using advanced denoisers within an iterative algorithm. In
this paper, we discuss our recent online variant of PnP that uses only a subset
of measurements at every iteration, which makes it scalable to very large
datasets. We additionally present novel convergence results for both batch and
online PnP algorithms."
425,"Developments in Brain Computer Interfaces (BCIs) are empowering those with
severe physical afflictions through their use in assistive systems. Common
methods of achieving this is via Motor Imagery (MI), which maps brain signals
to code for certain commands. Electroencephalogram (EEG) is preferred for
recording brain signal data on account of it being non-invasive. Despite their
potential utility, MI-BCI systems are yet confined to research labs. A major
cause for this is lack of robustness of such systems. As hypothesized by two
teams during Cybathlon 2016, a particular source of the system's vulnerability
is the sharp change in the subject's state of emotional arousal. This work aims
towards making MI-BCI systems resilient to such emotional perturbations. To do
so, subjects are exposed to high and low arousal-inducing virtual reality (VR)
environments before recording EEG data. The advent of COVID-19 compelled us to
modify our methodology. Instead of training machine learning algorithms to
classify emotional arousal, we opt for classifying subjects that serve as proxy
for each state. Additionally, MI models are trained for each subject instead of
each arousal state. As training subjects to use MI-BCI can be an arduous and
time-consuming process, reducing this variability and increasing robustness can
considerably accelerate the acceptance and adoption of assistive technologies
powered by BCI."
426,"We consider the problem of joint channel assignment and power allocation in
underlaid cellular vehicular-to-everything (C-V2X) systems where multiple
vehicle-to-network (V2N) uplinks share the time-frequency resources with
multiple vehicle-to-vehicle (V2V) platoons that enable groups of connected and
autonomous vehicles to travel closely together. Due to the nature of high user
mobility in vehicular environment, traditional centralized optimization
approach relying on global channel information might not be viable in C-V2X
systems with large number of users. Utilizing a multi-agent reinforcement
learning (RL) approach, we propose a distributed resource allocation (RA)
algorithm to overcome this challenge. Specifically, we model the RA problem as
a multi-agent system. Based solely on the local channel information, each
platoon leader, acting as an agent, collectively interacts with each other and
accordingly selects the optimal combination of sub-band and power level to
transmit its signals. Toward this end, we utilize the double deep Q-learning
algorithm to jointly train the agents under the objectives of simultaneously
maximizing the sum-rate of V2N links and satisfying the packet delivery
probability of each V2V link in a desired latency limitation. Simulation
results show that our proposed RL-based algorithm provides a close performance
compared to that of the well-known exhaustive search algorithm."
427,"Accurate detection of pathological conditions in human subjects can be
achieved through off-line analysis of recorded biological signals such as
electrocardiograms (ECGs). However, human diagnosis is time-consuming and
expensive, as it requires the time of medical professionals. This is especially
inefficient when indicative patterns in the biological signals are infrequent.
Moreover, patients with suspected pathologies are often monitored for extended
periods, requiring the storage and examination of large amounts of
non-pathological data, and entailing a difficult visual search task for
diagnosing professionals.
  In this work we propose a compact and sub-mW low power neural processing
system that can be used to perform on-line and real-time preliminary diagnosis
of pathological conditions, to raise warnings for the existence of possible
pathological conditions, or to trigger an off-line data recording system for
further analysis by a medical professional. We apply the system to real-time
classification of ECG data for distinguishing between healthy heartbeats and
pathological rhythms.
  Multi-channel analog ECG traces are encoded as asynchronous streams of binary
events and processed using a spiking recurrent neural network operated in a
reservoir computing paradigm. An event-driven neuron output layer is then
trained to recognize one of several pathologies. Finally, the filtered activity
of this output layer is used to generate a binary trigger signal indicating the
presence or absence of a pathological pattern.
  We validate the approach proposed using a Dynamic Neuromorphic Asynchronous
Processor (DYNAP) chip, implemented using a standard 180 nm CMOS VLSI process,
and present experimental results measured from the chip."
428,"Clinical electroencephalographic (EEG) data varies significantly depending on
a number of operational conditions (e.g., the type and placement of electrodes,
the type of electrical grounding used). This investigation explores the
statistical differences present in two different referential montages: Linked
Ear (LE) and Averaged Reference (AR). Each of these accounts for approximately
45% of the data in the TUH EEG Corpus. In this study, we explore the impact
this variability has on machine learning performance. We compare the
statistical properties of features generated using these two montages, and
explore the impact of performance on our standard Hidden Markov Model (HMM)
based classification system. We show that a system trained on LE data
significantly outperforms one trained only on AR data (77.2% vs. 61.4%). We
also demonstrate that performance of a system trained on both data sets is
somewhat compromised (71.4% vs. 77.2%). A statistical analysis of the data
suggests that mean, variance and channel normalization should be considered.
However, cepstral mean subtraction failed to produce an improvement in
performance, suggesting that the impact of these statistical differences is
subtler."
429,"Due to the very narrow beam used in millimeter wave communication (mmWave),
beam alignment (BA) is a critical issue. In this work, we investigate the issue
of mmWave BA and present a novel beam alignment scheme on the basis of a
machine learning strategy, Bayesian optimization (BO). In this context, we
consider the beam alignment issue to be a black box function and then use BO to
find the possible optimal beam pair. During the BA procedure, this strategy
exploits information from the measured beam pairs to predict the best beam
pair. In addition, we suggest a novel BO algorithm based on the gradient
boosting regression tree model. The simulation results demonstrate the spectral
efficiency performance of our proposed schemes for BA using three different
surrogate models. They also demonstrate that the proposed schemes can achieve
spectral efficiency with a small overhead when compared to the orthogonal match
pursuit (OMP) algorithm and the Thompson sampling-based multi-armed bandit
(TS-MAB) method."
430,"Binarized neural networks (BNNs) have shown exciting potential for utilising
neural networks in embedded implementations where area, energy and latency
constraints are paramount. With BNNs, multiply-accumulate (MAC) operations can
be simplified to XnorPopcount operations, leading to massive reductions in both
memory and computation resources. Furthermore, multiple efficient
implementations of BNNs have been reported on field-programmable gate array
(FPGA) implementations. This paper proposes a smaller, faster, more
energy-efficient approximate replacement for the XnorPopcountoperation, called
XNorMaj, inspired by state-of-the-art FPGAlook-up table schemes which benefit
FPGA implementations. Weshow that XNorMaj is up to 2x more resource-efficient
than the XnorPopcount operation. While the XNorMaj operation has a minor
detrimental impact on accuracy, the resource savings enable us to use larger
networks to recover the loss."
431,"Objective: The aim of this study is to develop an automated classification
algorithm for polysomnography (PSG) recordings to detect non-apneic and
non-hypopneic arousals. Our particular focus is on detecting the respiratory
effort-related arousals (RERAs) which are very subtle respiratory events that
do not meet the criteria for apnea or hypopnea, and are more challenging to
detect. Methods: The proposed algorithm is based on a bidirectional long
short-term memory (BiLSTM) classifier and 465 multi-domain features, extracted
from multimodal clinical time series. The features consist of a set of
physiology-inspired features (n = 75), obtained by multiple steps of feature
selection and expert analysis, and a set of physiology-agnostic features (n =
390), derived from scattering transform. Results: The proposed algorithm is
validated on the 2018 PhysioNet challenge dataset. The overall performance in
terms of the area under the precision-recall curve (AUPRC) is 0.50 on the
hidden test dataset. This result is tied for the second-best score during the
follow-up and official phases of the 2018 PhysioNet challenge. Conclusions: The
results demonstrate that it is possible to automatically detect subtle
non-apneic/non-hypopneic arousal events from PSG recordings. Significance:
Automatic detection of subtle respiratory events such as RERAs together with
other non-apneic/non-hypopneic arousals will allow detailed annotations of
large PSG databases. This contributes to a better retrospective analysis of
sleep data, which may also improve the quality of treatment."
432,"A spiking neural network (SNN) equalizer model suitable for electronic
neuromorphic hardware is designed for an IM/DD link. The SNN achieves the same
bit-error-rate as an artificial neural network, outperforming linear
equalization."
433,"In this study, we adopted visual motion imagery, which is a more intuitive
brain-computer interface (BCI) paradigm, for decoding the intuitive user
intention. We developed a 3-dimensional BCI training platform and applied it to
assist the user in performing more intuitive imagination in the visual motion
imagery experiment. The experimental tasks were selected based on the movements
that we commonly used in daily life, such as picking up a phone, opening a
door, eating food, and pouring water. Nine subjects participated in our
experiment. We presented statistical evidence that visual motion imagery has a
high correlation from the prefrontal and occipital lobes. In addition, we
selected the most appropriate electroencephalography channels using a
functional connectivity approach for visual motion imagery decoding and
proposed a convolutional neural network architecture for classification. As a
result, the averaged classification performance of the proposed architecture
for 4 classes from 16 channels was 67.50 % across all subjects. This result is
encouraging, and it shows the possibility of developing a BCI-based device
control system for practical applications such as neuroprosthesis and a robotic
arm."
434,"It is challenging to visually detect heart disease from the
electrocardiographic (ECG) signals. Implementing an automated ECG signal
detection system can help diagnosis arrhythmia in order to improve the accuracy
of diagnosis. In this paper, we proposed, implemented, and compared an
automated system using two different frameworks of the combination of
convolutional neural network (CNN) and long-short term memory (LSTM) for
classifying normal sinus signals, atrial fibrillation, and other noisy signals.
The dataset we used is from the MIT-BIT Arrhythmia Physionet. Our approach
demonstrated that the cascade of two deep learning network has higher
performance than the concatenation of them, achieving a weighted f1 score of
0.82. The experimental results have successfully validated that the cascade of
CNN and LSTM can achieve satisfactory performance on discriminating ECG
signals."
435,"In this work we propose an autoencoder based framework for simultaneous
reconstruction and classification of biomedical signals. Previously these two
tasks, reconstruction and classification were treated as separate problems.
This is the first work to propose a combined framework to address the issue in
a holistic fashion. Reconstruction techniques for biomedical signals for
tele-monitoring are largely based on compressed sensing (CS) based method,
these are designed techniques where the reconstruction formulation is based on
some assumption regarding the signal. In this work, we propose a new paradigm
for reconstruction we learn to reconstruct. An autoencoder can be trained for
the same. But since the final goal is to analyze classify the signal we learn a
linear classification map inside the autoencoder. The ensuing optimization
problem is solved using the Split Bregman technique. Experiments have been
carried out on reconstruction and classification of ECG arrhythmia
classification and EEG seizure classification signals. Our proposed tool is
capable of operating in a semi-supervised fashion. We show that our proposed
method is better and more than an order magnitude faster in reconstruction than
CS based methods; it is capable of real-time operation. Our method is also
better than recently proposed classification methods. Significance: This is the
first work offering an alternative to CS based reconstruction. It also shows
that representation learning can yield better results than hand-crafted
features for signal analysis."
436,"In this paper we present a memristor-inspired computational method for
obtaining a type of running spectrogram or fingerprint of epileptiform activity
generated by rodent hippocampal spheroids. It can be used to compute on the fly
and with low computational cost an alert-level signal for epileptiform events
onset. Here, we describe the computational method behind this fingerprint
technique and illustrate it using epileptiform events recorded from hippocampal
spheroids using a microelectrode array system."
437,"The use of electroencephalogram (EEG) as the main input signal in
brain-machine interfaces has been widely proposed due to the non-invasive
nature of the EEG. Here we are specifically interested in interfaces that
extract information from the auditory system and more specifically in the task
of classifying heard speech from EEGs. To do so, we propose to limit the
preprocessing of the EEGs and use machine learning approaches to automatically
extract their meaningful characteristics. More specifically, we use a regulated
recurrent neural network (RNN) reservoir, which has been shown to outperform
classic machine learning approaches when applied to several different
bio-signals, and we compare it with a deep neural network approach. Moreover,
we also investigate the classification performance as a function of the number
of EEG electrodes. A set of 8 subjects were presented randomly with 3 different
auditory stimuli (English vowels a, i and u). We obtained an excellent
classification rate of 83.2% with the RNN when considering all 64 electrodes. A
rate of 81.7% was achieved with only 10 electrodes."
438,"Autism Spectrum Disorder (ASD) is a developmental disorder that often impairs
a child's normal development of the brain. According to CDC, it is estimated
that 1 in 6 children in the US suffer from development disorders, and 1 in 68
children in the US suffer from ASD. This condition has a negative impact on a
person's ability to hear, socialize and communicate. Overall, ASD has a broad
range of symptoms and severity; hence the term spectrum is used. One of the
main contributors to ASD is known to be genetics. Up to date, no suitable cure
for ASD has been found. Early diagnosis is crucial for the long-term treatment
of ASD, but this is challenging due to the lack of a proper objective measures.
Subjective measures often take more time, resources, and have false positives
or false negatives. There is a need for efficient objective measures that can
help in diagnosing this disease early as possible with less effort.
  EEG measures the electric signals of the brain via electrodes placed on
various places on the scalp. These signals can be used to study complex
neuropsychiatric issues. Studies have shown that EEG has the potential to be
used as a biomarker for various neurological conditions including ASD. This
chapter will outline the usage of EEG measurement for the classification of ASD
using machine learning algorithms."
439,"Brain Computer Interfaces (BCI) have become very popular with
Electroencephalography (EEG) being one of the most commonly used signal
acquisition techniques. A major challenge in BCI studies is the individualistic
analysis required for each task. Thus, task-specific feature extraction and
classification are performed, which fails to generalize to other tasks with
similar time-series EEG input data. To this end, we design a GRU-based
universal deep encoding architecture to extract meaningful features from
publicly available datasets for five diverse EEG-based classification tasks.
Our network can generate task and format-independent data representation and
outperform the state of the art EEGNet architecture on most experiments. We
also compare our results with CNN-based, and Autoencoder networks, in turn
performing local, spatial, temporal and unsupervised analysis on the data."
440,"This work proposes a low-power high-accuracy embedded hand-gesture
recognition algorithm targeting battery-operated wearable devices using low
power short-range RADAR sensors. A 2D Convolutional Neural Network (CNN) using
range frequency Doppler features is combined with a Temporal Convolutional
Neural Network (TCN) for time sequence prediction. The final algorithm has a
model size of only 46 thousand parameters, yielding a memory footprint of only
92 KB. Two datasets containing 11 challenging hand gestures performed by 26
different people have been recorded containing a total of 20,210 gesture
instances. On the 11 hand gesture dataset, accuracies of 86.6% (26 users) and
92.4% (single user) have been achieved, which are comparable to the
state-of-the-art, which achieves 87% (10 users) and 94% (single user), while
using a TCN-based network that is 7500x smaller than the state-of-the-art.
Furthermore, the gesture recognition classifier has been implemented on a
Parallel Ultra-Low Power Processor, demonstrating that real-time prediction is
feasible with only 21 mW of power consumption for the full TCN sequence
prediction network, while a system-level power consumption of less than 100 mW
is achieved. We provide open-source access to all the code and data collected
and used in this work on tinyradar.ethz.ch."
441,"Recent advances in unmanned aerial vehicle (UAV) technology have
revolutionized a broad class of civil and military applications. However, the
designs of wireless technologies that enable real-time streaming of
high-definition video between UAVs and ground clients present a conundrum. Most
existing adaptive bitrate (ABR) algorithms are not optimized for the
air-to-ground links, which usually fluctuate dramatically due to the dynamic
flight states of the UAV. In this paper, we present SA-ABR, a new
sensor-augmented system that generates ABR video streaming algorithms with the
assistance of various kinds of inherent sensor data that are used to pilot
UAVs. By incorporating the inherent sensor data with network observations,
SA-ABR trains a deep reinforcement learning (DRL) model to extract salient
features from the flight state information and automatically learn an ABR
algorithm to adapt to the varying UAV channel capacity through the training
process. SA-ABR does not rely on any assumptions or models about UAV's flight
states or the environment, but instead, it makes decisions by exploiting
temporal properties of past throughput through the long short-term memory
(LSTM) to adapt itself to a wide range of highly dynamic environments. We have
implemented SA-ABR in a commercial UAV and evaluated it in the wild. We compare
SA-ABR with a variety of existing state-of-the-art ABR algorithms, and the
results show that our system outperforms the best known existing ABR algorithm
by 21.4% in terms of the average quality of experience (QoE) reward."
442,"Identification of the type of communication technology and/or modulation
scheme based on detected radio signal are challenging problems encountered in a
variety of applications including spectrum allocation and radio interference
mitigation. They are rendered difficult due to a growing number of emitter
types and varied effects of real-world channels upon the radio signal. Existing
spectrum monitoring techniques are capable of acquiring massive amounts of
radio and real-time spectrum data using compact sensors deployed in a variety
of settings. However, state-of-the-art methods that use such data to classify
emitter types and detect communication schemes struggle to achieve required
levels of accuracy at a computational efficiency that would allow their
implementation on low-cost computational platforms. In this paper, we present a
learning framework based on an LSTM denoising auto-encoder designed to
automatically extract stable and robust features from noisy radio signals, and
infer modulation or technology type using the learned features. The algorithm
utilizes a compact neural network architecture readily implemented on a
low-cost computational platform while exceeding state-of-the-art accuracy.
Results on realistic synthetic as well as over-the-air radio data demonstrate
that the proposed framework reliably and efficiently classifies received radio
signals, often demonstrating superior performance compared to state-of-the-art
methods."
443,"Accelerated multi-coil magnetic resonance imaging reconstruction has seen a
substantial recent improvement combining compressed sensing with deep learning.
However, most of these methods rely on estimates of the coil sensitivity
profiles, or on calibration data for estimating model parameters. Prior work
has shown that these methods degrade in performance when the quality of these
estimators are poor or when the scan parameters differ from the training
conditions. Here we introduce Deep J-Sense as a deep learning approach that
builds on unrolled alternating minimization and increases robustness: our
algorithm refines both the magnetization (image) kernel and the coil
sensitivity maps. Experimental results on a subset of the knee fastMRI dataset
show that this increases reconstruction performance and provides a significant
degree of robustness to varying acceleration factors and calibration region
sizes."
444,"The accuracy of channel state information (CSI) acquisition directly affects
the performance of millimeter wave (mmWave) communications. In this article, we
provide an overview on CSI acquisition, including beam training and channel
estimation for mmWave massive multiple-input multiple-output systems. The beam
training can avoid the estimation of a high-dimension channel matrix while the
channel estimation can flexibly exploit advanced signal processing techniques.
In addition to introducing the traditional and machine learning-based
approaches in this article, we also compare different approaches in terms of
spectral efficiency, computational complexity, and overhead."
445,"Effective analysis of EEG signals for potential clinical applications remains
a challenging task. So far, the analysis and conditioning of EEG have largely
remained sex-neutral. This paper employs a machine learning approach to explore
the evidence of sex effects on EEG signals, and confirms the generality of
these effects by achieving successful sex prediction of resting-state EEG
signals. We have found that the brain connectivity represented by the coherence
between certain sensor channels are good predictors of sex."
446,"Online machine learning (OML) algorithms do not need any training phase and
can be deployed directly in an unknown environment. OML includes multi-armed
bandit (MAB) algorithms that can identify the best arm among several arms by
achieving a balance between exploration of all arms and exploitation of optimal
arm. The Kullback-Leibler divergence based upper confidence bound (KLUCB) is
the state-of-the-art MAB algorithm that optimizes exploration-exploitation
trade-off but it is complex due to underlining optimization routine. This
limits its usefulness for robotics and radio applications which demand
integration of KLUCB with the PHY on the system on chip (SoC). In this paper,
we efficiently map the KLUCB algorithm on SoC by realizing optimization routine
via alternative synthesizable computation without compromising on the
performance. The proposed architecture is dynamically reconfigurable such that
the number of arms, as well as type of algorithm, can be changed on-the-fly.
Specifically, after initial learning, on-the-fly switch to light-weight UCB
offers around 10-factor improvement in latency and throughput. Since learning
duration depends on the unknown arm statistics, we offer intelligence embedded
in architecture to decide the switching instant. We validate the functional
correctness and usefulness of the proposed architecture via a realistic
wireless application and detailed complexity analysis demonstrates its
feasibility in realizing intelligent radios."
447,"Radar pulse streams exhibit increasingly complex temporal patterns and can no
longer rely on a purely value-based analysis of the pulse attributes for the
purpose of emitter classification. In this paper, we employ Recurrent Neural
Networks (RNNs) to efficiently model and exploit the temporal dependencies
present inside pulse streams. With the purpose of enhancing the network
prediction capability, we introduce two novel techniques: a per-sequence
normalization, able to mine the useful temporal patterns; and
attribute-specific RNN processing, capable of processing the extracted
information effectively. The new techniques are evaluated with an ablation
study and the proposed solution is compared to previous Deep Learning (DL)
approaches. Finally, a comparative study on the robustness of the same
approaches is conducted and its results are presented."
448,"Geospatial observations combined with computational models have become key to
understanding the physical systems of our environment and enable the design of
best practices to reduce societal harm. Cloud-based deployments help to scale
up these modeling and AI workflows. Yet, for practitioners to make robust
conclusions, model tuning and testing is crucial, a resource intensive process
which involves the variation of model input variables. We have developed the
Variational Exploration Module which facilitates the optimization and
validation of modeling workflows deployed in the cloud by orchestrating
workflow executions and using Bayesian and machine learning-based methods to
analyze model behavior. User configurations allow the combination of diverse
sampling strategies in multi-agent environments. The flexibility and robustness
of the model-agnostic module is demonstrated using real-world applications."
449,"User Stories record what must be built in projects that use agile practices.
User Stories serve both to estimate effort, generally measured in Story Points,
and to plan what should be done in a Sprint. Therefore, it is essential to
train software engineers on how to create simple, easily readable, and
comprehensive User Stories. For that reason, we designed, implemented, applied,
and evaluated a web application called User Story Tutor (UST). UST checks the
description of a given User Story for readability, and if needed, recommends
appropriate practices for improvement. UST also estimates a User Story effort
in Story Points using Machine Learning techniques. As such UST may support the
continuing education of agile development teams when writing and reviewing User
Stories. UST's ease of use was evaluated by 40 agile practitioners according to
the Technology Acceptance Model (TAM) and AttrakDiff. The TAM evaluation
averages were good in almost all considered variables. Application of the
AttrakDiff evaluation framework produced similar good results. Apparently, UST
can be used with good reliability. Applying UST to assist in the construction
of User Stories is a viable technique that, at the very least, can be used by
agile developments to complement and enhance current User Story creation."
450,"Formal methods apply algorithms based on mathematical principles to enhance
the reliability of systems. It would only be natural to try to progress from
verification, model checking or testing a system against its formal
specification into constructing it automatically. Classical algorithmic
synthesis theory provides interesting algorithms but also alarming high
complexity and undecidability results. The use of genetic programming, in
combination with model checking and testing, provides a powerful heuristic to
synthesize programs. The method is not completely automatic, as it is fine
tuned by a user that sets up the specification and parameters. It also does not
guarantee to always succeed and converge towards a solution that satisfies all
the required properties. However, we applied it successfully on quite
nontrivial examples and managed to find solutions to hard programming
challenges, as well as to improve and to correct code. We describe here several
versions of our method for synthesizing sequential and concurrent systems."
451,"Despite the immense popularity of the Automated Program Repair (APR) field,
the question of patch validation is still open. Most of the present-day
approaches follow the so-called Generate-and-Validate approach, where first a
candidate solution is being generated and after validated against an oracle.
The latter, however, might not give a reliable result, because of the
imperfections in such oracles; one of which is usually the test suite. Although
(re-) running the test suite is right under one's nose, in real life
applications the problem of over- and underfitting often occurs, resulting in
inadequate patches. Efforts that have been made to tackle with this problem
include patch filtering, test suite expansion, careful patch producing and many
more. Most approaches to date use post-filtering relying either on test
execution traces or make use of some similarity concept measured on the
generated patches. Our goal is to investigate the nature of these
similarity-based approaches. To do so, we trained a Doc2Vec model on an
open-source JavaScript project and generated 465 patches for 10 bugs in it.
These plausible patches alongside with the developer fix are then ranked based
on their similarity to the original program. We analyzed these similarity lists
and found that plain document embeddings may lead to misclassification - it
fails to capture nuanced code semantics. Nevertheless, in some cases it also
provided useful information, thus helping to better understand the area of
Automated Program Repair."
452,"In recent years, Explainable AI (xAI) attracted a lot of attention as various
countries turned explanations into a legal right. xAI allows for improving
models beyond the accuracy metric by, e.g., debugging the learned pattern and
demystifying the AI's behavior. The widespread use of xAI brought new
challenges. On the one hand, the number of published xAI algorithms underwent a
boom, and it became difficult for practitioners to select the right tool. On
the other hand, some experiments did highlight how easy data scientists could
misuse xAI algorithms and misinterpret their results. To tackle the issue of
comparing and correctly using feature importance xAI algorithms, we propose
Compare-xAI, a benchmark that unifies all exclusive functional testing methods
applied to xAI algorithms. We propose a selection protocol to shortlist
non-redundant functional tests from the literature, i.e., each targeting a
specific end-user requirement in explaining a model. The benchmark encapsulates
the complexity of evaluating xAI methods into a hierarchical scoring of three
levels, namely, targeting three end-user groups: researchers, practitioners,
and laymen in xAI. The most detailed level provides one score per test. The
second level regroups tests into five categories (fidelity, fragility,
stability, simplicity, and stress tests). The last level is the aggregated
comprehensibility score, which encapsulates the ease of correctly interpreting
the algorithm's output in one easy to compare value. Compare-xAI's interactive
user interface helps mitigate errors in interpreting xAI results by quickly
listing the recommended xAI solutions for each ML task and their current
limitations. The benchmark is made available at
https://karim-53.github.io/cxai/"
453,"Software engineers are increasingly adding semantic search capabilities to
applications using a strategy known as Retrieval Augmented Generation (RAG). A
RAG system involves finding documents that semantically match a query and then
passing the documents to a large language model (LLM) such as ChatGPT to
extract the right answer using an LLM. RAG systems aim to: a) reduce the
problem of hallucinated responses from LLMs, b) link sources/references to
generated responses, and c) remove the need for annotating documents with
meta-data. However, RAG systems suffer from limitations inherent to information
retrieval systems and from reliance on LLMs. In this paper, we present an
experience report on the failure points of RAG systems from three case studies
from separate domains: research, education, and biomedical. We share the
lessons learned and present 7 failure points to consider when designing a RAG
system. The two key takeaways arising from our work are: 1) validation of a RAG
system is only feasible during operation, and 2) the robustness of a RAG system
evolves rather than designed in at the start. We conclude with a list of
potential research directions on RAG systems for the software engineering
community."
454,"In software development, it is common for programmers to copy-paste or port
code snippets and then adapt them to their use case. This scenario motivates
the code adaptation task -- a variant of program repair which aims to adapt
variable identifiers in a pasted snippet of code to the surrounding,
preexisting source code. However, no existing approach has been shown to
effectively address this task. In this paper, we introduce AdaptivePaste, a
learning-based approach to source code adaptation, based on transformers and a
dedicated dataflow-aware deobfuscation pre-training task to learn meaningful
representations of variable usage patterns. We evaluate AdaptivePaste on a
dataset of code snippets in Python. Results suggest that our model can learn to
adapt source code with 79.8% accuracy. To evaluate how valuable is
AdaptivePaste in practice, we perform a user study with 10 Python developers on
a hundred real-world copy-paste instances. The results show that AdaptivePaste
reduces the dwell time to nearly half the time it takes for manual code
adaptation, and helps to avoid bugs. In addition, we utilize the participant
feedback to identify potential avenues for improvement of AdaptivePaste."
455,"Performance becomes an issue particularly when execution cost hinders the
functionality of a program. Typically a profiler can be used to find program
code execution which represents a large portion of the overall execution cost
of a program. Pinpointing where a performance issue exists provides a starting
point for tracing cause back through a program.
  While profiling shows where a performance issue manifests, we use mutation
analysis to show where a performance improvement is likely to exist. We find
that mutation analysis can indicate locations within a program which are highly
impactful to the overall execution cost of a program yet are executed
relatively infrequently. By better locating potential performance improvements
in programs we hope to make performance improvement more amenable to
automation."
456,"Prompt engineering is an increasingly important skill set needed to converse
effectively with large language models (LLMs), such as ChatGPT. Prompts are
instructions given to an LLM to enforce rules, automate processes, and ensure
specific qualities (and quantities) of generated output. Prompts are also a
form of programming that can customize the outputs and interactions with an
LLM. This paper describes a catalog of prompt engineering techniques presented
in pattern form that have been applied to solve common problems when conversing
with LLMs. Prompt patterns are a knowledge transfer method analogous to
software patterns since they provide reusable solutions to common problems
faced in a particular context, i.e., output generation and interaction when
working with LLMs. This paper provides the following contributions to research
on prompt engineering that apply LLMs to automate software development tasks.
First, it provides a framework for documenting patterns for structuring prompts
to solve a range of problems so that they can be adapted to different domains.
Second, it presents a catalog of patterns that have been applied successfully
to improve the outputs of LLM conversations. Third, it explains how prompts can
be built from multiple patterns and illustrates prompt patterns that benefit
from combination with other prompt patterns."
457,"Vulnerability identification is crucial for cyber security in the
software-related industry. Early identification methods require significant
manual efforts in crafting features or annotating vulnerable code. Although the
recent pre-trained models alleviate this issue, they overlook the multiple rich
structural information contained in the code itself. In this paper, we propose
a novel Multi-View Pre-Trained Model (MV-PTM) that encodes both sequential and
multi-type structural information of the source code and uses contrastive
learning to enhance code representations. The experiments conducted on two
public datasets demonstrate the superiority of MV-PTM. In particular, MV-PTM
improves GraphCodeBERT by 3.36\% on average in terms of F1 score."
458,"Over the last decades, the amount of data of all kinds available
electronically has increased dramatically. Data are accessible through a range
of interfaces including Web browsers, database query languages,
application-specific interfaces, built on top of a number of different data
exchange formats. All these data span from un-structured to highly structured
data. Very often, some of them have structure even if the structure is
implicit, and not as rigid or regular as that found in standard database
systems. Spreadsheet documents are prototypical in this respect. Spreadsheets
are the lightweight technology able to supply companies with easy to build
business management and business intelligence applications, and business people
largely adopt spreadsheets as smart vehicles for data files generation and
sharing. Actually, the more spreadsheets grow in complexity (e.g., their use in
product development plans and quoting), the more their arrangement,
maintenance, and analysis appear as a knowledge-driven activity. The
algorithmic approach to the problem of automatic data structure extraction from
spreadsheet documents (i.e., grid-structured and free topological-related data)
emerges from the WIA project: Worksheets Intelligent Analyser. The
WIA-algorithm shows how to provide a description of spreadsheet contents in
terms of higher level of abstractions or conceptualisations. In particular, the
WIA-algorithm target is about the extraction of i) the calculus work-flow
implemented in the spreadsheets formulas and ii) the logical role played by the
data which take part into the calculus. The aim of the resulting
conceptualisations is to provide spreadsheets with abstract representations
useful for further model refinements and optimizations through evolutionary
algorithms computations."
459,"Automation engineering is the task of integrating, via software, various
sensors, actuators, and controls for automating a real-world process. Today,
automation engineering is supported by a suite of software tools including
integrated development environments (IDE), hardware configurators, compilers,
and runtimes. These tools focus on the automation code itself, but leave the
automation engineer unassisted in their decision making. This can lead to
increased time for software development because of imperfections in decision
making leading to multiple iterations between software and hardware. To address
this, this paper defines multiple challenges often faced in automation
engineering and propose solutions using machine learning to assist engineers
tackle such challenges. We show that machine learning can be leveraged to
assist the automation engineer in classifying automation, finding similar code
snippets, and reasoning about the hardware selection of sensors and actuators.
We validate our architecture on two real datasets consisting of 2,927 Arduino
projects, and 683 Programmable Logic Controller (PLC) projects. Our results
show that paragraph embedding techniques can be utilized to classify automation
using code snippets with precision close to human annotation, giving an
F1-score of 72%. Further, we show that such embedding techniques can help us
find similar code snippets with high accuracy. Finally, we use autoencoder
models for hardware recommendation and achieve a p@3 of 0.79 and p@5 of 0.95."
460,"This paper presents an ensemble part-of-speech tagging approach for source
code identifiers. Ensemble tagging is a technique that uses machine-learning
and the output from multiple part-of-speech taggers to annotate natural
language text at a higher quality than the part-of-speech taggers are able to
obtain independently. Our ensemble uses three state-of-the-art part-of-speech
taggers: SWUM, POSSE, and Stanford. We study the quality of the ensemble's
annotations on five different types of identifier names: function, class,
attribute, parameter, and declaration statement at the level of both individual
words and full identifier names. We also study and discuss the weaknesses of
our tagger to promote the future amelioration of these problems through further
research. Our results show that the ensemble achieves 75\% accuracy at the
identifier level and 84-86\% accuracy at the word level. This is an increase of
+17\% points at the identifier level from the closest independent
part-of-speech tagger."
461,"Program source code contains complex structure information, which can be
represented in structured data forms like trees or graphs. To acquire the
structural information in source code, most existing researches use abstract
syntax trees (AST). A group of works add additional edges to ASTs to convert
source code into graphs and use graph neural networks to learn representations
for program graphs. Although these works provide additional control or data
flow information to ASTs for downstream tasks, they neglect an important aspect
of structure information in AST itself: the different types of nodes and edges.
In ASTs, different nodes contain different kinds of information like variables
or control flow, and the relation between a node and all its children can also
be different.
  To address the information of node and edge types, we bring the idea of
heterogeneous graphs to learning on source code and present a new formula of
building heterogeneous program graphs from ASTs with additional type
information for nodes and edges. We use the ASDL grammar of programming
language to define the node and edge types of program graphs. Then we use
heterogeneous graph neural networks to learn on these graphs. We evaluate our
approach on two tasks: code comment generation and method naming. Both tasks
require reasoning on the semantics of complete code snippets. Experiment
results show that our approach outperforms baseline models, including
homogeneous graph-based models, showing that leveraging the type information of
nodes and edges in program graphs can help in learning program semantics."
462,"Cyber-Physical System (CPS) represents systems that join both hardware and
software components to perform real-time services. Maintaining the system's
reliability is critical to the continuous delivery of these services. However,
the CPS running environment is full of uncertainties and can easily lead to
performance degradation. As a result, the need for a recovery technique is
highly needed to achieve resilience in the system, with keeping in mind that
this technique should be as green as possible. This early doctorate proposal,
suggests a game theory solution to achieve resilience and green in CPS. Game
theory has been known for its fast performance in decision-making, helping the
system to choose what maximizes its payoffs. The proposed game model is
described over a real-life collaborative artificial intelligence system (CAIS),
that involves robots with humans to achieve a common goal. It shows how the
expected results of the system will achieve the resilience of CAIS with
minimized CO2 footprint."
463,"This paper aims to evaluate GitHub Copilot's generated code quality based on
the LeetCode problem set using a custom automated framework. We evaluate the
results of Copilot for 4 programming languages: Java, C++, Python3 and Rust. We
aim to evaluate Copilot's reliability in the code generation stage, the
correctness of the generated code and its dependency on the programming
language, problem's difficulty level and problem's topic. In addition to that,
we evaluate code's time and memory efficiency and compare it to the average
human results. In total, we generate solutions for 1760 problems for each
programming language and evaluate all the Copilot's suggestions for each
problem, resulting in over 50000 submissions to LeetCode spread over a 2-month
period. We found that Copilot successfully solved most of the problems.
However, Copilot was rather more successful in generating code in Java and C++
than in Python3 and Rust. Moreover, in case of Python3 Copilot proved to be
rather unreliable in the code generation phase. We also discovered that
Copilot's top-ranked suggestions are not always the best. In addition, we
analysed how the topic of the problem impacts the correctness rate. Finally,
based on statistics information from LeetCode, we can conclude that Copilot
generates more efficient code than an average human."
464,"Large language models (LLMs) have shown remarkable abilities to generate
code, however their ability to develop software for embedded systems, which
requires cross-domain knowledge of hardware and software has not been studied.
In this paper we develop an extensible, open source hardware-in-the-loop
framework to systematically evaluate leading LLMs (GPT-3.5, GPT-4, PaLM 2) to
assess their capabilities and limitations for embedded system development. We
observe through our study that even when these tools fail to produce working
code, they consistently generate helpful reasoning about embedded design tasks.
We leverage this finding to study how human programmers interact with these
tools, and develop an human-AI based software engineering workflow for building
embedded systems.
  Our evaluation platform for verifying LLM generated programs uses sensor
actuator pairs for physical evaluation. We compare all three models with N=450
experiments and find surprisingly that GPT-4 especially shows an exceptional
level of cross-domain understanding and reasoning, in some cases generating
fully correct programs from a single prompt. In N=50 trials, GPT-4 produces
functional I2C interfaces 66% of the time. GPT-4 also produces register-level
drivers, code for LoRa communication, and context-specific power optimizations
for an nRF52 program resulting in over 740x current reduction to 12.2uA. We
also characterize the models' limitations to develop a generalizable human-AI
workflow for using LLMs in embedded system development. We evaluate our
workflow with 15 users including novice and expert programmers. We find that
our workflow improves productivity for all users and increases the success rate
for building a LoRa environmental sensor from 25% to 100%, including for users
with zero hardware or C/C++ experience."
465,"A Collaborative Artificial Intelligence System (CAIS) is a cyber-physical
system that learns actions in collaboration with humans in a shared environment
to achieve a common goal. In particular, a CAIS is equipped with an AI model to
support the decision-making process of this collaboration. When an event
degrades the performance of CAIS (i.e., a disruptive event), this
decision-making process may be hampered or even stopped. Thus, it is of
paramount importance to monitor the learning of the AI model, and eventually
support its decision-making process in such circumstances. This paper
introduces a new methodology to automatically support the decision-making
process in CAIS when the system experiences performance degradation after a
disruptive event. To this aim, we develop a framework that consists of three
components: one manages or simulates CAIS's environment and disruptive events,
the second automates the decision-making process, and the third provides a
visual analysis of CAIS behavior. Overall, our framework automatically monitors
the decision-making process, intervenes whenever a performance degradation
occurs, and recommends the next action. We demonstrate our framework by
implementing an example with a real-world collaborative robot, where the
framework recommends the next action that balances between minimizing the
recovery time (i.e., resilience), and minimizing the energy adverse effects
(i.e., greenness)."
466,"Unit testing frameworks are nowadays considered a best practice, included in
almost all modern software development processes, to achieve rapid development
of correct specifications. Knowledge representation and reasoning paradigms
such as Answer Set Programming (ASP), that have been used in industry-level
applications, are not an exception. Indeed, the first unit testing
specification language for ASP was proposed in 2011 as a feature of the ASPIDE
development environment. Later, a more portable unit testing language was
included in the LANA annotation language. In this paper we revisit both
languages and tools for unit testing in ASP. We propose a new unit test
specification language that allows one to inline tests within ASP programs, and
we identify the computational complexity of the tasks associated with checking
the various program-correctness assertions. Test-case specifications are
transparent to the traditional evaluation, but can be interpreted by a specific
testing tool. Thus, we present a novel environment supporting test driven
development of ASP programs."
467,"Self-adaptive software can assess and modify its behavior when the assessment
indicates that the program is not performing as intended or when improved
functionality or performance is available. Since the mid-1960s, the subject of
system adaptivity has been extensively researched, and during the last decade,
many application areas and technologies involving self-adaptation have gained
prominence. All of these efforts have in common the introduction of
self-adaptability through software. Thus, it is essential to investigate
systematic software engineering methods to create self-adaptive systems that
may be used across different domains. The primary objective of this research is
to summarize current advances in awareness requirements for adaptive strategies
based on an examination of state-of-the-art methods described in the
literature. This paper presents a review of self-adaptive systems in the
context of requirement awareness and summarizes the most common methodologies
applied. At first glance, it gives a review of the previous surveys and works
about self-adaptive systems. Afterward, it classifies the current self-adaptive
systems based on six criteria. Then, it presents and evaluates the most common
self-adaptive approaches. Lastly, an evaluation among the self-adaptive models
is conducted based on four concepts (requirements description, monitoring,
relationship, dependency/impact, and tools)."
468,"(Source) Code summarization aims to automatically generate summaries/comments
for a given code snippet in the form of natural language. Such summaries play a
key role in helping developers understand and maintain source code. Existing
code summarization techniques can be categorized into extractive methods and
abstractive methods. The extractive methods extract a subset of important
statements and keywords from the code snippet using retrieval techniques, and
generate a summary that preserves factual details in important statements and
keywords. However, such a subset may miss identifier or entity naming, and
consequently, the naturalness of generated summary is usually poor. The
abstractive methods can generate human-written-like summaries leveraging
encoder-decoder models from the neural machine translation domain. The
generated summaries however often miss important factual details.
  To generate human-written-like summaries with preserved factual details, we
propose a novel extractive-and-abstractive framework. The extractive module in
the framework performs a task of extractive code summarization, which takes in
the code snippet and predicts important statements containing key factual
details. The abstractive module in the framework performs a task of abstractive
code summarization, which takes in the entire code snippet and important
statements in parallel and generates a succinct and human-written-like natural
language summary. We evaluate the effectiveness of our technique, called EACS,
by conducting extensive experiments on three datasets involving six programming
languages. Experimental results show that EACS significantly outperforms
state-of-the-art techniques in terms of all three widely used metrics,
including BLEU, METEOR, and ROUGH-L."
469,"With the ever-increasing use of web APIs in modern-day applications, it is
becoming more important to test the system as a whole. In the last decade,
tools and approaches have been proposed to automate the creation of
system-level test cases for these APIs using evolutionary algorithms (EAs). One
of the limiting factors of EAs is that the genetic operators (crossover and
mutation) are fully randomized, potentially breaking promising patterns in the
sequences of API requests discovered during the search. Breaking these patterns
has a negative impact on the effectiveness of the test case generation process.
To address this limitation, this paper proposes a new approach that uses
agglomerative hierarchical clustering (AHC) to infer a linkage tree model,
which captures, replicates, and preserves these patterns in new test cases. We
evaluate our approach, called LT-MOSA, by performing an empirical study on 7
real-world benchmark applications w.r.t. branch coverage and real-fault
detection capability. We also compare LT-MOSA with the two existing
state-of-the-art white-box techniques (MIO, MOSA) for REST API testing. Our
results show that LT-MOSA achieves a statistically significant increase in test
target coverage (i.e., lines and branches) compared to MIO and MOSA in 4 and 5
out of 7 applications, respectively. Furthermore, LT-MOSA discovers 27 and 18
unique real-faults that are left undetected by MIO and MOSA, respectively."
470,"Developers often wonder how to implement a certain functionality (e.g., how
to parse XML files) using APIs. Obtaining an API usage sequence based on an
API-related natural language query is very helpful in this regard. Given a
query, existing approaches utilize information retrieval models to search for
matching API sequences. These approaches treat queries and APIs as bag-of-words
(i.e., keyword matching or word-to-word alignment) and lack a deep
understanding of the semantics of the query.
  We propose DeepAPI, a deep learning based approach to generate API usage
sequences for a given natural language query. Instead of a bags-of-words
assumption, it learns the sequence of words in a query and the sequence of
associated APIs. DeepAPI adapts a neural language model named RNN
Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length
context vector, and generates an API sequence based on the context vector. We
also augment the RNN Encoder-Decoder by considering the importance of
individual APIs. We empirically evaluate our approach with more than 7 million
annotated code snippets collected from GitHub. The results show that our
approach generates largely accurate API sequences and outperforms the related
approaches."
471,"In the process of code generation, it is essential to guarantee the generated
code satisfies grammar constraints of programming language (PL). However,
neglecting grammar constraints is a fatal drawback of commonly used
sequence-based code generation. In this paper, we devise a pushdown automaton
(PDA)-based methodology to address this problem, exploiting the principle that
PL is a subset of PDA recognizable language and code accepted by PDA is
grammatical. Specifically, we construct a PDA module and design an algorithm to
constrain the generation of sequence-based models to ensure grammatical
correctness. Guided by this methodology, we further propose CodePAD, a
sequence-based code generation framework equipped with a PDA module, to
integrate the deduction of PDA into deep learning. Additionally, this framework
can leverage states of PDA deduction (including state representation, state
prediction task, and joint prediction with state) to assist models in learning
PDA deduction. To comprehensively evaluate CodePAD, we construct a PDA for
Python and conduct extensive experiments on four public benchmark datasets.
CodePAD can leverage existing sequence-based models, and we show that it can
achieve 100\% grammatical correctness percentage on these benchmark datasets.
Thus, it relatively improve 17\% CodeBLEU on CONALA, 8\% EM on DJANGO, and 15\%
CodeBLEU on JUICE-10K compared to base models. In addition, our method
significantly enhances pre-trained models, e.g., CodeBLEU of CodeGen-350M
improvement from 3.21 to 21.54 on MBPP in zero-shot setting."
472,"The successful completion of a software development process depends on the
analytical capability and foresightedness of the project manager. For the
project manager, the main intriguing task is to manage the risk factors as they
adversely influence the completion deadline. One such key risk factor is staff
training. The risk of this factor can be avoided by pre-judging the amount of
training required by the staff. So, a procedure is required to help the project
manager make this decision. This paper presents a system that uses influence
diagrams to implement the risk model to aid decision making. The system also
considers the cost of conducting the training, based on various risk factors
such as, (i) Lack of experience with project software; (ii) Newly appointed
staff; (iii) Staff not well versed with the required quality standards; and
(iv) Lack of experience with project environment. The system provides estimated
requirement details for staff training at the beginning of a software
development project."
473,"Model compression can significantly reduce the sizes of deep neural network
(DNN) models, and thus facilitates the dissemination of sophisticated, sizable
DNN models, especially for their deployment on mobile or embedded devices.
However, the prediction results of compressed models may deviate from those of
their original models. To help developers thoroughly understand the impact of
model compression, it is essential to test these models to find those deviated
behaviors before dissemination. However, this is a non-trivial task because the
architectures and gradients of compressed models are usually not available.
  To this end, we propose DFLARE, a novel, search-based, black-box testing
technique to automatically find triggering inputs that result in deviated
behaviors in image classification tasks. DFLARE iteratively applies a series of
mutation operations to a given seed image, until a triggering input is found.
For better efficacy and efficiency, DFLARE models the search problem as Markov
Chains and leverages the Metropolis-Hasting algorithm to guide the selection of
mutation operators in each iteration. Further, DFLARE utilizes a novel fitness
function to prioritize the mutated inputs that either cause large differences
between two models' outputs, or trigger previously unobserved models'
probability vectors. We evaluated DFLARE on 21 compressed models for image
classification tasks with three datasets. The results show that DFLARE
outperforms the baseline in terms of efficacy and efficiency. We also
demonstrated that the triggering inputs found by DFLARE can be used to repair
up to 48.48% deviated behaviors in image classification tasks and further
decrease the effectiveness of DFLARE on the repaired models."
474,"Testing is an integral part of the software development process. Yet, writing
tests is time-consuming and therefore often neglected. Classical test
generation tools such as EvoSuite generate behavioral test suites by optimizing
for coverage, but tend to produce tests that are hard to understand. Language
models trained on code can generate code that is highly similar to that written
by humans, but current models are trained to generate each file separately, as
is standard practice in natural language processing, and thus fail to consider
the code-under-test context when producing a test file. In this work, we
propose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style
language model with 2.7 Billion parameters, trained on a corpus of Python and
Java projects. We utilize a novel pretraining signal that explicitly considers
the mapping between code and test files when available. We also drastically
increase the maximum sequence length of inputs to 8,192 tokens, 4x more than
typical code generation models, to ensure that the code context is available to
the model when generating test code. We analyze its usefulness for realistic
applications, showing that sampling with filtering (e.g., by compilability,
coverage) allows it to efficiently produce tests that achieve coverage similar
to ones written by developers while resembling their writing style. By
utilizing the code context, CAT-LM generates more valid tests than even much
larger language models trained with more data (CodeGen 16B and StarCoder) and
substantially outperforms a recent test-specific model (TeCo) at test
completion. Overall, our work highlights the importance of incorporating
software-specific insights when training language models for code and paves the
way to more powerful automated test generation."
475,"There is growing interest in software migration as the development of
software and society. Manually migrating projects between languages is
error-prone and expensive. In recent years, researchers have begun to explore
automatic program translation using supervised deep learning techniques by
learning from large-scale parallel code corpus. However, parallel resources are
scarce in the programming language domain, and it is costly to collect
bilingual data manually. To address this issue, several unsupervised
programming translation systems are proposed. However, these systems still rely
on huge monolingual source code to train, which is very expensive. Besides,
these models cannot perform well for translating the languages that are not
seen during the pre-training procedure. In this paper, we propose SDA-Trans, a
syntax and domain-aware model for program translation, which leverages the
syntax structure and domain knowledge to enhance the cross-lingual transfer
ability. SDA-Trans adopts unsupervised training on a smaller-scale corpus,
including Python and Java monolingual programs. The experimental results on
function translation tasks between Python, Java, and C++ show that SDA-Trans
outperforms many large-scale pre-trained models, especially for unseen language
translation."
476,"Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide
efficient automatic analysis of real-world feature models (FM) of systems
ranging from cars to operating systems. It is well-known that solver-based
analysis of real-world FMs scale very well even though SAT instances obtained
from such FMs are large, and the corresponding analysis problems are known to
be NP-complete. To better understand why SAT solvers are so effective, we
systematically studied many syntactic and semantic characteristics of a
representative set of large real-world FMs. We discovered that a key reason why
large real-world FMs are easy-to-analyze is that the vast majority of the
variables in these models are unrestricted, i.e., the models are satisfiable
for both true and false assignments to such variables under the current partial
assignment. Given this discovery and our understanding of CDCL SAT solvers, we
show that solvers can easily find satisfying assignments for such models
without too many backtracks relative to the model size, explaining why solvers
scale so well. Further analysis showed that the presence of unrestricted
variables in these real-world models can be attributed to their high-degree of
variability. Additionally, we experimented with a series of well-known
non-backtracking simplifications that are particularly effective in solving
FMs. The remaining variables/clauses after simplifications, called the core,
are so few that they are easily solved even with backtracking, further
strengthening our conclusions."
477,"Self-admitted technical debt (SATD) refers to a form of technical debt in
which developers explicitly acknowledge and document the existence of technical
shortcuts, workarounds, or temporary solutions within the codebase. Over recent
years, researchers have manually labeled datasets derived from various software
development artifacts: source code comments, messages from the issue tracker
and pull request sections, and commit messages. These datasets are designed for
training, evaluation, performance validation, and improvement of machine
learning and deep learning models to accurately identify SATD instances.
However, class imbalance poses a serious challenge across all the existing
datasets, particularly when researchers are interested in categorizing the
specific types of SATD. In order to address the scarcity of labeled data for
SATD \textit{identification} (i.e., whether an instance is SATD or not) and
\textit{categorization} (i.e., which type of SATD is being classified) in
existing datasets, we share the \textit{SATDAUG} dataset, an augmented version
of existing SATD datasets, including source code comments, issue tracker, pull
requests, and commit messages. These augmented datasets have been balanced in
relation to the available artifacts and provide a much richer source of labeled
data for training machine learning or deep learning models."
478,"Log analysis and monitoring are essential aspects in software maintenance and
identifying defects. In particular, the temporal nature and vast size of log
data leads to an interesting and important research question: How can logs be
summarised and monitored over time? While this has been a fundamental topic of
research in the software engineering community, work has typically focused on
heuristic-, syntax-, or static-based methods. In this work, we suggest an
online semantic-based clustering approach to error logs that dynamically
updates the log clusters to enable monitoring code error life-cycles. We also
introduce a novel metric to evaluate the performance of temporal log clusters.
We test our system and evaluation metric with an industrial dataset and find
that our solution outperforms similar systems. We hope that our work encourages
further temporal exploration in defect datasets."
479,"In software engineering, software maintenance is the process of correction,
updating, and improvement of software products after handed over to the
customer. Through offshore software maintenance outsourcing clients can get
advantages like reduce cost, save time, and improve quality. In most cases, the
OSMO vendor generates considerable revenue. However, the selection of an
appropriate proposal among multiple clients is one of the critical problems for
OSMO vendors. The purpose of this paper is to suggest an effective machine
learning technique that can be used by OSMO vendors to assess or predict the
OSMO client proposal. The dataset is generated through a survey of OSMO vendors
working in a developing country. The results showed that supervised
learning-based classifiers like Na\""ive Bayesian, SMO, Logistics apprehended
69.75, 81.81, and 87.27 percent testing accuracy respectively. This study
concludes that supervised learning is the most suitable technique to predict
the OSMO client's proposal."
480,"DevOps has emerged as one of the most rapidly evolving software development
paradigms. With the growing concerns surrounding security in software systems,
the DevSecOps paradigm has gained prominence, urging practitioners to
incorporate security practices seamlessly into the DevOps workflow. However,
integrating security into the DevOps workflow can impact agility and impede
delivery speed. Recently, the advancement of artificial intelligence (AI) has
revolutionized automation in various software domains, including software
security. AI-driven security approaches, particularly those leveraging machine
learning or deep learning, hold promise in automating security workflows. They
reduce manual efforts, which can be integrated into DevOps to ensure
uninterrupted delivery speed and align with the DevSecOps paradigm
simultaneously. This paper seeks to contribute to the critical intersection of
AI and DevSecOps by presenting a comprehensive landscape of AI-driven security
techniques applicable to DevOps and identifying avenues for enhancing security,
trust, and efficiency in software development processes. We analyzed 99
research papers spanning from 2017 to 2023. Specifically, we address two key
research questions (RQs). In RQ1, we identified 12 security tasks associated
with the DevSecOps process and reviewed existing AI-driven security approaches,
the problems they addressed, and the 65 benchmarks used to evaluate those
approaches. Drawing insights from our findings, in RQ2, we discussed
state-of-the-art AI-driven security approaches, highlighted 15 challenges in
existing research, and proposed 15 corresponding avenues for future
opportunities."
481,"This paper introduces a novel concept of self-forensics to complement the
standard autonomic self-CHOP properties of the self-managed systems, to be
specified in the Forensic Lucid language. We argue that self-forensics, with
the forensics taken out of the cybercrime domain, is applicable to
""self-dissection"" for the purpose of verification of autonomous software and
hardware systems of flight-critical systems for automated incident and anomaly
analysis and event reconstruction by the engineering teams in a variety of
incident scenarios during design and testing as well as actual flight data."
482,"Automated Program Repair (APR) helps improve the efficiency of software
development and maintenance. Recent APR techniques use deep learning,
particularly the encoder-decoder architecture, to generate patches. Though
existing DL-based APR approaches have proposed different encoder architectures,
the decoder remains to be the standard one, which generates a sequence of
tokens one by one to replace the faulty statement. This decoder has multiple
limitations: 1) allowing to generate syntactically incorrect programs, 2)
inefficiently representing small edits, and 3) not being able to generate
project-specific identifiers.
  In this paper, we propose Recoder, a syntax-guided edit decoder with
placeholder generation. Recoder is novel in multiple aspects: 1) Recoder
generates edits rather than modified code, allowing efficient representation of
small edits; 2) Recoder is syntax-guided, with the novel provider/decider
architecture to ensure the syntactic correctness of the patched program and
accurate generation; 3) Recoder generates placeholders that could be
instantiated as project-specific identifiers later.
  We conduct experiments to evaluate Recoder on 395 bugs from Defects4J v1.2
and 420 additional bugs from Defects4J v2.0. Our results show that Recoder
repairs 53 bugs on Defects4J v1.2, which achieves 21.4% improvement over the
previous state-of-the-art approach for single-hunk bugs (TBar). Importantly, to
our knowledge, Recoder is the first DL-based APR approach that has outperformed
the traditional APR approaches on this dataset. Furthermore, Recoder also
repairs 19 bugs on the additional bugs from Defects4J v2.0, which is 137.5%
more than TBar (8 bugs) and 850% more than SimFix (2 bugs). This result
suggests that Recoder has better generalizability than existing APR approaches."
483,"While specifications for digital systems are provided in natural language,
engineers undertake significant efforts to translate them into the programming
languages understood by compilers for digital systems. Automating this process
allows designers to work with the language in which they are most comfortable
--the original natural language -- and focus instead on other downstream design
challenges. We explore the use of state-of-the-art machine learning (ML) to
automatically derive Verilog snippets from English via fine-tuning GPT-2, a
natural language ML system. We describe our approach for producing a suitable
dataset of novice-level digital design tasks and provide a detailed exploration
of GPT-2, finding encouraging translation performance across our task sets
(94.8% correct), with the ability to handle both simple and abstract design
tasks."
484,"While AI is extensively transforming Software Engineering (SE) fields, SE is
still in need of a framework to overall consider all phases to facilitate
Automated Software Evolution (ASEv), particularly for intelligent applications
that are context-rich, instead of conquering each division independently. Its
complexity comes from the intricacy of the intelligent applications, the
heterogeneity of the data sources, and the constant changes in the context.
This study proposes a conceptual framework for achieving automated software
evolution, emphasizing the importance of multimodality learning. A Selective
Sequential Scope Model (3S) model is developed based on the conceptual
framework, and it can be used to categorize existing and future research when
it covers different SE phases and multimodal learning tasks. This research is a
preliminary step toward the blueprint of a higher-level ASEv. The proposed
conceptual framework can act as a practical guideline for practitioners to
prepare themselves for diving into this area. Although the study is about
intelligent applications, the framework and analysis methods may be adapted for
other types of software as AI brings more intelligence into their life cycles."
485,"CodeCompose is an AI-assisted code authoring tool powered by large language
models (LLMs) that provides inline suggestions to 10's of thousands of
developers at Meta. In this paper, we present how we scaled the product from
displaying single-line suggestions to multi-line suggestions. This evolution
required us to overcome several unique challenges in improving the usability of
these suggestions for developers.
  First, we discuss how multi-line suggestions can have a 'jarring' effect, as
the LLM's suggestions constantly move around the developer's existing code,
which would otherwise result in decreased productivity and satisfaction.
  Second, multi-line suggestions take significantly longer to generate; hence
we present several innovative investments we made to reduce the perceived
latency for users. These model-hosting optimizations sped up multi-line
suggestion latency by 2.5x.
  Finally, we conduct experiments on 10's of thousands of engineers to
understand how multi-line suggestions impact the user experience and contrast
this with single-line suggestions. Our experiments reveal that (i) multi-line
suggestions account for 42% of total characters accepted (despite only
accounting for 16% for displayed suggestions) (ii) multi-line suggestions
almost doubled the percentage of keystrokes saved for users from 9% to 17%.
Multi-line CodeCompose has been rolled out to all engineers at Meta, and less
than 1% of engineers have opted out of multi-line suggestions."
486,"With the growing adoption of self-adaptive systems in various domains, there
is an increasing need for strategies to assess their correct behavior. In
particular self-healing systems, which aim to provide resilience and
fault-tolerance, often deal with unanticipated failures in critical and highly
dynamic environments. Their reactive and complex behavior makes it challenging
to assess if these systems execute according to the desired goals. Recently,
several studies have expressed concern about the lack of systematic evaluation
methods for self-healing behavior.
  In this paper, we propose CHESS, an approach for the systematic evaluation of
self-adaptive and self-healing systems that builds on chaos engineering. Chaos
engineering is a methodology for subjecting a system to unexpected conditions
and scenarios. It has shown great promise in helping developers build resilient
microservice architectures and cyber-physical systems. CHESS turns this idea
around by using chaos engineering to evaluate how well a self-healing system
can withstand such perturbations. We investigate the viability of this approach
through an exploratory study on a self-healing smart office environment. The
study helps us explore the promises and limitations of the approach, as well as
identify directions where additional work is needed. We conclude with a summary
of lessons learned."
487,"Test Case Selection (TCS) aims to select a subset of the test suite to run
for regression testing. The selection is typically based on past coverage and
execution cost data. Researchers have successfully used multi-objective
evolutionary algorithms (MOEAs), such as NSGA-II and its variants, to solve
this problem. These MOEAs use traditional crossover operators to create new
candidate solutions through genetic recombination. Recent studies in numerical
optimization have shown that better recombinations can be made using machine
learning, in particular link-age learning. Inspired by these recent advances in
this field, we propose a new variant of NSGA-II, called L2-NSGA, that uses
linkage learning to optimize test case selection. In particular, we use an
unsupervised clustering algorithm to infer promising patterns among the
solutions (subset of test suites). Then, these patterns are used in the next
iterations of L2-NSGA to create solutions that preserve these inferred
patterns. Our results show that our customizations make NSGA-II more effective
for test case selection. The test suite sub-sets generated by L2-NSGA are less
expensive and detect more faults than those generated by MOEAs used in the
literature for regression testing."
488,"Public vulnerability databases such as CVE and NVD account for only 60% of
security vulnerabilities present in open-source projects, and are known to
suffer from inconsistent quality. Over the last two years, there has been
considerable growth in the number of known vulnerabilities across projects
available in various repositories such as NPM and Maven Central. Such an
increasing risk calls for a mechanism to infer the presence of security threats
in a timely manner. We propose novel hierarchical deep learning models for the
identification of security-relevant commits from either the commit diff or the
source code for the Java classes. By comparing the performance of our model
against code2vec, a state-of-the-art model that learns from path-based
representations of code, and a logistic regression baseline, we show that deep
learning models show promising results in identifying security-related commits.
We also conduct a comparative analysis of how various deep learning models
learn across different input representations and the effect of regularization
on the generalization of our models."
489,"This paper presents an ontology-based approach for the design of a
collaborative business process model (CBP). This CBP is considered as a
specification of needs in order to build a collaboration information system
(CIS) for a network of organisations. The study is a part of a model driven
engineering approach of the CIS in a specific enterprise interoperability
framework that will be summarised. An adaptation of the Business Process
Modeling Notation (BPMN) is used to represent the CBP model. We develop a
knowledge-based system (KbS) which is composed of three main parts: knowledge
gathering, knowledge representation and reasoning, and collaborative business
process modelling. The first part starts from a high abstraction level where
knowledge from business partners is captured. A collaboration ontology is
defined in order to provide a structure to store and use the knowledge
captured. In parallel, we try to reuse generic existing knowledge about
business processes from the MIT Process Handbook repository. This results in a
collaboration process ontology that is also described. A set of rules is
defined in order to extract knowledge about fragments of the CBP model from the
two previous ontologies. These fragments are finally assembled in the third
part of the KbS. A prototype of the KbS has been developed in order to
implement and support this approach. The prototype is a computer-aided design
tool of the CBP. In this paper, we will present the theoretical aspects of each
part of this KbS as well as the tools that we developed and used in order to
support its functionalities."
490,"There have been major developments in Automated Driving (AD) and Driving
Assist Systems (ADAS) in recent years. However, their safety assurance, thus
methodologies for testing, verification and validation AD/ADAS safety-critical
applications remain as one the main challenges. Inevitably AI also penetrates
into AD/ADAS applications, such as object detection. Despite important
benefits, adoption of such learned-enabled components and systems in
safety-critical scenarios causes that conventional testing approaches (e.g.,
distance-based testing in automotive) quickly become infeasible. Similarly,
safety engineering approaches usually assume model-based components and do not
handle learning-enabled ones well. The authors have participated in the
public-funded project FOCETA , and developed an Automated Valet Parking (AVP)
use case. As the nature of the baseline implementation is imperfect, it offers
a space for continuous improvement based on modelling, verification,
validation, and monitoring techniques. In this publication, we explain the
simulation-based development platform that is designed to verify and validate
safety-critical learning-enabled systems in continuous engineering loops."
491,"Monolithic software encapsulates all functional capabilities into a single
deployable unit. But managing it becomes harder as the demand for new
functionalities grow. Microservice architecture is seen as an alternate as it
advocates building an application through a set of loosely coupled small
services wherein each service owns a single functional responsibility. But the
challenges associated with the separation of functional modules, slows down the
migration of a monolithic code into microservices. In this work, we propose a
representation learning based solution to tackle this problem. We use a
heterogeneous graph to jointly represent software artifacts (like programs and
resources) and the different relationships they share (function calls,
inheritance, etc.), and perform a constraint-based clustering through a novel
heterogeneous graph neural network. Experimental studies show that our approach
is effective on monoliths of different types."
492,"Despite decades of research, SE lacks widely accepted models (that offer
precise quantitative stable predictions) about what factors most influence
software quality. This paper provides a promising result showing such stable
models can be generated using a new transfer learning framework called
""STABILIZER"". Given a tree of recursively clustered projects (using project
meta-data), STABILIZER promotes a model upwards if it performs best in the
lower clusters (stopping when the promoted model performs worse than the models
seen at a lower level).
  The number of models found by STABILIZER is minimal: one for defect
prediction (756 projects) and less than a dozen for project health (1628
projects). Hence, via STABILIZER, it is possible to find a few projects which
can be used for transfer learning and make conclusions that hold across
hundreds of projects at a time. Further, the models produced in this manner
offer predictions that perform as well or better than the prior
state-of-the-art.
  To the best of our knowledge, STABILIZER is order of magnitude faster than
the prior state-of-the-art transfer learners which seek to find conclusion
stability, and these case studies are the largest demonstration of the
generalizability of quantitative predictions of project quality yet reported in
the SE literature.
  In order to support open science, all our scripts and data are online at
https://github.com/Anonymous633671/STABILIZER."
493,"In industry as well as education as well as academics we see a growing need
for knowledge on how to apply machine learning in software applications. With
the educational programme ICT & AI at Fontys UAS we had to find an answer to
the question: ""How should we educate software engineers to become AI
engineers?"" This paper describes our educational programme, the open source
tools we use, and the literature it is based on. After three years of
experience, we present our lessons learned for both educational institutions
and software engineers in practice."
494,"In software engineering practice, fixing a bug promptly reduces the
associated costs. On the other hand, the manual bug fixing process can be
time-consuming, cumbersome, and error-prone. In this work, we introduce a bug
triaging method, called Dependency-aware Bug Triaging (DABT), which leverages
natural language processing and integer programming to assign bugs to
appropriate developers. Unlike previous works that mainly focus on one aspect
of the bug reports, DABT considers the textual information, cost associated
with each bug, and dependency among them. Therefore, this comprehensive
formulation covers the most important aspect of the previous works while
considering the blocking effect of the bugs. We report the performance of the
algorithm on three open-source software systems, i.e., EclipseJDT, LibreOffice,
and Mozilla. Our result shows that DABT is able to reduce the number of overdue
bugs up to 12\%. It also decreases the average fixing time of the bugs by half.
Moreover, it reduces the complexity of the bug dependency graph by prioritizing
blocking bugs."
495,"Artificial intelligence (AI) in its various forms finds more and more its way
into complex distributed systems. For instance, it is used locally, as part of
a sensor system, on the edge for low-latency high-performance inference, or in
the cloud, e.g. for data mining. Modern complex systems, such as connected
vehicles, are often part of an Internet of Things (IoT). To manage complexity,
architectures are described with architecture frameworks, which are composed of
a number of architectural views connected through correspondence rules. Despite
some attempts, the definition of a mathematical foundation for architecture
frameworks that are suitable for the development of distributed AI systems
still requires investigation and study. In this paper, we propose to extend the
state of the art on architecture framework by providing a mathematical model
for system architectures, which is scalable and supports co-evolution of
different aspects for example of an AI system. Based on Design Science
Research, this study starts by identifying the challenges with architectural
frameworks. Then, we derive from the identified challenges four rules and we
formulate them by exploiting concepts from category theory. We show how
compositional thinking can provide rules for the creation and management of
architectural frameworks for complex systems, for example distributed systems
with AI. The aim of the paper is not to provide viewpoints or architecture
models specific to AI systems, but instead to provide guidelines based on a
mathematical formulation on how a consistent framework can be built up with
existing, or newly created, viewpoints. To put in practice and test the
approach, the identified and formulated rules are applied to derive an
architectural framework for the EU Horizon 2020 project ``Very efficient deep
learning in the IoT"" (VEDLIoT) in the form of a case study."
496,"Software quality is one of the essential aspects of a software. With
increasing demand, software designs are becoming more complex, increasing the
probability of software defects. Testers improve the quality of software by
fixing defects. Hence the analysis of defects significantly improves software
quality. The complexity of software also results in a higher number of defects,
and thus manual detection can become a very time-consuming process. This gave
researchers incentives to develop techniques for automatic software defects
detection. In this paper, we try to analyze the state of the art machine
learning algorithms' performance for software defect classification. We used
seven datasets from the NASA promise dataset repository for this research work.
The performance of Neural Networks and Gradient Boosting classifier dominated
other algorithms."
497,"Online social networks have become an integral aspect of our daily lives and
play a crucial role in shaping our relationships with others. However, bugs and
glitches, even minor ones, can cause anything from frustrating problems to
serious data leaks that can have farreaching impacts on millions of users. To
mitigate these risks, fuzz testing, a method of testing with randomised inputs,
can provide increased confidence in the correct functioning of a social
network. However, implementing traditional fuzz testing methods can be
prohibitively difficult or impractical for programmers outside of the social
network's development team. To tackle this challenge, we present Socialz, a
novel approach to social fuzz testing that (1) characterises real users of a
social network, (2) diversifies their interaction using evolutionary
computation across multiple, non-trivial features, and (3) collects performance
data as these interactions are executed. With Socialz, we aim to put social
testing tools in everybody's hands, thereby improving the reliability and
security of social networks used worldwide. In our study, we came across (1)
one known limitation of the current GitLab CE and (2) 6,907 errors, of which
40.16% are beyond our debugging skills."
498,"In this paper, we present NSGA-II-SVM (Non-dominated Sorting Genetic
Algorithm with Support Vector Machine Guidance), a novel learnable evolutionary
and search-based testing algorithm that leverages Support Vector Machine (SVM)
classification models to direct the search towards failure-revealing test
inputs. Supported by genetic search, NSGA-II-SVM creates iteratively SVM-based
models of the test input space, learning which regions in the search space are
promising to be explored. A subsequent sampling and repetition of evolutionary
search iterations allow to refine and make the model more accurate in the
prediction. Our preliminary evaluation of NSGA-II-SVM by testing an Automated
Valet Parking system shows that NSGA-II-SVM is more effective in identifying
more critical test cases than a state of the art learnable evolutionary testing
technique as well as naive random search."
499,"Deep learning (DL) defines a new data-driven programming paradigm that
constructs the internal system logic of a crafted neuron network through a set
of training data. We have seen wide adoption of DL in many safety-critical
scenarios. However, a plethora of studies have shown that the state-of-the-art
DL systems suffer from various vulnerabilities which can lead to severe
consequences when applied to real-world applications. Currently, the testing
adequacy of a DL system is usually measured by the accuracy of test data.
Considering the limitation of accessible high quality test data, good accuracy
performance on test data can hardly provide confidence to the testing adequacy
and generality of DL systems. Unlike traditional software systems that have
clear and controllable logic and functionality, the lack of interpretability in
a DL system makes system analysis and defect detection difficult, which could
potentially hinder its real-world deployment. In this paper, we propose
DeepGauge, a set of multi-granularity testing criteria for DL systems, which
aims at rendering a multi-faceted portrayal of the testbed. The in-depth
evaluation of our proposed testing criteria is demonstrated on two well-known
datasets, five DL systems, and with four state-of-the-art adversarial attack
techniques against DL. The potential usefulness of DeepGauge sheds light on the
construction of more generic and robust DL systems."
500,"With the maturity of web services, containers, and cloud computing
technologies, large services in traditional systems (e.g. the computation
services of machine learning and artificial intelligence) are gradually being
broken down into many microservices to increase service reusability and
flexibility. Therefore, this study proposes an efficiency analysis framework
based on queuing models to analyze the efficiency difference of breaking down
traditional large services into n microservices. For generalization, this study
considers different service time distributions (e.g. exponential distribution
of service time and fixed service time) and explores the system efficiency in
the worst-case and best-case scenarios through queuing models (i.e. M/M/1
queuing model and M/D/1 queuing model). In each experiment, it was shown that
the total time required for the original large service was higher than that
required for breaking it down into multiple microservices, so breaking it down
into multiple microservices can improve system efficiency. It can also be
observed that in the best-case scenario, the improvement effect becomes more
significant with an increase in arrival rate. However, in the worst-case
scenario, only slight improvement was achieved. This study found that breaking
down into multiple microservices can effectively improve system efficiency and
proved that when the computation time of the large service is evenly
distributed among multiple microservices, the best improvement effect can be
achieved. Therefore, this study's findings can serve as a reference guide for
future development of microservice architecture."
501,"Data-driven engineering refers to systematic data collection and processing
using machine learning to improve engineering systems. Currently, the
implementation of data-driven engineering relies on fundamental data science
and software engineering skills. At the same time, model-based engineering is
gaining relevance for the engineering of complex systems. In previous work, a
model-based engineering approach integrating the formalization of machine
learning tasks using the general-purpose modeling language SysML is presented.
However, formalized machine learning tasks still require the implementation in
a specialized programming languages like Python. Therefore, this work aims to
facilitate the implementation of data-driven engineering in practice by
extending the previous work of formalizing machine learning tasks by
integrating model transformation to generate executable code. The method
focuses on the modifiability and maintainability of the model transformation so
that extensions and changes to the code generation can be integrated without
requiring modifications to the code generator. The presented method is
evaluated for feasibility in a case study to predict weather forecasts. Based
thereon, quality attributes of model transformations are assessed and
discussed. Results demonstrate the flexibility and the simplicity of the method
reducing efforts for implementation. Further, the work builds a theoretical
basis for standardizing data-driven engineering implementation in practice."
502,"Existing language models such as n-grams for software code often fail to
capture a long context where dependent code elements scatter far apart. In this
paper, we propose a novel approach to build a language model for software code
to address this particular issue. Our language model, partly inspired by human
memory, is built upon the powerful deep learning-based Long Short Term Memory
architecture that is capable of learning long-term dependencies which occur
frequently in software code. Results from our intrinsic evaluation on a corpus
of Java projects have demonstrated the effectiveness of our language model.
This work contributes to realizing our vision for DeepSoft, an end-to-end,
generic deep learning-based framework for modeling software and its development
process."
503,"The paper presents two equivalent definitions of answer sets for logic
programs with aggregates. These definitions build on the notion of unfolding of
aggregates, and they are aimed at creating methodologies to translate logic
programs with aggregates to normal logic programs or positive programs, whose
answer set semantics can be used to defined the semantics of the original
programs. The first definition provides an alternative view of the semantics
for logic programming with aggregates described by Pelov et al.
  The second definition is similar to the traditional answer set definition for
normal logic programs, in that, given a logic program with aggregates and an
interpretation, the unfolding process produces a positive program. The paper
shows how this definition can be extended to consider aggregates in the head of
the rules.
  The proposed views of logic programming with aggregates are simple and
coincide with the ultimate stable model semantics, and with other semantic
characterizations for large classes of program (e.g., programs with monotone
aggregates and programs that are aggregate-stratified).
  Moreover, it can be directly employed to support an implementation using
available answer set solvers. The paper describes a system, called ASP^A, that
is capable of computing answer sets of programs with arbitrary (e.g.,
recursively defined) aggregates."
